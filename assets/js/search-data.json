{
  
    
        "post0": {
            "title": "The Efficient Frontier",
            "content": "Objective . We have all heard the saying, &quot;There is no such thing a free lunch&quot; and as with other things, this is usually the case in the financial markets, just as with all other cases. However, there is an exception to this rule and it has to do with portfolio diversification. . We will show how diversification can reduce a portfolio&#39;s risk without affecting its&#39; return, with some explicit examples. We will then explore how to use this concept to build the &quot;best&quot; portfolios available, a concept called the efficient frontier, developed by Harry Markowitz back in 1952. After that we will see that that while theoretically pleasing, there are some major drawbacks with this method and propose alternatives to this method of portfolio construction. . Let&#39;s start by looking at his &quot;free lunch&quot; idea. . # Setup / Imports import numpy as np import pandas as pd from pandas_datareader import data import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline from scipy.optimize import minimize from IPython.display import Markdown as md pd.set_option(&#39;display.float_format&#39;, lambda x: &#39;%.5f&#39; % x) print(&quot;Setup Complete&quot;) . . Setup Complete . Free Lunch - Diversification . We will begin by looking at some randomized returns and volatilities of 5 stocks separately and them see what happens if we combine them together in a portfolio. . Let&#39;s create some these randomized annual returns for 5 individual stocks over 10 years assuming: . * expected return of 10% * expected volatility of 20%. . np.random.seed(24) stock_a = np.random.normal(0.10, 0.20, 10) stock_b = np.random.normal(0.10, 0.20, 10) stock_c = np.random.normal(0.10, 0.20, 10) stock_d = np.random.normal(0.10, 0.20, 10) stock_e = np.random.normal(0.10, 0.20, 10) returns = pd.DataFrame({ &quot;Stock A&quot;: stock_a, &quot;Stock B&quot;: stock_b, &quot;Stock C&quot;: stock_c, &quot;Stock D&quot;: stock_d, &quot;Stock E&quot;: stock_e }) returns . Stock A Stock B Stock C Stock D Stock E . 0 0.36584 | 0.23576 | -0.16739 | 0.02286 | 0.35282 | . 1 -0.05401 | 0.47785 | 0.21257 | 0.20396 | 0.15801 | . 2 0.03674 | 0.29231 | 0.37857 | 0.43732 | -0.29406 | . 3 -0.09816 | 0.12080 | 0.08733 | -0.16519 | 0.26078 | . 4 -0.11416 | 0.00377 | 0.12433 | 0.38580 | 0.30611 | . 5 -0.18774 | 0.27005 | 0.34152 | -0.31787 | 0.12362 | . 6 0.21288 | 0.39068 | 0.09959 | 0.07404 | 0.09563 | . 7 0.15914 | 0.31155 | 0.42556 | 0.22630 | 0.10937 | . 8 -0.22528 | 0.13311 | 0.17090 | -0.01731 | -0.22575 | . 9 0.14391 | 0.20300 | 0.30751 | 0.15814 | 0.02153 | . Let&#39;s look at the simulated returns and volatilies for these 5 stocks. . ave_returns = ((1+returns).prod()) ** (1/10) - 1 ave_vol = returns.std() portfolio = pd.DataFrame({ &quot;Ann Returns&quot;: ave_returns, &quot;Ann Volatility&quot;: ave_vol }) portfolio . Ann Returns Ann Volatility . Stock A 0.00801 | 0.19218 | . Stock B 0.23693 | 0.13772 | . Stock C 0.18539 | 0.17600 | . Stock D 0.07670 | 0.23355 | . Stock E 0.07000 | 0.21160 | . def annualize_rets(returns, periods_per_year=12): # compound each years&#39; return at 1+r compounded_growth = (1+returns).prod() # calculate the number of periods in ind_returns n_periods = returns.shape[0] return compounded_growth ** (periods_per_year / n_periods) - 1 def annualize_stdev(returns, periods_per_year=12): return returns.std() * np.sqrt(periods_per_year) . returns[&quot;Equal Weight&quot;] = returns.mean(axis=1) avg_returns = annualize_rets(returns, 1) avg_vol = annualize_stdev(returns, 1) portfolio = pd.DataFrame({ &quot;Ann Returns&quot;: avg_returns, &quot;Ann Volatility&quot;: avg_vol }) portfolio . Ann Returns Ann Volatility . Stock A 0.00801 | 0.19218 | . Stock B 0.23693 | 0.13772 | . Stock C 0.18539 | 0.17600 | . Stock D 0.07670 | 0.23355 | . Stock E 0.07000 | 0.21160 | . Equal Weight 0.12846 | 0.08562 | . WOW! Look at what happened to our Volatility number. For the equal weight portfolio, our volatility has dropped dramatically. How is this possible? . Well, let&#39;s take a look at the first 3 year of returns and see what is happening. . returns.loc[0:2,] . Stock A Stock B Stock C Stock D Stock E Equal Weight . 0 0.36584 | 0.23576 | -0.16739 | 0.02286 | 0.35282 | 0.16198 | . 1 -0.05401 | 0.47785 | 0.21257 | 0.20396 | 0.15801 | 0.19968 | . 2 0.03674 | 0.29231 | 0.37857 | 0.43732 | -0.29406 | 0.17018 | . From these returns, we can see that the extreme return levels seen at the individual stock level are averaged out at the portfolio level, significantly reducing the volatility of the portfolio. . To be honest, this is bit of a contrived example. The reason is that we made the assumption, albeit without explicitly stating it, that all of these 5 stocks have no relationship to each other, or completely uncorrelated. As anyone who has watched the stock market at all, will know that isn&#39;t likely to be true. Do all stocks move perfectly together, no - but the overall direction of the market does have an strong effect on each individual stocks. This is called market risk. . Market risk is one of the two main types of risks that make up an individual stock&#39;s risk. The other being stock-specific or idiosyncratic risk. This risk can be an industry-driven risk, such as the price of oil, which may affect all airlines&#39; stock price, and can be stock-specific, such-as an indiviudal company missing the trend towards online shopping. This idiosyncratic risk is what we can diversify away by holding a variety of stocks from different industries with different specific risk, and what we will be left with is simply market risk. . Let&#39;s back to the how each stock&#39;s correlation with each other impacts volatility. As we can see from this somewhat intimidating equation for the volatility of a 2-stock portfolio: . $$ sigma^2(w_a, w_b) = sigma_A^2w_A^2 + sigma_B^2w_B^2 + 2w_Aw_B sigma_A sigma_B rho_{A,B} $$ . the last term of $2w_Aw_B sigma_A sigma_B rho_{A,B}$ is really what we are interested in. Here $ rho_{A,B}$ represents the correlation between Stock A and Stock B. As the correlation approaches 1, the volatility of the 2-stock portfolio with approach the average of the 2 stock&#39;s volatilies. Above, we showed an example of what happens at a correlation of 0, but if you can actually find two assets that move in opposite directions, this will have an even further reduction in the overall volatility of a portfolio. . Now, with this background under our belts, we will look at expanding this idea to various assets and look to construct an &quot;optimal&quot; portfolio. . The Efficient Frontier . In the above example, we only showed a portfolio constructed with an even amount allocated to each security. However, in the real world different investors will have different goals, and such with have different risk tolerance and reward targets. As such, we should be able to construct a variety of optimal portfolios for each risk tolerance. This concept is called the Efficient Frontier - a set of portfolios which provide the highest expected return for a given level of risk or vice-versa - lowest risk for an expected return. . If we generalize our 2-stock portfolio volatility formula to include n-securities, we can then solve for the optimal weights, $w_i$, for each risk level that provide the highest return. Let&#39;s look at his using the following 10 Exchanged-Traded-Funds to determine this efficient frontier. . IVV - Large Cap US Stocks | IJH - Mid Cap US Stocks | IJR - Small Cap US Stocks | IEFA - Devevloped International Stocks | EEM - Emerging Market Stocks | GOVT - US Treasury Bond | LQD - Investment-Grade Corporate Bonds | HYG - High Yield Bonds | IYR - Real Estate | GSG - Commodities | . We will import this data from Yahaoo Finance. inspect the beginning and end of the data and quickly chart the ETF prices. . tickers = [&quot;IVV&quot;, &quot;IJH&quot;, &quot;IJR&quot;, &quot;IEFA&quot;, &quot;EEM&quot;, &quot;GOVT&quot;, &quot;LQD&quot;, &quot;HYG&quot;, &quot;IYR&quot;, &quot;GSG&quot;] etf_prices = data.DataReader(tickers, data_source=&quot;yahoo&quot;, start=&quot;1/1/2015&quot;, end=&quot;12/31/2019&quot;)[&quot;Adj Close&quot;] etf_prices.head() . Symbols IVV IJH IJR IEFA EEM GOVT LQD HYG IYR GSG . Date . 2015-01-02 183.563370 | 132.017548 | 52.162510 | 47.162361 | 34.465927 | 22.891134 | 98.729385 | 65.700562 | 63.215065 | 21.219999 | . 2015-01-05 180.340149 | 130.036240 | 51.365459 | 46.103111 | 33.852528 | 22.927330 | 99.133095 | 65.091957 | 63.426365 | 20.620001 | . 2015-01-06 178.715271 | 128.593613 | 50.508526 | 45.624733 | 33.710285 | 23.035944 | 99.536781 | 64.842636 | 63.889561 | 20.280001 | . 2015-01-07 180.926254 | 130.337570 | 51.033741 | 46.094570 | 34.439262 | 23.035944 | 99.668640 | 65.245941 | 64.718445 | 20.219999 | . 2015-01-08 184.158295 | 132.346237 | 51.927528 | 46.683990 | 35.025986 | 22.981640 | 99.347290 | 65.737221 | 65.084122 | 20.290001 | . etf_prices.tail() . Symbols IVV IJH IJR IEFA EEM GOVT LQD HYG IYR GSG . Date . 2019-12-24 317.87265 | 202.80856 | 83.10228 | 64.21871 | 44.31289 | 25.40180 | 124.62210 | 83.80858 | 89.70977 | 16.24000 | . 2019-12-26 319.51791 | 203.25221 | 82.99363 | 64.47554 | 44.63104 | 25.43118 | 124.84631 | 83.93262 | 90.18898 | 16.39000 | . 2019-12-27 319.45880 | 202.91702 | 82.61832 | 64.63360 | 44.81000 | 25.46057 | 125.02174 | 83.89445 | 90.43347 | 16.38000 | . 2019-12-30 317.73474 | 202.71982 | 82.56893 | 64.15944 | 44.51173 | 25.44098 | 125.24596 | 83.83720 | 90.50192 | 16.33000 | . 2019-12-31 318.45389 | 202.91702 | 82.81585 | 64.44591 | 44.61115 | 25.41159 | 124.72932 | 83.91354 | 91.03003 | 16.21000 | . etf_norm = etf_prices / etf_prices.iloc[0] etf_norm.plot(figsize=(15, 7)); . Next, in order to calculate the Efficient Frontier, we need 2 inputs. These inputs are the annualized returns and the covariance matrix between each security. . First we will calculate the daily returns for the time series. From that, the covariance is calculated as $ sigma_A sigma_B rho_{A,B}$ or the variance between two stocks. Below, we will calculate the covariance matrix and have a look at the correlation matrix, which is a little more intuitive to understand. As a reminder, a correlation of 1 means that the stock move perfected together, and -1 means they move perfectly opposite each other. . We can see from the correlation heatmap, that the stock ETFs are highly correlated (close to 1) and the bond ETFs are negatively correlated with stocks (below 0). . etf_returns = etf_prices.pct_change() etf_returns.drop(etf_returns.index[0], axis=0, inplace=True) etf_returns . Symbols IVV IJH IJR IEFA EEM GOVT LQD HYG IYR GSG . Date . 2015-01-05 -0.017559 | -0.015008 | -0.015280 | -0.022460 | -0.017797 | 0.001581 | 0.004089 | -0.009263 | 0.003343 | -0.028275 | . 2015-01-06 -0.009010 | -0.011094 | -0.016683 | -0.010376 | -0.004202 | 0.004737 | 0.004072 | -0.003830 | 0.007303 | -0.016489 | . 2015-01-07 0.012372 | 0.013562 | 0.010399 | 0.010298 | 0.021625 | 0.000000 | 0.001325 | 0.006220 | 0.012974 | -0.002959 | . 2015-01-08 0.017864 | 0.015411 | 0.017514 | 0.012787 | 0.017036 | -0.002357 | -0.003224 | 0.007530 | 0.005650 | 0.003462 | . 2015-01-09 -0.008438 | -0.008486 | -0.009759 | -0.004575 | -0.003300 | 0.002757 | 0.002654 | 0.005020 | 0.000375 | -0.010843 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 2019-12-24 0.000124 | -0.000097 | 0.002741 | -0.000922 | -0.002239 | 0.001159 | 0.001018 | 0.000798 | 0.002733 | 0.004329 | . 2019-12-26 0.005176 | 0.002188 | -0.001307 | 0.003999 | 0.007180 | 0.001157 | 0.001799 | 0.001480 | 0.005342 | 0.009236 | . 2019-12-27 -0.000185 | -0.001649 | -0.004522 | 0.002451 | 0.004010 | 0.001156 | 0.001405 | -0.000455 | 0.002711 | -0.000610 | . 2019-12-30 -0.005397 | -0.000972 | -0.000598 | -0.007336 | -0.006656 | -0.000769 | 0.001794 | -0.000682 | 0.000757 | -0.003052 | . 2019-12-31 0.002263 | 0.000973 | 0.002990 | 0.004465 | 0.002234 | -0.001155 | -0.004125 | 0.000910 | 0.005835 | -0.007348 | . 1257 rows × 10 columns . cov_mat = etf_returns.cov() # Calculate and plot the correlation matrix using a Heatmap plt.figure(figsize=(8, 8)) sns.heatmap(etf_returns.corr(), annot=True, cmap=&quot;coolwarm&quot;); . Calculating the efficient frontier . As mentioned earlier, the efficient frontier is simply the various portfolios which have the lowest risk for a given return or the highest return for a given risk. Before we optimize our 10-ETF portfolio, let&#39;s look at a 2-security example to understand a little better. . We will create 2 news functions, which calculate porfolio returns based on the weights of each security and another which calcualtes the portfolio volatility which was that scary formula above. That formula can minimize to a few matrix-multiplications as seen in the function definition. . def portfolio_return(weights, returns): return weights.T @ returns def portfolio_vol(weights, cov_mat): return (weights.T @ cov_mat @ weights)**0.5 . For this example, we will use: . &#39;IVV&#39; - large cap US stocks | &#39;GOVT&#39; - US government bonds. | . We will calcualte their annualized returns and a covariance matrix. Next, we will simply build portfolios from 0% in stocks and 100% in bonds to 100% / 0% and do so in 5% increments. . From these weightings, we can then calculate the portfolio&#39;s return and risk, and then plot it. One important thing to take away from the plot, if these are the only 2 securities that you can invest in, you would never invest in 100% &#39;GOVT&#39;. The reason is that there are portfolios that &#39;dominate&#39; that portfolio. This means that other portfolios provide either less risk or more return, or in fact you can actually achieve both, by just sliding a few portfolios to the left. . This is the Efficient Frontier for a 2-security portfolio. . port_2 = etf_returns[[&quot;IVV&quot;, &quot;GOVT&quot;]] port_2_rets = annualize_rets(port_2, 252) cov_2 = port_2.cov() . n_points = 21 weights = [ np.array([w, 1-w]) for w in np.linspace(0, 1, n_points) ] port_returns = [portfolio_return(w, port_2_rets) for w in weights] port_vol = [portfolio_vol(w, cov_2) for w in weights] ef = pd.DataFrame({ &quot;Return&quot;: port_returns, &quot;Vol&quot;: port_vol }) . ef.plot.line(x=&quot;Vol&quot;, y=&quot;Return&quot;, figsize=(12,8), style=&quot;.-&quot;, legend=None) plt.title(&quot;Efficient Frontier for 2-Security Portfolio&quot;); . Efficient Frontier - Multi-Asset . To build the efficient frontier for a multi-asset portfolio, we need to create an optimiztion function. Thankfully, scipy provides an optimization function, minimize. . First, we will create the minimization routine. We set a variety of constraints; each weight must be bound between 0 and 1, weights must sum to 1, and we will set our target to a given return. From this we will optimize the weights to minimize the portfolio_vol using Quadratic Programming. . Next, we will create a function which returns a list of weights given a list of returns by calling that minimuzation routine. The list of returns will be created by simply identifying the minimum and maximum return from the series and creating a linearly spaced array along the possible returns. . Lastly, we will createa function which plots those results. . def minimize_vol(target_return, exp_rets, cov_mat): n = len(exp_rets) init_guess = np.repeat(1/n, n) # equal weight as initial guess bounds = ((0.0, 1.0),) * n # creates a tuple of tuples with a range for weights for each weight # constraints for optimizer; type = &#39;eq&#39;, and &#39;fun&#39; are solved for equal to 0 return_is_target = { &#39;type&#39;: &#39;eq&#39;, &#39;args&#39;: (exp_rets,), &#39;fun&#39;: lambda weights, exp_rets: target_return - portfolio_return(weights, exp_rets) } weights_sum_to_1 = { &#39;type&#39;: &#39;eq&#39;, &#39;fun&#39;: lambda weights: np.sum(weights) - 1 } # run the optimizer results = minimize(portfolio_vol, init_guess, args=(cov_mat,), method=&quot;SLSQP&quot;, options={&#39;disp&#39;: False}, constraints=(return_is_target, weights_sum_to_1), bounds=bounds ) return results.x def optimal_weights(n_points, exp_rets, cov_mat): target_rets = np.linspace(max(exp_rets.min(),0), exp_rets.max(), n_points) weights = [minimize_vol(target_return, exp_rets, cov_mat) for target_return in target_rets] return weights def plot_ef(n_points, exp_rets, cov_mat, figsize=(10, 7)): weights = optimal_weights(n_points, exp_rets, cov_mat) rets = [portfolio_return(w, exp_rets) for w in weights] vols = [portfolio_vol(w, cov_mat) for w in weights] ef = pd.DataFrame({ &quot;Returns&quot;: rets, &quot;Volatility&quot;: vols }) return ef.plot.line(x=&quot;Volatility&quot;, y=&quot;Returns&quot;, style=&quot;.-&quot;, figsize=figsize) . Now that we have created all the tools we need to calculate and plot the efficient frontier. Let&#39;s see how the efficient frontier is mapped to our 10-etf portfolio. We will calculate the annualized returns and covariance matrix and plot the results for 10 different optimal portfolios. . Further, we will display the weights for each of the portfolios created. Here, we see the first flaw in the efficient frontier. All the portfolios are only allocated to a few ETFs. In fact, IEFA, IYR and EEM are never assigned any weights. So, the inputs or historical time period over which the inputs are calculated can drive the allocation dramatically. . In the 2nd weight matrix, we simply change the expected return for IVV to 8.5% and you can see that it dramatically shifts the allocations, now EEM is the only security to not receive an allocation. . etf_ann_rets = annualize_rets(etf_returns, 252) etf_cov_mat = etf_returns.cov() plot_ef(10, etf_ann_rets, etf_cov_mat); . pd.DataFrame(optimal_weights(10, etf_ann_rets, etf_cov_mat), columns=etf_returns.columns) . Symbols IVV IJH IJR IEFA EEM GOVT LQD HYG IYR GSG . 0 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.71295 | 0.00000 | 0.00000 | 0.00000 | 0.28705 | . 1 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.88015 | 0.00000 | 0.00631 | 0.00000 | 0.11354 | . 2 0.00000 | 0.00054 | 0.02014 | 0.00000 | 0.00000 | 0.75004 | 0.00000 | 0.19569 | 0.00000 | 0.03358 | . 3 0.08731 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.58608 | 0.04059 | 0.28602 | 0.00000 | 0.00000 | . 4 0.16782 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.30057 | 0.34171 | 0.18990 | 0.00000 | 0.00000 | . 5 0.30462 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.15142 | 0.54397 | 0.00000 | 0.00000 | 0.00000 | . 6 0.43420 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.56580 | 0.00000 | 0.00000 | 0.00000 | . 7 0.62280 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.37720 | 0.00000 | 0.00000 | 0.00000 | . 8 0.81140 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.18860 | 0.00000 | 0.00000 | 0.00000 | . 9 1.00000 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | . ex_ann_rets = etf_ann_rets ex_ann_rets[&#39;IVV&#39;] = 0.085 pd.DataFrame(optimal_weights(10, ex_ann_rets, etf_cov_mat), columns=etf_returns.columns) . Symbols IVV IJH IJR IEFA EEM GOVT LQD HYG IYR GSG . 0 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.71295 | 0.00000 | 0.00000 | 0.00000 | 0.28705 | . 1 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.85930 | 0.00000 | 0.00000 | 0.00000 | 0.14070 | . 2 0.00416 | 0.00000 | 0.03216 | 0.00000 | 0.00000 | 0.81500 | 0.00000 | 0.08429 | 0.00000 | 0.06438 | . 3 0.00301 | 0.00131 | 0.04721 | 0.00289 | 0.00000 | 0.69785 | 0.00000 | 0.24773 | 0.00000 | 0.00000 | . 4 0.03720 | 0.02238 | 0.03875 | 0.00000 | 0.00000 | 0.35650 | 0.32046 | 0.22472 | 0.00000 | 0.00000 | . 5 0.02025 | 0.00000 | 0.12191 | 0.00000 | 0.00000 | 0.04324 | 0.64600 | 0.16861 | 0.00000 | 0.00000 | . 6 0.08178 | 0.06723 | 0.22194 | 0.00000 | 0.00000 | 0.00000 | 0.62905 | 0.00000 | 0.00000 | 0.00000 | . 7 0.14925 | 0.15687 | 0.25037 | 0.00000 | 0.00000 | 0.00000 | 0.33124 | 0.00000 | 0.11227 | 0.00000 | . 8 0.21771 | 0.24764 | 0.30797 | 0.00000 | 0.00000 | 0.00000 | 0.05659 | 0.00000 | 0.17008 | 0.00000 | . 9 0.00000 | 0.00000 | 1.00000 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | . Conclusion . We have seen the potential benefits of diversifcations, and the theoretical framework for creating an optimal portfolio, however, this comes with signficant drawbacks. In a future post, we will look at an alternative allocation strategy called the Global Minimum Variance Portfolio (along with variants of this idea), as well as using the Black Litterman approach to assigning weights to each security in a portfolio. . References: . EDHEC/Coursera - Investment Management with Python and Machine Learning Investopedia - https://www.investopedia.com/terms/e/efficientfrontier.asp Investopedia - https://www.investopedia.com/terms/m/modernportfoliotheory.asp .",
            "url": "https://scottknappnj.github.io/FastPagesBlog/2020/12/07/Markowitz-Efficient-Frontier-and-Alternatives.html",
            "relUrl": "/2020/12/07/Markowitz-Efficient-Frontier-and-Alternatives.html",
            "date": " • Dec 7, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Tuning Random Forest Models",
            "content": "On a weekly basis, a dataset is released for the R community to create visualizations and perform other analysis. Julia Silge, of R-Studio, produces a periodic tutorial on the Tidymodels framework utilizing these datasets. Here we will replicate her analysis in Python. . This week we have a dataset from IKEA. We will attempt to predict prices using a Random Forest model. There is a relatively small dataset 3,694 items with 13 features. Of these 13 features many are just descriptive of the item (an index, item_id, name, web link, online availability, color_availablity, designer). We have identified 4 variables which may offer some predictive capacity, 3 numeric (depth, height, weight) and 1 categorical (category). We will do some feature engineering with these variables, including imputation using K-Nearest-Neighbors (KNN), and One-Hot encoding. . Let&#39;s begin. . Setup and Data Import . import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from sklearn.model_selection import train_test_split from sklearn.impute import KNNImputer from sklearn.preprocessing import OneHotEncoder from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_absolute_error from sklearn.model_selection import RandomizedSearchCV from sklearn.model_selection import GridSearchCV %matplotlib inline print(&quot;Setup Complete&quot;) . Setup Complete . We will import the data from the website provided and then perform for data cleaning and exploratory data anaylsis. . It is usually a good idea to inspeact the head and tail of the dataset to for a first check that the import was successful. | We will drop the Unnamed: 0 column, as it appears to just be an index. | . One thing that you will notice throughout is the constant use a .shape. This is used as a quick check to ensure that we are at least returning a shape that&#39;s expected. . ikea_raw = pd.read_csv(&quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-11-03/ikea.csv&quot;) ikea_raw.head() . Unnamed: 0 item_id name category price old_price sellable_online link other_colors short_description designer depth height width . 0 0 | 90420332 | FREKVENS | Bar furniture | 265.0 | No old price | True | https://www.ikea.com/sa/en/p/frekvens-bar-tabl... | No | Bar table, in/outdoor, 51x51 cm | Nicholai Wiig Hansen | NaN | 99.0 | 51.0 | . 1 1 | 368814 | NORDVIKEN | Bar furniture | 995.0 | No old price | False | https://www.ikea.com/sa/en/p/nordviken-bar-tab... | No | Bar table, 140x80 cm | Francis Cayouette | NaN | 105.0 | 80.0 | . 2 2 | 9333523 | NORDVIKEN / NORDVIKEN | Bar furniture | 2095.0 | No old price | False | https://www.ikea.com/sa/en/p/nordviken-nordvik... | No | Bar table and 4 bar stools | Francis Cayouette | NaN | NaN | NaN | . 3 3 | 80155205 | STIG | Bar furniture | 69.0 | No old price | True | https://www.ikea.com/sa/en/p/stig-bar-stool-wi... | Yes | Bar stool with backrest, 74 cm | Henrik Preutz | 50.0 | 100.0 | 60.0 | . 4 4 | 30180504 | NORBERG | Bar furniture | 225.0 | No old price | True | https://www.ikea.com/sa/en/p/norberg-wall-moun... | No | Wall-mounted drop-leaf table, ... | Marcus Arvonen | 60.0 | 43.0 | 74.0 | . ikea_raw.tail() . Unnamed: 0 item_id name category price old_price sellable_online link other_colors short_description designer depth height width . 3689 3689 | 99157902 | ELVARLI | Wardrobes | 750.0 | SR 820 | True | https://www.ikea.com/sa/en/p/elvarli-1-section... | No | 1 section, 92x51x222-350 cm | Ehlén Johansson | 50.0 | NaN | 91.0 | . 3690 3690 | 9158152 | ELVARLI | Wardrobes | 1572.0 | SR 1,755 | True | https://www.ikea.com/sa/en/p/elvarli-2-section... | No | 2 sections, 135x51x222-350 cm | Ehlén Johansson | 50.0 | NaN | 135.0 | . 3691 3691 | 59157541 | ELVARLI | Wardrobes | 924.0 | SR 1,050 | True | https://www.ikea.com/sa/en/p/elvarli-2-section... | No | 2 sections, 175x51x222-350 cm | Ehlén Johansson | 50.0 | NaN | 175.0 | . 3692 3692 | 89157573 | ELVARLI | Wardrobes | 2745.0 | SR 3,130 | True | https://www.ikea.com/sa/en/p/elvarli-3-section... | No | 3 sections, 178x51x222-350 cm | Ehlén Johansson | 50.0 | NaN | 178.0 | . 3693 3693 | 69157376 | ELVARLI | Wardrobes | 1231.0 | SR 1,535 | True | https://www.ikea.com/sa/en/p/elvarli-2-section... | No | 2 sections, 175x51x222-350 cm | Ehlén Johansson | 50.0 | NaN | 175.0 | . ikea = ikea_raw.drop(&quot;Unnamed: 0&quot;, axis=1) . ikea.shape . (3694, 13) . After this initial cursory look at the data. We next look to identify the completeness of our data. We see that we have quite of few NAs, from both a table perspective and using a heatmap to see the distribution of the NAs. Visually, it looks like we may be able to preserve some of the incomplete observations by imputing the missing values. This is beacuse the missing data is all in the dimensions of the product. If an item is only missing a single dimension, we can attempt to impute the missing value given the other 2 dimenstions using KNN, this will be discusseed in more depth later on. Unfortunately, we will have to drop those items which are missing 2 or more dimensions. . ikea.isna().sum() . item_id 0 name 0 category 0 price 0 old_price 0 sellable_online 0 link 0 other_colors 0 short_description 0 designer 0 depth 1463 height 988 width 589 dtype: int64 . plt.figure(figsize=(10, 10)) sns.heatmap(ikea.isna()); . drop_index = ikea.isna().sum(axis=1) &gt; 1 ikea.drop(ikea[drop_index].index).isna().sum() . item_id 0 name 0 category 0 price 0 old_price 0 sellable_online 0 link 0 other_colors 0 short_description 0 designer 0 depth 692 height 251 width 24 dtype: int64 . ikea.drop(ikea[drop_index].index, inplace=True) . Data Visualizations . Next, we will quickly tale a look at our features as compared to our target variable. When we look at the price variable, we see that it ranges from a minimum of 5 to a maximum of 9585. To contend with this large range, we will do a log-transformation on the price. We will quickly chart the target vs each or the dimensions and against the various categories. . For the categpry feature, we see that we have 17 different categories, likely too many for our model given the small number of observations, we will take the 11 largest categories and combine the remaining categories into one called other. . From all these visuals, there does not appear to be great predictive value in any one feature, so we must temper our expectations of our model predictions. . ikea[&#39;price&#39;].describe() . count 2866.000000 mean 1113.905652 std 1401.535864 min 5.000000 25% 220.000000 50% 585.000000 75% 1468.000000 max 9585.000000 Name: price, dtype: float64 . ikea[&quot;price_log&quot;] = np.log(ikea[&quot;price&quot;]) . fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (15, 7)) ax1.scatter(&quot;depth&quot;, &quot;price_log&quot;, data=ikea, color = &quot;red&quot;) ax1.set_xlabel(&quot;Depth&quot;) ax2.scatter(&quot;height&quot;, &quot;price_log&quot;, data=ikea, color = &quot;green&quot;) ax2.set_xlabel(&quot;Height&quot;) ax3.scatter(&quot;width&quot;, &quot;price_log&quot;, data=ikea, color = &quot;blue&quot;) ax3.set_xlabel(&quot;Width&quot;); . ikea[&quot;category&quot;].value_counts() . Bookcases &amp; shelving units 493 Tables &amp; desks 390 Chairs 334 Sofas &amp; armchairs 271 Cabinets &amp; cupboards 254 Wardrobes 233 TV &amp; media furniture 152 Beds 150 Outdoor furniture 148 Chests of drawers &amp; drawer units 123 Children&#39;s furniture 111 Nursery furniture 87 Bar furniture 37 Trolleys 27 Sideboards, buffets &amp; console tables 23 Café furniture 21 Room dividers 12 Name: category, dtype: int64 . top_11_cats = ikea[&quot;category&quot;].value_counts().head(11).index ikea[&quot;category&quot;] = ikea[&quot;category&quot;].apply(lambda cat: &quot;Other&quot; if cat not in top_11_cats else cat) ikea[&quot;category&quot;].value_counts() . Bookcases &amp; shelving units 493 Tables &amp; desks 390 Chairs 334 Sofas &amp; armchairs 271 Cabinets &amp; cupboards 254 Wardrobes 233 Other 207 TV &amp; media furniture 152 Beds 150 Outdoor furniture 148 Chests of drawers &amp; drawer units 123 Children&#39;s furniture 111 Name: category, dtype: int64 . plt.figure(figsize=(15, 7)) plt.xticks(rotation=90) plt.bar(&quot;category&quot;, &quot;price_log&quot;, data=ikea); . Data Preparation . We have done some feature engineering above. But we will need to category to ensure that our data is prepared to be entered into a model. . First, we will subset our dataset into the X (predictors or features) and y (target). We will then create a training set, in which we will use to fit the model, and a test set, over which we will evaluate the model. . Numerical variables For the numerical variables, we still need to replace our missing values. As mentioned we will use the KNN model to impute the missing values from the known dimensions. In the KNN algorithm, we will use 3 nearest neighbors, meaning that we will find the 3 closest obeservations to determine the missing value. The closest observations will be found by calculating the eclidian distance between each observation and then simply using the 3 closest to imput the missing value. Note: When performing the transformation, we fit and transform the training set, but then simply use that fit to only transform the test set. . Categorical variables For the categorical variables, we will use One-Hot Encoding to convert the categories into numeric values. In One-Hot Encoding, we simply create a dummy feature for each category with a value of 1 or 0 for each variable depending if the category pertains to that observation or not. . Lastly, we concatenate the numeric and categorical feature back into a single DataFrame ready for modeling. . X = ikea[[&#39;category&#39;, &#39;depth&#39;, &#39;height&#39;, &#39;width&#39;]] y = ikea[&#39;price_log&#39;] X.shape, y.shape . ((2866, 4), (2866,)) . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42) X_train.shape, X_test.shape, y_train.shape, y_test.shape . ((2149, 4), (717, 4), (2149,), (717,)) . num_cats = [&#39;depth&#39;, &#39;height&#39;, &#39;width&#39;] X_train_num = X_train[num_cats] X_test_num = X_test[num_cats] # perform the imputation using KNN, with 3 neighbors (copy=False allows for in-place transformations) imputer = KNNImputer(n_neighbors=3, copy=False) imputer.fit_transform(X_train_num) imputer.transform(X_test_num) X_train_num.shape, X_test_num.shape . ((2149, 3), (717, 3)) . X_train_num.isnull().sum(), X_test_num.isnull().sum() . (depth 0 height 0 width 0 dtype: int64, depth 0 height 0 width 0 dtype: int64) . One_Hot = OneHotEncoder(handle_unknown=&#39;ignore&#39;, sparse=False) X_train_cat = X_train[[&#39;category&#39;]] X_test_cat = X_test[[&#39;category&#39;]] X_train_cat = pd.DataFrame(One_Hot.fit_transform(X_train_cat)) X_test_cat = pd.DataFrame(One_Hot.transform(X_test_cat)) # One-Hot Encoding drops the index, so we need to replace it X_train_cat.index = X_train.index X_test_cat.index = X_test.index X_train_cat.shape, X_test_cat.shape . ((2149, 12), (717, 12)) . X_train_trans = pd.concat([X_train_num, X_train_cat], axis=1) X_test_trans = pd.concat([X_test_num, X_test_cat], axis=1) X_train_trans.shape, X_test_trans.shape . ((2149, 15), (717, 15)) . Train the Model . Now, that our data has been transformed for modeling, we can run our initial base model. Here we will use n_estimators of 200 (we will discuss this in the tuning section). We will then use that fitted model to make predictions on our test data. Those predictions will allow us to evaluate our model. We will use a simple Mean Absolute Error for evaluation. We must remember to transform our predictions back so that the MAE makes more intuitive sense. . We didn&#39;t have high expectations for the model, and a MAE of 396 or 35% isn&#39;t very impressive. Next, we will see if we tuning the model improves our predictive capacity. . rf_model = RandomForestRegressor(n_estimators=200, random_state=42) rf_model.fit(X_train_trans, y_train) preds = rf_model.predict(X_test_trans) mean_absolute_error(y_test, preds) . 0.4496975465145763 . mean_absolute_error(np.exp(y_test), np.exp(preds)), ikea[&#39;price&#39;].mean() . (396.9394406603129, 1113.90565247732) . Tune the Model . Here we will look to tune the model&#39;s hyperparamters. The Random Forest model has a variety of tuning parameters, we will focus on 4 here. . n_estimators: the number of trees that will be constructed | max_depth: maximum number of levels of each tree | min_samples_split: minimum number of items requires to split at a node | min_samples_leaf: minimum number of items at each leaf node | . Other parameters that can be tuned: . max_features: which is the number of features to be considered at each split | bootstrap: whether to use bootstrapping to selct the samples. | . When tuning you provide a range of inputs that you want the model to calculate over, based on what is chosen below this would leave $6*10*3*3$ or 540 iterations of the model. Rather than doing each iteration which would be done using GridSearchCV, we will utilize RandomizedSearchCV which will perform a choose random searches up to the defined number of iterations, we will run 50. . In addition, in order to find the best parameters, we will utilize cross-validation with 5 folds. This will divide our training set into 5 groups, in the first run, it will train on groups (1,2,3,4) and evaluate on 5, in the 2nd run, it will train on groups (1,2,3,5) and test on 4...etc. We will perform this cross-validation on each of the 50 randomly choosen parameter sets and the model will output the best set of parameters for us to use. . n_estimators = [int(x) for x in np.linspace(start=200,stop=1200, num=6)] max_depth = [int(x) for x in np.linspace(10, 50, 10)] min_samples_split = [2, 5, 10] min_samples_leaf = [1, 2, 4] random_grid = {&#39;n_estimators&#39;: n_estimators, &#39;max_depth&#39;: max_depth, &#39;min_samples_split&#39;: min_samples_split, &#39;min_samples_leaf&#39;: min_samples_leaf } random_grid, (6*10*3*3) . ({&#39;n_estimators&#39;: [200, 400, 600, 800, 1000, 1200], &#39;max_depth&#39;: [10, 14, 18, 23, 27, 32, 36, 41, 45, 50], &#39;min_samples_split&#39;: [2, 5, 10], &#39;min_samples_leaf&#39;: [1, 2, 4]}, 540) . rf = RandomForestRegressor(verbose=0) rf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid, n_iter=50, cv=5, verbose=0, random_state=42) rf_random.fit(X_train_trans, y_train) . RandomizedSearchCV(cv=5, estimator=RandomForestRegressor(), n_iter=50, param_distributions={&#39;max_depth&#39;: [10, 14, 18, 23, 27, 32, 36, 41, 45, 50], &#39;min_samples_leaf&#39;: [1, 2, 4], &#39;min_samples_split&#39;: [2, 5, 10], &#39;n_estimators&#39;: [200, 400, 600, 800, 1000, 1200]}, random_state=42) . rf_model.estimators_ . [DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1608637542), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1273642419), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1935803228), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=787846414), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=996406378), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1201263687), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=423734972), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=415968276), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=670094950), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1914837113), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=669991378), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=429389014), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=249467210), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1972458954), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1572714583), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1433267572), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=434285667), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=613608295), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=893664919), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=648061058), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=88409749), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=242285876), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=2018247425), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=953477463), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1427830251), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1883569565), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=911989541), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=3344769), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=780932287), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=2114032571), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=787716372), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=504579232), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1306710475), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=479546681), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=106328085), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=30349564), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1855189739), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=99052376), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1250819632), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=106406362), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=480404538), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1717389822), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=599121577), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=200427519), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1254751707), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=2034764475), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1573512143), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=999745294), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1958805693), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=389151677), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1224821422), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=508464061), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=857592370), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1642661739), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=61136438), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=2075460851), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=396917567), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=2004731384), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=199502978), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1545932260), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=461901618), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=774414982), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=732395540), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1934879560), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=279394470), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=56972561), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1927948675), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1899242072), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1999874363), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=271820813), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1324556529), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1655351289), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1308306184), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=68574553), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=419498548), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=991681409), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=791274835), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1035196507), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1890440558), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=787110843), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=524150214), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=472432043), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=2126768636), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1431061255), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=147697582), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=744595490), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1758017741), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1679592528), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1111451555), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=782698033), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=698027879), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1096768899), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1338788865), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1826030589), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=86191493), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=893102645), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=200619113), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=290770691), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=793943861), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=134489564), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=2016850622), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1470101905), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1181686489), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1931679275), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1887633569), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1697157321), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1695770557), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1832485859), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=420477197), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=976125790), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1811936047), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1402481934), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=380072391), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=302554573), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=841739990), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=89482491), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=194249720), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1980718781), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1397283111), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1479761620), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1669356239), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1062231788), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1165435217), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=170476398), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1411916852), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=372593431), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1532243865), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1998256344), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1206604539), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=459708603), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=183378299), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1185407468), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=605264936), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1272485020), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1297926158), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=709816108), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=320192576), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=67157848), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=2091163462), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1818495496), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1169282391), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1696003200), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=853477355), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1260522119), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=23717335), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=60472382), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1354896522), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=854021618), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=888445520), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=907706759), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=983578274), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1246294434), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1165097248), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=455094650), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=318019332), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1830948329), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1539598566), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=648870905), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=497653800), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1782238235), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1559517318), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1503404232), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=529561415), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1930375947), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1421196193), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=409783328), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=272981039), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1592652278), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1335658902), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=725167677), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1396651735), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=712631076), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=986151010), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=392121003), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=590804839), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1179921109), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1663066074), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=263038498), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=2028147648), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1644658402), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=513653348), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=2025988014), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=915879373), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1498573442), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1120063232), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=952321028), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=263183577), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1013547510), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1163795198), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1099805069), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=2120835942), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=173660954), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=97636744), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=904790222), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1836274702), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=476272473), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=109174313), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1886935931), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=463390156), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=866377394)] . rf_random.best_params_ . {&#39;n_estimators&#39;: 1200, &#39;min_samples_split&#39;: 2, &#39;min_samples_leaf&#39;: 1, &#39;max_depth&#39;: 23} . rf_best= rf_random.best_estimator_ preds = rf_best.predict(X_test_trans) mean_absolute_error(np.exp(y_test), np.exp(preds)) . 397.27203830019045 . plt.figure(figsize=(12, 6)) plt.scatter(y_test, preds, alpha=0.6) plt.title(&quot;Predictions vs Target Variables&quot;) plt.xlabel(&quot;Target&quot;) plt.ylabel(&quot;Predictions&quot;); . Conclusion: . Visually, our model seems to have done a reasonable just at estimating price, even if our MAE is still near 400. And unfortunately, tuning the model did not improve the predictions on our test data. This isn&#39;t entirely suprising given the preliminary analysis we did in the beginning. However, we were able to work through some feature engineering and do some model tuning along the way. . References: . Tune random forests for #TidyTuesday IKEA prices: https://juliasilge.com/blog/ikea-prices/ . Hyperparameter Tuning the Random Forest in Python: https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 .",
            "url": "https://scottknappnj.github.io/FastPagesBlog/2020/12/05/Tuning-Random-Forests.html",
            "relUrl": "/2020/12/05/Tuning-Random-Forests.html",
            "date": " • Dec 5, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Using Python OOP to Create HTML Code",
            "content": "We will utilize Python&#39;s Obejct-Oriented-Programming to ease the ability to construct some simple HTML code. First, let&#39;s take a look at what is needed for HTML. An HTML 4 documents contains three elements: . 1) a line containing the version information of HTML being used 2) a header section enclosed in &lt;HEAD&gt; tags 3) a body section, where contains the document&#39;s content, enclosed in &lt;BODY&gt; tags . Here&#39;s an example of an HTML document: . &lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.01//EN&quot; &quot;http://www.w3.org/TR/html4/strict.dtd&quot;&gt; &lt;HTML&gt; &lt;HEAD&gt; &lt;TITLE&gt;My first HTML document&lt;/TITLE&gt; &lt;/HEAD&gt; &lt;BODY&gt; &lt;P&gt;Hello world! &lt;/BODY&gt; &lt;/HTML&gt; . From this we can start to see what we need in order to create this simple code. . We will begin by creating a Tag class. We will use this class across the classes we will create below. The Tag class will take a name and contents as input. From these, __init__ will create the start_tag, the contents, and the end_tag. This will allow us to create a __str__ and display method to print the Tag. . class Tag(object): def __init__(self, name, contents): self.start_tag = &#39;&lt;{}&gt;&#39;.format(name) self.end_tag = &#39;&lt;/{}&gt;&#39;.format(name) self.contents = contents def __str__(self): return &quot;{0.start_tag}{0.contents}{0.end_tag}&quot;.format(self) def display(self, file=None): print(self, file=file) . With the Tag class created, we can now use Inheritance to create the following classes: DocType, Head and Body. . The DocType class will need to adjust the use of Tag slightly. Using super() to inherit the Tag methods, we specify name as our doctype, leaving contents as a blank string. We will also have to overwrite the end_tag to a blank string to construct the DocType correctly. . class DocType(Tag): def __init__(self): super().__init__(&#39;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.01//EN&quot; http://www.w3.org/TR/html4/strict.dtd&#39;, &#39;&#39;) . Next, we will create our Head class. Here, we will again utilize super() to inherit the Tag class, but we will include the ability to add a title. We will initialize title equal to None and include an if statement to include the information if provided. . class Head(Tag): def __init__(self, title=None): super().__init__(&#39;HEAD&#39;, &#39;&#39;) if title: self._title_tag = Tag(&#39;TITLE&#39;, title) self.contents = str(self._title_tag) . We will continue on, creating our Body class. Here, we will be including an add_tag method to provide the ability to include a variety of tags to the Body. This will be done by appending the new_tag to the _body_contents list. Additionally, we will have to overwrite the display method as now, we will have to loop through the _body_contents list to print. . This is a good place for a quick aside on the leading _ that is used throughout the code. By including a leading underscore, such as with _body_contents, Python does not display this as a potential object to be called. It is still accessible if the user enters the full object name, but by not displaying it as a possible object, it reduces the likelihood that a user will directly change this object. . class Body(Tag): def __init__(self): super().__init__(&#39;BODY&#39;, &#39;&#39;) self._body_contents = [] def add_tag(self, name, contents): new_tag = Tag(name, contents) self._body_contents.append(&quot; n t&quot;) self._body_contents.append(new_tag) def display(self, file=None): for tag in self._body_contents: self.contents += str(tag) super().display(file=file) . Lastly, we will combine all these to create our HtmlDoc class, which will combine the use of Composition and Aggregation. First we will define our class as needing two inputs, head and body. These will be instantiated and defined by the user and then fed into the HtmlDoc object. The use of a created class as an input into another class is known as Aggregation. Composition is when you use another class during the construction of the class, as we will do with the DocType() class. We set the doc_type this way as it is unlikely that we will need to amend the contents of DocType so we can hard-code it into the HtmlDoc construction and away from the user. . We have added the add_tag method which allows you to add to Body of the document if you&#39;d like. And created an expanded Display method to produce our final code. . class HtmlDoc(object): def __init__(self, head, body): self._doc_type = DocType() self._head = head self._body = body def add_tag(self, name, contents): self._body.add_tag(name, contents) def display(self, file=None): self._doc_type.display(file=file) print(&#39;&lt;HTML&gt;&#39;, file=file) self._head.display(file=file) self._body.display(file=file) print(&#39;&lt;/HTML&gt;&#39;, file=file) . Now that we&#39;ve created all the necessary classes, let&#39;s test it out. We will display the text here, as well as create a &#39;test.html&#39; file, which we can open in our browser to confirm everything is working. . We include the . if __name__ == &#39;__main__&#39;: . to ensure that if this module was imported into another one, this code isn&#39;t run. . if __name__ == &#39;__main__&#39;: new_header = Head(&#39;HTML Test Document&#39;) new_body = Body() new_body.add_tag(&#39;h1&#39;, &#39;HTML Code&#39;) new_body.add_tag(&#39;h2&#39;, &#39;Using Python OOP&#39;) new_body.add_tag(&#39;p&#39;, &#39;This code uses Python OOP to create HTML Code&#39;) new_body.add_tag(&#39;p&#39;, &#39;The code utulizes the concepts of Inheritance, Composition and Aggregation&#39;) new_page = HtmlDoc(new_header, new_body) new_page.display() with open(&#39;test.html&#39;, &#39;w&#39;) as test_doc: new_page.display(file=test_doc) . &lt;!DOCTYPE HTML PUBLIC &#34;-//W3C//DTD HTML 4.01//EN&#34; http://www.w3.org/TR/html4/strict.dtd&gt;&lt;/!DOCTYPE HTML PUBLIC &#34;-//W3C//DTD HTML 4.01//EN&#34; http://www.w3.org/TR/html4/strict.dtd&gt; &lt;HTML&gt; &lt;HEAD&gt;&lt;TITLE&gt;HTML Test Document&lt;/TITLE&gt;&lt;/HEAD&gt; &lt;BODY&gt; &lt;h1&gt;HTML Code&lt;/h1&gt; &lt;h2&gt;Using Python OOP&lt;/h2&gt; &lt;p&gt;This code uses Python OOP to create HTML Code&lt;/p&gt; &lt;p&gt;The code utulizes the concepts of Inheritance, Composition and Aggregation&lt;/p&gt;&lt;/BODY&gt; &lt;/HTML&gt; . OK. Everything looks good. We could do a little work on the spacing to make everything more readable, but this is simply a cosmetic issue, as HTML does not use new lines or tabs in compliing the code. So, all-in-all, we learn a little Object-Oriented-Programming concepts while learning some HTML along the way. . References: . Learn Python Programming Masterclass - Udemy - Tim Buchalka | The Global Structure of an HTML document - https://www.w3.org/TR/html401/struct/global.html | .",
            "url": "https://scottknappnj.github.io/FastPagesBlog/2020/12/03/Using-Python-OOP-to-Create-HTML.html",
            "relUrl": "/2020/12/03/Using-Python-OOP-to-Create-HTML.html",
            "date": " • Dec 3, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Equal Weighted vs Value Weighted Portfolio Construction",
            "content": "import pandas as pd import numpy as np from scipy.stats import norm import matplotlib.pyplot as plt %matplotlib inline . . The Data . Dartmouth College, through their Fama-French Data Library, offers an extensive array of raw data and factor portfolios going back to 1926. We will be utilizing the 49 Industry Portfolio dataset, analzying monthly data from January, 2000 to September, 2020. This time period will encompass 3 full market cycles, which will enable a more robust analysis of the various allocation methodologies. . The data can be downloaded here: https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html . We wil begin by importing monthly return data for both the value-weighted and equal-weighted company portfolios. These portfolios denote the weighting of the individual company in each industry. For example, if there are 44 firms in the Cnstr industry, the equal-weighted portfolio will assume an allocation of $ frac{1}{44}$, or more generally $ frac{1}{N}$, to each company. . Additionally, we will import the datasets for both the average firm size in each industry and the number of firms in each industry to be able to calculate a cap-weighted (value-weighted) index. . (This is not to be confused with the above. First, we separate how each industry is weighted to each company; then we determine how we allocate to each industry; value vs equal). | . m_vw_rets = pd.read_csv(&#39;data/ind49_m_vw_rets.csv&#39;, header=0, index_col=0, parse_dates=True) / 100 # convert the index to equal the date, for time-series analysis m_vw_rets.index = pd.to_datetime(m_vw_rets.index, format=&quot;%Y%m&quot;).to_period(&#39;M&#39;) # eliminate white space in column names for easier indexing m_vw_rets.columns = m_vw_rets.columns.str.strip() m_vw_rets = m_vw_rets[&quot;2000&quot;:] . The Industries that this dataset uses are: . m_vw_rets.columns . Index([&#39;Agric&#39;, &#39;Food&#39;, &#39;Soda&#39;, &#39;Beer&#39;, &#39;Smoke&#39;, &#39;Toys&#39;, &#39;Fun&#39;, &#39;Books&#39;, &#39;Hshld&#39;, &#39;Clths&#39;, &#39;Hlth&#39;, &#39;MedEq&#39;, &#39;Drugs&#39;, &#39;Chems&#39;, &#39;Rubbr&#39;, &#39;Txtls&#39;, &#39;BldMt&#39;, &#39;Cnstr&#39;, &#39;Steel&#39;, &#39;FabPr&#39;, &#39;Mach&#39;, &#39;ElcEq&#39;, &#39;Autos&#39;, &#39;Aero&#39;, &#39;Ships&#39;, &#39;Guns&#39;, &#39;Gold&#39;, &#39;Mines&#39;, &#39;Coal&#39;, &#39;Oil&#39;, &#39;Util&#39;, &#39;Telcm&#39;, &#39;PerSv&#39;, &#39;BusSv&#39;, &#39;Hardw&#39;, &#39;Softw&#39;, &#39;Chips&#39;, &#39;LabEq&#39;, &#39;Paper&#39;, &#39;Boxes&#39;, &#39;Trans&#39;, &#39;Whlsl&#39;, &#39;Rtail&#39;, &#39;Meals&#39;, &#39;Banks&#39;, &#39;Insur&#39;, &#39;RlEst&#39;, &#39;Fin&#39;, &#39;Other&#39;], dtype=&#39;object&#39;) . m_ew_rets = pd.read_csv(&#39;data/ind49_m_ew_rets.csv&#39;, header=0, index_col=0, parse_dates=True) / 100 m_ew_rets.index = pd.to_datetime(m_ew_rets.index, format=&quot;%Y%m&quot;).to_period(&#39;M&#39;) m_ew_rets.columns = m_ew_rets.columns.str.strip() m_ew_rets = m_ew_rets[&quot;2000&quot;:] . We have imported and cleaned the returns data for both equal-weighted company portfolios and value-weighted portfolios. Next we will bring in the average firm size and number of firms, so we can create a total-market value-weighted portfolio for both value-weighted industries and equal-weighted industries. . ind_size = pd.read_csv(&#39;data/ind49_m_size.csv&#39;, header=0, index_col=0, parse_dates=True) ind_size.index = pd.to_datetime(ind_size.index, format=&quot;%Y%m&quot;).to_period(&#39;M&#39;) ind_size.columns = ind_size.columns.str.strip() ind_size = ind_size[&quot;2000&quot;:] ind_nfirms = pd.read_csv (&#39;data/ind49_m_nfirms.csv&#39;, header=0, index_col=0, parse_dates=True) ind_nfirms.index = pd.to_datetime(ind_nfirms.index, format=&quot;%Y%m&quot;).to_period(&#39;M&#39;) ind_nfirms.columns = ind_nfirms.columns.str.strip() ind_nfirms = ind_nfirms[&quot;2000&quot;:] . In order to create the total-market value-weighted portfolios for each industry-weighted portfolio, we will write a quick function to do the calculation. Then we will create those portfolios and combine all 4 into a DataFrame . def value_weighted_returns(ind_returns, ind_size, ind_nfirms): # Calculate the market cap for each industry ind_mktcap = ind_size * ind_nfirms # Colculate the total market cap for all industries total_mktcap = ind_mktcap.sum(axis=&quot;columns&quot;) # Calculate the weighting of each industry in the total market cap ind_cap_wgt = ind_mktcap.divide(total_mktcap, axis = &quot;rows&quot;) # Calcualte the total market return for each period total_market_return = (ind_cap_wgt * ind_returns).sum(axis=&quot;columns&quot;) return total_market_return . m_vw_vw_rets = value_weighted_returns(m_vw_rets, ind_size, ind_nfirms) # Calculate the value-weighted portfolio market returns for the equal-weighted industries m_vw_ew_rets = value_weighted_returns(m_ew_rets, ind_size, ind_nfirms) . Calculating the equal-weighted industry portfolios is simply and average return across all industries. We will make that calculation here. . m_ew_vw_rets = m_vw_rets.mean(axis=&quot;columns&quot;) # Calculate the equal-weighted portfolios returns for the equal-weigthed industries m_ew_ew_rets = m_ew_rets.mean(axis=&quot;columns&quot;) . returns = pd.DataFrame({ &quot;Value-Weighted - EW Port&quot;: m_vw_ew_rets, &quot;Value-Weighted - VW Port&quot;: m_vw_vw_rets, &quot;Equal-Weighted - EW Port&quot;: m_ew_ew_rets, &quot;Equal-Weighted - VW Port&quot;: m_ew_vw_rets, }) returns . Value-Weighted - EW Port Value-Weighted - VW Port Equal-Weighted - EW Port Equal-Weighted - VW Port . 2000-01 0.070894 | -0.040130 | 0.044173 | -0.033673 | . 2000-02 0.188816 | 0.019456 | 0.090876 | -0.021078 | . 2000-03 -0.005319 | 0.074055 | 0.022084 | 0.076998 | . 2000-04 -0.113548 | -0.045298 | -0.056082 | -0.003986 | . 2000-05 -0.070897 | -0.030830 | -0.040545 | -0.016059 | . ... ... | ... | ... | ... | . 2020-05 0.085765 | 0.055291 | 0.069798 | 0.055661 | . 2020-06 0.069212 | 0.023037 | 0.073351 | 0.018688 | . 2020-07 0.057484 | 0.058037 | 0.060947 | 0.060033 | . 2020-08 0.051738 | 0.077836 | 0.051535 | 0.065782 | . 2020-09 -0.029560 | -0.036431 | -0.023606 | -0.017476 | . 249 rows × 4 columns . Next, we will create some summary statistics to be to compare these portfolios. . Annualized Returns: the compounded annualized return over the period. . $(1 + R_{t,t+1}) ^{n} - 1$ | . | Annualized Vol: the annualized standard deviation over the period . $ sigma_R = sqrt{ frac{1}{N} sum_{i=1}^N(R_i - bar{R})^2} $ | . | Sharpe Ratio: measures a unit of excess return over of the risk-free rate for each additional unit of risk. . $ text{Sharpe Ratio} = frac{Return - Risk Free Rate}{Volatility} $ | . | Max Drawdown: shows the largest percentage drop in a portfolio from a previous high valuation. . | Skewness: measures the distortion from a normal distribution . $S(R) = frac{E[(R - E(R))^3]}{[Var(R)^{3/2}]}$ | . | Kurtosis: measures the thickness of the tails as compared to a normal distribution . $K(R) = frac{E[(R - E(R))^4]}{[Var(R)^{2}]}$ | . | Histroic VaR (5%): represents the level in which 5% of historical period losses were greater than . | Cornish-Fisher VaR: parametric calculation of Value-at-Risk, which adjusts for the skewness and kurtosis of a distribution . $ tilde{z_a} = z_a + frac{1}{6}(z_a^2 - 1)S + frac{1}{24}(z_a^3 - 3Z_a)(K-3) - frac{1}{36}(2z_a^3 - 5Z_a)S^2$ | . | . def annualize_rets(returns, periods_per_year=12): # compound each years&#39; return at 1+r compounded_growth = (1+returns).prod() # calculate the number of periods in ind_returns n_periods = returns.shape[0] return compounded_growth ** (periods_per_year / n_periods) - 1 def annualize_stdev(returns, periods_per_year=12): return returns.std() * np.sqrt(periods_per_year) def sharpe_ratio(returns, risk_free_rate=0, periods_per_year=12): # calculate the per period risk_free_rate rf_per_period = (1+risk_free_rate) ** (1/periods_per_year) - 1 # calculate the excess return excess_ret = returns - rf_per_period # annualize the excess return ann_ex_ret = annualize_rets(excess_ret, periods_per_year) # calculate the annual volatility ann_sd = annualize_stdev(returns, periods_per_year) return ann_ex_ret / ann_sd def max_drawdown(returns): # calculate the accumulated growth at each period compounded_growth = (1+returns).cumprod() # calculate the previous peak value at each period previous_peaks = compounded_growth.cummax() # calculate the drawdowns at each period drawdowns = (compounded_growth - previous_peaks) / previous_peaks return -drawdowns.min() def skewness(returns): # calculate each period&#39;s return difference from the average return demeaned_r = returns - returns.mean() # calculate the standard devistion of the portfolio sigma_r = returns.std(ddof=0) # using ddof=0, to calculate population standard deviation # caluclate the numerator in the equation exp = (demeaned_r**3).mean() return exp / sigma_r**3 def kurtosis(returns): # calculate each period&#39;s return difference from the average return demeaned_r = returns - returns.mean() # calculate the standard devistion of the portfolio sigma_r = returns.std(ddof=0) # using ddof=0, to calculate population standard deviation # caluclate the numerator in the equation exp = (demeaned_r**4).mean() return exp / sigma_r**4 def var_historic(returns, level=5): return -np.percentile(returns, level) def var_cornish_fisher(returns, level=5): # compute the Z score assuming it was Gaussian z = norm.ppf(level/100) # compute the skewness s = skewness(returns) # compute the kurtosis k = kurtosis(returns) # compute the adjusted Z score z = (z + (z**2 - 1) * s/6 + (z**3 - 3*z) * (k-3)/24 - (2*z**3 - 5*z) * (s**2)/36 ) return -(returns.mean() + z * returns.std(ddof=0)) def summary_stats(returns, periods_per_year=12, risk_free_rate=0.02): summary_df = pd.DataFrame({ &quot;Annualized Return&quot;: returns.aggregate(annualize_rets, periods_per_year=periods_per_year), &quot;Annualized Vol&quot;: returns.aggregate(annualize_stdev, periods_per_year=periods_per_year), &quot;Sharpe Ratio&quot;: returns.aggregate(sharpe_ratio, risk_free_rate=risk_free_rate, periods_per_year=periods_per_year), &quot;Max Drawdown&quot;: returns.aggregate(max_drawdown), &quot;Skewness&quot;: returns.aggregate(skewness), &quot;Kurtosis&quot;: returns.aggregate(kurtosis), &quot;Historic 5% VaR&quot;: returns.aggregate(var_historic), &quot;CF 5% VaR&quot;: returns.aggregate(var_cornish_fisher) }) return summary_df . summary_stats(returns) . Annualized Return Annualized Vol Sharpe Ratio Max Drawdown Skewness Kurtosis Historic 5% VaR CF 5% VaR . Value-Weighted - EW Port 0.091434 | 0.216458 | 0.323901 | 0.583520 | -0.067811 | 4.919207 | 0.093561 | 0.092089 | . Value-Weighted - VW Port 0.066911 | 0.154981 | 0.297115 | 0.500088 | -0.568248 | 3.966089 | 0.078383 | 0.073090 | . Equal-Weighted - EW Port 0.101640 | 0.207453 | 0.386287 | 0.598352 | -0.271297 | 5.841772 | 0.087737 | 0.089501 | . Equal-Weighted - VW Port 0.088844 | 0.165339 | 0.408761 | 0.528351 | -0.642084 | 5.549245 | 0.079556 | 0.075951 | . Additionally, let&#39;s take a look at the growth of these portfolios graphically. First, we will create a cumulative returns DataFrame and then plot that. . cum_returns = (1+returns).cumprod() cum_returns.plot(figsize=(14,6)) . &lt;AxesSubplot:&gt; . &lt;Figure size 1008x432 with 0 Axes&gt; . Takeaways . So, what can we take away from these tables. . Equal-Weighted Company portfolio vs Value-Weighted Company Portfolios. . It appears that for both industry weighting schemes; value-weight (9.14% vs 6.69%), equal-weight (10.16 vs 8.88%), the equally weighted company portfolios outperformed the value-weighted ones. This may simply be explained to the small-cap vs large-cap bias. And that can be seen in the increased volatility exhibited (21.65% vs 15.49% and 20.74 vs 16.53%) and increased Max Drawdowns (58.3% vs 50.0% and 59.8% vs 52.8%). And further looking at the Sharpe Ratio of these portfolios it is not clear, what the best company weighting is, as there seems to be a clear trade-off of risk vs return. . Equal-Weighted Industry portfolio vs Value-Weighted Industry Portfolios. . Here, while holding the company-weighting constant, it is clear that equal-weighting the industries outperforms the value-weighted industry portfolios (10.16% vs 9.14% and 8.88% vs 6.69%). And this is accomplished with little change in the volatility of the portfolios (20.74% vs 21.64% and 16.53% vs 15.50%). This results in dramatically improved Sharpe Ratios (.38 vs .32 and 0.40 vs 0.30). . Overall, from a Sharpe Ratio perspective, the portfolio that performed best over this period and construction schemes was the Equal-Weighted Industry and Value-Weighted Company portfolio, with a Sharpe Ratio of 0.40. .",
            "url": "https://scottknappnj.github.io/FastPagesBlog/2020/12/01/Equal-vs-Value-Weighted-Portfolios.html",
            "relUrl": "/2020/12/01/Equal-vs-Value-Weighted-Portfolios.html",
            "date": " • Dec 1, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Understanding Gradient Descent",
            "content": "!pip install -Uqq fastbook import fastbook fastbook.setup_book() . . from fastai.vision.all import * from fastbook import * matplotlib.rc(&#39;image&#39;, cmap=&#39;Greys&#39;) . . In recent years, with the increase in computing speed, increaed access to GPUs (Graphics Processing Unit) and enhanced machine learning libraries, Gradient Descent has become an integral part of many machine learning and deep learning processes, especially those associated with Neural Networks. . We will begin by defining what Gradient Descent is, why it is useful and how it is used in practice. We will illustrate this with a simple example from the fastai course and book Deep Learning for Coders with fastai and PyTorch. . What is Gradient Descent? . Let&#39;s start by defining what a gradient is. If you remember back to high school calculus, a derivative of a function calculates the rate of change of that function at a specific point, its&#39; rise over run. It tells you how much the output changes if you change the inputs a tiny amount. . Well, when you have a bunch of variables in an equation, you will need to know how much the output changes if you change each of these input variables a tiny bit. This is done by calculating the partial derivatives with respect to each of the variables. And the gradient is the vector which basically stores all of this information. . Gradient Descent is simply the algorithm that uses this information to update a model&#39;s parameters a little at a time improving its&#39; performance. . A Simple Example . Let&#39;s look at an example of the fictional equation of the speed of a roller coaster going over a hill for a period of time, 20. Using PyTorch&#39;s arange method we can create a sequence from 0-19 for this example. . time = torch.arange(0, 20).float() time . tensor([ 0., 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19.]) . We will then simply create a random function which has the speed decreasing to 0 at the midpoint and increasing again after. We will also add some random noise using randn. And we will graph what the function looks like. . speed = torch.randn(20)*2 + 0.6*(time-9.5)**2 + 1 plt.scatter(time, speed); . Next, we will estimate that the function is a quadratic in the form $ a t^2 + bt + c$ and we will look to solve for a, b, c. Let&#39;s go ahead on define this function. . def f(t, params): a, b, c = params return a*(t**2) + (b*t) + c . How will we know if our estimated paramters are good or not? We need to create a loss function that calculates the error rate of our predictions against the actual data. In our case, we will use a simple Mean Squared Error loss function. To calculate this, we simply: . 1) Calculate the difference between our predictions and our target values 2) Square the difference 3) Take a mean of the Squared differences. . This is the loss function that we will use our Gradient Descent algorithm to minimize. . def mse(preds, target): return ((preds - target)**2).mean() . Seven-Step Process . Now, we begin the process of Gradient Descent, which involves seven-steps. . 1) Initialize the parameters 2) Calculate the predictions 3) Calculate the loss 4) Calculate the gradients 5) Step the weights 6) Repeat steps 2) through 5) 7) Stop the process . Step 1: Initialize the Parameters . We begin by setting the initial values for our parameters. It has been proven that using random initial parameters works as well as any other process, so that&#39;s what we will do here utilizing randn again. . We will also add the method .requires_grad_(). This tells PyTorch to remember the necessary information so that we can later calculate the gradients with respect to these parameters. . params = torch.randn(3).requires_grad_() . Step 2: Calculate the Predictions . In this step, we will calculate the predictions based on our current parameter estimates. We will also write a quick function to view our targets results along with our current estimates. . preds = f(time, params) . def show_preds(preds, ax=None): if ax is None: ax=plt.subplots()[1] ax.scatter(time, speed) ax.scatter(time, to_np(preds), color=&#39;red&#39;) # to_np() converts a tensor to an array ax.set_ylim(-300, 100) show_preds(preds) . These predictions look pretty terrible, but we did just start with random parameters. Hopefully, our Gradient Descent algorithm will help improve these results . Step 3: Calculate the loss . In this step, we will simply calcualte the loss using our mse function that we previously created. As you can see from the output, the information for the gradient is also created along with the result. . loss = mse(preds, speed) loss . tensor(24176.3906, grad_fn=&lt;MeanBackward0&gt;) . Step 4: Calculate the gradients . In this step, we will calculate the gradients. And thankfully, we won&#39;t have to do this calcuation manually, PyTorch does this all on its&#39; own. And as you can see from the results, it returns the gradient for each of our parameters. . loss.backward() params.grad . tensor([-51698.6758, -3324.1123, -243.4835]) . Step 5: Step the parameters . Here, we will update the parameters of our model, but how? From the last step we know how much our loss function will change as we change our parameters, but how much should we adjust them by? If we adjust our parameters too fast, we run the risk of overshooting our optimal solution and simply bouncing back and forth and never converging to an answer. If we adjust our parameters too slowly, it will take a long time to converge to an optimal solution. Additionally, we may be fooled if there are local minimums in the function and if we see the loss going back up, we may interpret that as an incorrect global minimum. . The number that we use as a scalar of the gradients is called the learning rate. There is no answer as to what the correct learning rate is, this can be a tunable parameter in the model, meaning we can try a few different rates and choose the best performing one. For this purpose, we will set our learning rate to 1e-5. . Another step with we need to do after adjusting the parameters is to zero-out the gradients for our next iteration, as PyTorch we simply add any calculated gradients to the existing ones, which we don&#39;t want. . lr = 1e-5 params.data -= lr * params.grad.data params.grad = None . Step 6: Repeat the process . Here, we will go back to step #2 and #3. We will make the predictions with our new parameters and calculate the loss and visualize our new predictions. . preds = f(time, params) mse(preds, speed) . tensor(4919.5947, grad_fn=&lt;MeanBackward0&gt;) . show_preds(preds) . We&#39;ve improved our Mean Squared Error significantly. Not too bad, for only updating the parameters once. Let&#39;s write a function including everything we&#39;ve done so far, so we can repeat the process a few more times. . def apply_step(params, prn=True): preds = f(time, params) loss = mse(preds, speed) loss.backward() params.data -= lr * params.grad.data params.grad = None if prn: print(loss.item()) return preds . for i in range(10): apply_step(params) . 4919.5947265625 1275.6175537109375 586.0635986328125 455.57635498046875 430.88177490234375 426.20635986328125 425.319091796875 425.14862060546875 425.114013671875 425.10498046875 . Step 7: Stop the process . We have decided to stop after an additional 10 epochs arbitrarily. In practice, we would likely watch the loss function and our metrics to decide when to step. But here we have seen our Mean Squared Error to a stable amount in only 5-6 epochs. . an epoch is the number of times our algorithm works through all of our data. | . Summary . From this simple example, I think you can begin to see the power of using the Gradient Descent algorithm to update model parameters to improve performance. .",
            "url": "https://scottknappnj.github.io/FastPagesBlog/2020/11/29/Understanding-Gradient-Descent.html",
            "relUrl": "/2020/11/29/Understanding-Gradient-Descent.html",
            "date": " • Nov 29, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://scottknappnj.github.io/FastPagesBlog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://scottknappnj.github.io/FastPagesBlog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Since you are here reading my blog, I figured I’d tell you a little about how I began this journey into Data Science. My entire career has been spent in finance arena with the first half in the Interest Rate Derivative world. That world of swaps and swaptions, caps and floors, and anything else which would cause an unassuming questioner’s eyes to glaze over after innocently asking ‘So, what do you do?’ I spent about a dozen years managing interest rate derivative portfolios and pricing cutting-edge structured products for clients to help them hedge their interest rate exposure. It was an interesting place to be, for an Accounting major, who’s last math class was Multivariate Calculus, freshmen year of college and without any programming background. And as interested as I was to dive into the math and coding, back in the late 1990’s / early 2000’s, the paths to learning higher-levels of math, programming skills and financial modeling weren’t as easily available as today. As a result, I developed very strong intutions of how models behave by taking a top-down approach to financial modeling which served as a very nice balance to the PhD math/physics/engineering backgrounds of the quantitative modeling team which tended to look at everything from a bottom-up approach. But even with this lack of technical skills, I still was able to develop the ability to identify cutting-edge modeling; implementing the SABR (Stochastic Alpha-Beta-Rho) option model, early after the original paper was published and developed new derivative solutions to hedge mortgage pre-payment risk, dubbed ‘Balanced-Guaranteed’ products. . After the 2008/09 financial crisis, there was a huge shift in the banking industry in the ability to manage risk, meaning they no longer did, banks simply hedged it away. This meant a huge contraction in the industry, leaving most positions to be filled by traders with quantitative backgrounds, who could automate trade execution and risk management. This led me to move away from the markets and to work with my wife for a few years as we tried to start a franchise business, and then to me taking my experience in the finanical markets and starting my own Financial Planning practice. . The old saying, ‘you don’t know what you don’t know’ was never more true, and that first year was filled with starts and stops as I learned the industry and redefined what type of practice I wanted to create. The end result was to be a planning-first practice, which was product and company agnostic, and used deep comprehensive planning in a variety of areas (Retirement, Investment, Tax, Insurance and Estate) to drive recommendations for the clients. Over the next few years, I developed a great deal of knowledge in all of those areas and experience some levels of success and acquired my CFA certification, but the point came where an unfortunate realization had to be made. This realization was that success in the financial advisory arena, is not determined by the knowledge of your craft or the quality of the work you perform, but how well you can sell. And while I consider myself a very good planner, and a good relatoionship manager, if I’m being honest I am a mediocre sales person. Then the opportunity arose to sell my practice and I had to make that difficult choice of selling. Then Covid hit. . While job-seeking became difficult, this gave me the opporunity to dive head first into learning to code with a focus on Data Science. It began with the 10-course specialization offered by John Hopkins through Coursera. This course is taught in the R language, which for me was a great initital foray to learning programming while dipping my toes into what Data Science encompasses. From that I’ve taken a bunch of other online courses, focusing on math: (Statistics, Linear Algebra, Calculus), coding (R, Python, SQL) and in machine learning (ISLR, NLP, deep learning, etc.). Additionally, I’ve sprinkled in some Sustainable Investing education, taking a v ariety of courses offered by PRI (Principles for Responsible Investment) and being the first class to earn the Sustainability and Climate Risk certification offered by GARP. . Now, I’m focusing on a few things. I’m working through the Deep Learning with fastai book/course, still brushing on some Linear Algebra/Calculus and beginning to put together some projects. These projects will be a culmination of all the different skills I’ve developed over the past few months, focusing on some of the specific applications that I’ve learned in a recent course on Coursera, Investment Management with Python. There will also be other shorter blog posts about things I’m currently learning. . It hasn’t been a straight line, but I do think I’m beginning to converge on the correct path. And in the end, hopefully this will not all be for nought. Obviously, the near-term goal of all of this is to be able to find a role to in the Data Science world, that I can transition into and best utilize my extensive financial background while continuing to develop these newly acquired skills. . I hope you enjoy reading about my journey. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://scottknappnj.github.io/FastPagesBlog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://scottknappnj.github.io/FastPagesBlog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}