{
  
    
        "post0": {
            "title": "Global Minimum Variance Portfolios",
            "content": "Background . Continuing our discussions around portfolio construction, we will begin to build from the Efficient Frontier to the Global Minimum Variance (GMV) Portfolio. . Let&#39;s define what the GMV is: it is simply the portfolio along the frontier that has the minumum portfolio volatility. We will look at how this porfolio is constructed, how to improve this construction and how it performs over time. . # Setup / Imports import numpy as np import pandas as pd from pandas_datareader import data import matplotlib.pyplot as plt import seaborn as sns from scipy.optimize import minimize from scipy.stats import norm np.set_printoptions(suppress=True) pd.set_option(&#39;display.float_format&#39;, lambda x: &#39;%.5f&#39; % x) %matplotlib inline print(&quot;Setup Complete&quot;) . . Setup Complete . The Data . We will utilize the 9 sector ETFs from State Street to construct our GMV portfolio. We will compare these results to the cap-weighted &#39;SPY&#39; as well as to the EW allocation across all 9 ETFs. Here are the sector ETFs: . XLY: Consumer Discretionary Sector | XLP: Consumer Staples Sector | XLE: Energy Sector | XLF: Financial Sector | XLV: Health Care Sector | XLI: Industrial Sector | XLB: Materials Sector | XLK: Technology Sector | XLU: Utilities Sector | . We will import this data from Yahoo Finance from the period running Jan 1, 2005 to Dec 31, 2019. Next, convert the price series to a return series and calculate the covariances and visualize the correlation matrix. Not surprisingly, all the sectors are highly correlated. . sectors = [&quot;XLY&quot;, &quot;XLP&quot;, &quot;XLE&quot;, &quot;XLF&quot;, &quot;XLV&quot;, &quot;XLI&quot;, &quot;XLB&quot;, &quot;XLK&quot;, &quot;XLU&quot;] sector_prices = data.DataReader(sectors, data_source=&quot;yahoo&quot;, start=&quot;1/1/2005&quot;, end=&quot;12/31/2019&quot;)[&quot;Adj Close&quot;] sector_prices.head() . Symbols XLY XLP XLE XLF XLV XLI XLB XLK XLU . Date . 2005-01-03 28.18542 | 15.36704 | 23.59226 | 13.66623 | 22.74287 | 22.35205 | 20.63189 | 16.73737 | 15.67110 | . 2005-01-04 27.85581 | 15.31377 | 23.47087 | 13.53595 | 22.55996 | 22.07628 | 20.26033 | 16.42549 | 15.56295 | . 2005-01-05 27.75130 | 15.23387 | 23.36970 | 13.51349 | 22.52185 | 21.92386 | 19.97991 | 16.36951 | 15.31818 | . 2005-01-06 27.60660 | 15.30710 | 23.77437 | 13.58087 | 22.68953 | 21.99644 | 20.11311 | 16.32153 | 15.39787 | . 2005-01-07 27.59856 | 15.38035 | 23.58552 | 13.50900 | 22.67428 | 21.89484 | 20.14817 | 16.35352 | 15.39787 | . sector_returns = sector_prices.pct_change() sector_returns.drop(sector_returns.index[0], axis=0, inplace=True) . sector_cov_mat = sector_returns.cov() *252 . plt.figure(figsize=(10, 8)) sns.heatmap(sector_returns.corr(), cmap=&#39;coolwarm&#39;, annot=True); . Calculate Competing Portfolios . Calculating GMV . In order to calculate the Global Minimum Variance portfolio, we will create a few functions. First, we will create a portfolio_vol function to implement this formula: $$ sigma^2(w_a, w_b) = sigma_A^2w_A^2 + sigma_B^2w_B^2 + 2w_Aw_B sigma_A sigma_B rho_{A,B} $$ . This summarizes to a matrix multiplication shown in the function below. . Next, we will define a function to calculate the GMV. In order to do this, we will utilize the minimize function from scipy.stats. minimize will solve for the optimal weights to minimize the volatility of the portfolio. In order to do this, we must specify: . init_guess: set the initial guess to equal weights | bounds: sets bounds for each weights, can be user-defined with min_alloc and max_alloc, default is 0, 1. | weights_sum_to_1: the sum of all the weights must add up to 1. | . Additionally, we will use Quadratic Programming to solve this optimization problem . def portfolio_vol(weights, cov_mat): &#39;&#39;&#39;Calculates the portfolio volatility given a set of weights and a covariance matrix&#39;&#39;&#39; return (weights.T @ cov_mat @ weights)**0.5 def gmv(rets, cov_mat, min_alloc=0., max_alloc=1.): &#39;&#39;&#39;Calculates the optimal weights for Global Minimize Variance portfolio for a set of securities Arguments: rets: pd.DataFrame of returns cov_mat: covariance_matrix for the securities min_alloc: minimum bound for the weights of each security; default=0. max_alloc: maxmimum bound for the wieghts of each security; default=1. Returns: pd.Series of the weights for the GMV &#39;&#39;&#39; # count the number of securities n = len(rets.columns) # set initial guess to equal weight init_guess = np.repeat(1/n, n) # set the bounds for each security to the min and max specified bounds = ((min_alloc, max_alloc), ) * n # create the constraint that all weights must sum to 1 weights_sum_to_1 = {&#39;type&#39;: &#39;eq&#39;, &#39;fun&#39;: lambda weights: np.sum(weights) - 1 } # perform the optimization to minimize portfolio_vol using &#39;SLSQP&#39; weights = minimize(portfolio_vol, init_guess, args=(cov_mat), method=&#39;SLSQP&#39;, options={&#39;disp&#39;: False}, constraints=(weights_sum_to_1,), bounds=bounds ) return pd.Series(weights.x, index=cov_mat.index) . Now that we have defined our functions we will create a rolling calcuation looking back 6 months to calculate the covariance matrix to define the weights. This will result in a time series of GMV weights, which we will utilize in our next step. We will assume a minimum allocation of 0% and a maximum of 33% for any individual sector. . After we create this time_series, we can plot the weighting scheme to identify changes over time. XLU, XLP and XLV appear to be consistently large allocatons to this portfolio over time. . Lastly, we will use these weights to calculate the period returns for the GMV portfolio. . estimation_window = 126 # Define the number of backtest periods backtest_periods = total_periods - estimation_window # create a list of tuples (start, end) for each estimaton window in the backtest period windows = [(start, start+estimation_window) for start in range(backtest_periods)] . rets = sector_returns # use list comprehension to create the weights for each window weights = [gmv(rets.iloc[win[0]:win[1]], (rets.iloc[win[0]:win[1]]).cov()*252, max_alloc=.33) for win in windows] # convert the list to a pd.DataFrame weights = pd.DataFrame(weights, index=rets.iloc[estimation_window:].index, columns=rets.columns) . weights.head() . Symbols XLY XLP XLE XLF XLV XLI XLB XLK XLU . Date . 2005-07-06 0.00000 | 0.33000 | 0.01105 | 0.04970 | 0.33000 | 0.00000 | 0.00000 | 0.03333 | 0.24591 | . 2005-07-07 0.00000 | 0.33000 | 0.00850 | 0.05007 | 0.33000 | 0.00000 | 0.00000 | 0.06183 | 0.21960 | . 2005-07-08 0.00000 | 0.33000 | 0.00509 | 0.03270 | 0.33000 | 0.00000 | 0.00000 | 0.06056 | 0.24165 | . 2005-07-11 0.00000 | 0.33000 | 0.01924 | 0.05751 | 0.33000 | 0.00000 | 0.00000 | 0.03214 | 0.23111 | . 2005-07-12 0.00000 | 0.33000 | 0.01960 | 0.05897 | 0.33000 | 0.00000 | 0.00000 | 0.02839 | 0.23304 | . weights.plot.area(figsize=(14,7)); . bt_gmv_rets = (weights * sector_returns).sum(axis=&#39;columns&#39;) . Calculating Equal-Weight returns . Next, we will create some functions to calculate the equal-weight portfolio returns over time. This will utilize similar functionality used above. . def weight_ew(rets): &#39;&#39;&#39;Returns an equal-weight pd.Series from a list of returns&#39;&#39;&#39; n = len(rets.columns) return pd.Series(1/n, index=rets.columns) def backtest(rets, estimation_window=36, weighting=weight_ew, **kwargs): &#39;&#39;&#39;Calculates returns of a weighting scheme over a defined period with a defined estimation window&#39;&#39;&#39; total_periods = rets.shape[0] backtest_periods = total_periods - estimation_window windows = [(start, start+estimation_window) for start in range(backtest_periods)] weights = [weighting(rets.iloc[win[0]:win[1]], **kwargs) for win in windows] weights = pd.DataFrame(weights, index=rets.iloc[estimation_window:].index, columns=rets.columns) returns = (weights * rets).sum(axis=&quot;columns&quot;, min_count=1) return returns . bt_ew_rets = backtest(sector_returns) . Calculating Cap-Weighted returns . Here we will simply import the prices for the &#39;SPY&#39; ETF and calculate the return series over the same time period. . cap_wgt_prices = data.DataReader(&quot;SPY&quot;, data_source=&quot;yahoo&quot;, start=&quot;1/1/2005&quot;, end=&quot;12/31/2019&quot;)[&quot;Adj Close&quot;] cap_wgt_rets = cap_wgt_prices.pct_change() cap_wgt_rets.drop(cap_wgt_rets.index[0], axis=0, inplace=True) . bt_ew_rets = backtest(sector_returns) . Evaluating the resutls. . Finally, we will compare the results. In order to do so, we have created a variety of functions in order to do so. Here are the metrics we will calculate: . Annualized Returns: the compounded annualized return over the period. . $(1 + R_{t,t+1}) ^{n} - 1$ | . | Annualized Vol: the annualized standard deviation over the period . $ sigma_R = sqrt{ frac{1}{N} sum_{i=1}^N(R_i - bar{R})^2} $ | . | Sharpe Ratio: measures a unit of excess return over of the risk-free rate for each additional unit of risk. . $ text{Sharpe Ratio} = frac{Return - Risk Free Rate}{Volatility} $ | . | Max Drawdown: shows the largest percentage drop in a portfolio from a previous high valuation. . | Skewness: measures the distortion from a normal distribution . $S(R) = frac{E[(R - E(R))^3]}{[Var(R)^{3/2}]}$ | . | Kurtosis: measures the thickness of the tails as compared to a normal distribution . $K(R) = frac{E[(R - E(R))^4]}{[Var(R)^{2}]}$ | . | Histroic VaR (5%): represents the level in which 5% of historical period losses were greater than . | Cornish-Fisher VaR: parametric calculation of Value-at-Risk, which adjusts for the skewness and kurtosis of a distribution . $ tilde{z_a} = z_a + frac{1}{6}(z_a^2 - 1)S + frac{1}{24}(z_a^3 - 3Z_a)(K-3) - frac{1}{36}(2z_a^3 - 5Z_a)S^2$ | . | . def annualize_rets(returns, periods_per_year=12): # compound each years&#39; return at 1+r compounded_growth = (1+returns).prod() # calculate the number of periods in ind_returns n_periods = returns.shape[0] return compounded_growth ** (periods_per_year / n_periods) - 1 def annualize_stdev(returns, periods_per_year=12): return returns.std() * np.sqrt(periods_per_year) def sharpe_ratio(returns, risk_free_rate=0, periods_per_year=12): # calculate the per period risk_free_rate rf_per_period = (1+risk_free_rate) ** (1/periods_per_year) - 1 # calculate the excess return excess_ret = returns - rf_per_period # annualize the excess return ann_ex_ret = annualize_rets(excess_ret, periods_per_year) # calculate the annual volatility ann_sd = annualize_stdev(returns, periods_per_year) return ann_ex_ret / ann_sd def max_drawdown(returns): # calculate the accumulated growth at each period compounded_growth = (1+returns).cumprod() # calculate the previous peak value at each period previous_peaks = compounded_growth.cummax() # calculate the drawdowns at each period drawdowns = (compounded_growth - previous_peaks) / previous_peaks return -drawdowns.min() def skewness(returns): # calculate each period&#39;s return difference from the average return demeaned_r = returns - returns.mean() # calculate the standard devistion of the portfolio sigma_r = returns.std(ddof=0) # using ddof=0, to calculate population standard deviation # caluclate the numerator in the equation exp = (demeaned_r**3).mean() return exp / sigma_r**3 def kurtosis(returns): # calculate each period&#39;s return difference from the average return demeaned_r = returns - returns.mean() # calculate the standard devistion of the portfolio sigma_r = returns.std(ddof=0) # using ddof=0, to calculate population standard deviation # caluclate the numerator in the equation exp = (demeaned_r**4).mean() return exp / sigma_r**4 def var_historic(returns, level=5): return -np.percentile(returns, level) def var_cornish_fisher(returns, level=5): # compute the Z score assuming it was Gaussian z = norm.ppf(level/100) # compute the skewness s = skewness(returns) # compute the kurtosis k = kurtosis(returns) # compute the adjusted Z score z = (z + (z**2 - 1) * s/6 + (z**3 - 3*z) * (k-3)/24 - (2*z**3 - 5*z) * (s**2)/36 ) return -(returns.mean() + z * returns.std(ddof=0)) def summary_stats(returns, periods_per_year=12, risk_free_rate=0.02): summary_df = pd.DataFrame({ &quot;Annualized Return&quot;: returns.aggregate(annualize_rets, periods_per_year=periods_per_year), &quot;Annualized Vol&quot;: returns.aggregate(annualize_stdev, periods_per_year=periods_per_year), &quot;Sharpe Ratio&quot;: returns.aggregate(sharpe_ratio, risk_free_rate=risk_free_rate, periods_per_year=periods_per_year), &quot;Max Drawdown&quot;: returns.aggregate(max_drawdown), &quot;Skewness&quot;: returns.aggregate(skewness), &quot;Kurtosis&quot;: returns.aggregate(kurtosis), &quot;Historic 5% VaR&quot;: returns.aggregate(var_historic), &quot;CF 5% VaR&quot;: returns.aggregate(var_cornish_fisher) }) return summary_df . After creating this summary statistics functions, we can combine the 3 competing return series in to a DataFrame and run the view the summary_stats. . The GMV has competitive returns to the other 2 portfolios, with significantly lower Annualized Vol and associated Max Drawdown and VaR calculations. Pretty impressive for a relative simple strategy. . Below we can see the visualize of these 3 portfolios. . returns = pd.DataFrame({ &quot;SPY&quot;: cap_wgt_rets[&quot;2005-08-01&quot;:], &quot;EW&quot;: bt_ew_rets[&quot;2005-08-01&quot;:], &quot;GMV&quot;: bt_gmv_rets[&quot;2005-08-01&quot;:] }) summary_stats(returns, periods_per_year=252, risk_free_rate=0.01) . Annualized Return Annualized Vol Sharpe Ratio Max Drawdown Skewness Kurtosis Historic 5% VaR CF 5% VaR . SPY 0.09056 | 0.18581 | 0.42927 | 0.55189 | 0.14777 | 19.23411 | 0.01775 | 0.01451 | . EW 0.09639 | 0.17980 | 0.47573 | 0.52531 | -0.15127 | 14.48108 | 0.01709 | 0.01606 | . GMV 0.09206 | 0.13515 | 0.60120 | 0.42073 | -0.07637 | 15.67896 | 0.01306 | 0.01162 | . (1+returns).cumprod().plot(figsize=(14, 8)); .",
            "url": "https://scottknappnj.github.io/FastPagesBlog/2020/12/13/GMV.html",
            "relUrl": "/2020/12/13/GMV.html",
            "date": " • Dec 13, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Identifying Recessions",
            "content": "Background and the Data . The ability to project an upcoming recession has been long sought after by government bodies (Federal Reserve, Dept. of Treasury), financial institutions (banks, investment firms and insurers), and other financial practitioners (academics). In the recent decades, a variety of advances has made this endeavor more achievable. The available of large datasets by a simple download from the internet, such as the Federal Reserve Economic Data (FRED) database, has democratized data collection away from a few select institutions that housed this data. The development of new sophisticated modeling techniques, which allow for significantly more indepenent predictors provides the ability to obtain more robust results. And finally the increase in computation speeds allows the ability to take advantage of the larger, more accessible datasets and increasely complex algorithms. . To assist in this regards, in 2015, Michael McCracken and Serena Ng at the St. Louis Federal Reserve, created the Macroecomic Database. This work was intended to build on the Stock-Watson datasets that were introduced in 1996. According to their working paper: &#39;The dataset mimics the coverage of those already used in the literature, but has three appealing features. . First, it is designed to be updated monthly using the FRED database. | Second, it will be publicly accessible, facilitating comparison of related research and replication of empirical work. | Third, it will relieve researchers from having to manage data changes and revisions.&#39; | . The data are classified into 8 categories: . 1) output and sale, 2) employment, 3) new orders, 4) inventories, 5) prices, 6) interest rates, 7) money and credit, 8) other variables. . Detail description of the variables under each category can be found in this appendix. . We will look this utilize this data to identify previous recessions by applying a variety of Classification algorithms, and then drill down deeper into a few of these. . # load libraries import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import datetime from statsmodels.tsa.stattools import adfuller #to check unit root in time series from sklearn import preprocessing from sklearn.preprocessing import StandardScaler from sklearn.metrics import classification_report from sklearn.metrics import confusion_matrix from sklearn.metrics import f1_score from sklearn.metrics import roc_auc_score, roc_curve, auc from sklearn.model_selection import TimeSeriesSplit from sklearn.model_selection import GridSearchCV from sklearn.model_selection import cross_val_score from sklearn.feature_selection import SelectFromModel from sklearn.linear_model import LogisticRegression from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.neighbors import KNeighborsClassifier from sklearn.svm import SVC from sklearn.ensemble import GradientBoostingClassifier from sklearn.ensemble import AdaBoostClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.tree import DecisionTreeClassifier #!python -m pip install xgboost import xgboost as xgb import os import warnings warnings.filterwarnings(&#39;ignore&#39;) pd.set_option(&#39;display.float_format&#39;, lambda x: &#39;%.5f&#39; % x) np.set_printoptions(suppress=True) print(&quot;Setup Completed Successfully&quot;) . . Setup Completed Successfully . Importing and Cleaning the Data . We will begin by reading in the data and performing some cleaning and transformation. The official recession period data has been complied by the National Bureasu of Economic Research, which we will merge with Macroeconomic Data, as Regime. . macro_data = pd.read_csv(&#39;./data/recessions/current.csv&#39;) recessions = pd.read_csv(&#39;./data/recessions/Recession_Periods_current.csv&#39;) macro_data.rename(columns={&#39;sasdate&#39;:&#39;Date&#39;}, inplace=True) macro_data.insert(loc=1, column=&quot;Regime&quot;, value=recessions[&#39;Regime&#39;].values) macro_data . Date Regime RPI W875RX1 DPCERA3M086SBEA CMRMTSPLx RETAILx INDPRO IPFPNSS IPFINAL ... DSERRG3M086SBEA CES0600000008 CES2000000008 CES3000000008 UMCSENTx MZMSL DTCOLNVHFNM DTCTHFNM INVEST VXOCLSx . 0 1/1/1959 | Normal | 2437.29600 | 2288.80000 | 17.30200 | 292258.83290 | 18235.77392 | 22.62500 | 23.45810 | 22.19040 | ... | 11.35800 | 2.13000 | 2.45000 | 2.04000 | nan | 274.90000 | 6476.00000 | 12298.00000 | 84.20430 | nan | . 1 2/1/1959 | Normal | 2446.90200 | 2297.00000 | 17.48200 | 294429.54530 | 18369.56308 | 23.06810 | 23.77470 | 22.38270 | ... | 11.37500 | 2.14000 | 2.46000 | 2.05000 | nan | 276.00000 | 6476.00000 | 12298.00000 | 83.52800 | nan | . 2 3/1/1959 | Normal | 2462.68900 | 2314.00000 | 17.64700 | 293425.38130 | 18523.05762 | 23.40040 | 23.91860 | 22.49250 | ... | 11.39500 | 2.15000 | 2.45000 | 2.07000 | nan | 277.40000 | 6508.00000 | 12349.00000 | 81.64050 | nan | . 3 4/1/1959 | Normal | 2478.74400 | 2330.30000 | 17.58400 | 299331.65050 | 18534.46600 | 23.89890 | 24.26410 | 22.82210 | ... | 11.43600 | 2.16000 | 2.47000 | 2.08000 | nan | 278.10000 | 6620.00000 | 12484.00000 | 81.80990 | nan | . 4 5/1/1959 | Normal | 2493.22800 | 2345.80000 | 17.79600 | 301372.95970 | 18679.66354 | 24.25890 | 24.46550 | 23.04180 | ... | 11.45400 | 2.17000 | 2.48000 | 2.08000 | 95.30000 | 280.10000 | 6753.00000 | 12646.00000 | 80.73150 | nan | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 737 6/1/2020 | Recession | 18070.39300 | 13600.80000 | 114.88000 | 1515750.00000 | 529962.00000 | 97.60320 | 94.05100 | 93.15700 | ... | 119.83800 | 25.38000 | 29.32000 | 22.61000 | 78.10000 | 20979.80000 | 334036.91000 | 720036.14000 | 4171.01460 | 30.07440 | . 738 7/1/2020 | Recession | 18185.08700 | 13799.50000 | 116.22600 | 1549627.00000 | 535923.00000 | 101.74990 | 98.17580 | 97.83440 | ... | 120.07000 | 25.44000 | 29.30000 | 22.81000 | 72.50000 | 21080.10000 | 340151.21000 | 724516.67000 | 4291.17980 | 24.37850 | . 739 8/1/2020 | Recession | 17675.11900 | 13969.10000 | 117.28500 | 1554138.00000 | 543404.00000 | 102.48760 | 99.22330 | 98.82590 | ... | 120.37600 | 25.48000 | 29.38000 | 22.83000 | 74.10000 | 21115.10000 | 344417.03000 | 727095.20000 | 4347.74300 | 20.27770 | . 740 9/1/2020 | Recession | 17778.66900 | 14107.50000 | 118.52200 | 1560211.00000 | 551934.00000 | 102.10340 | 98.57940 | 98.03330 | ... | 120.77800 | 25.47000 | 29.07000 | 23.01000 | 80.40000 | 21284.50000 | 344813.77000 | 727399.62000 | 4406.14200 | 28.32410 | . 741 10/1/2020 | Recession | 17662.21100 | 14217.20000 | 119.09800 | nan | 553329.00000 | 103.20540 | 99.67770 | 98.83540 | ... | 120.90500 | 25.58000 | 29.36000 | 23.00000 | 81.80000 | 21403.60000 | nan | nan | 4498.23840 | 30.94080 | . 742 rows × 130 columns . macro_data[[&#39;Date&#39;, &#39;Regime&#39;]].groupby(&#39;Regime&#39;).count() . Date . Regime . Normal 640 | . Recession 102 | . Dealing with Missing Values . We will need to address any missing values in the data. From the heatmap below, we can see that a few columns have many missing values from the beginning of the series. Additionally, a few variables have a couple of missing values at the beginning and the end of the series. To deal with these missing data, we will: 1) identify and drop any variable with more than 10 missing values, 2) drop observation periods at the beginning and the end of the data that have any missing values. . plt.figure(figsize=(8, 8)) sns.heatmap(macro_data.isna(), cbar=False, xticklabels=False); . missing_data_cols = [] for i in macro_data: missing_data = len(macro_data) - macro_data[i].count() if (missing_data &gt; 10): print(i+&quot;:&quot;+str(missing_data)) missing_data_cols.append(i) # drop the missing columns macro_data.drop(missing_data_cols, axis =1, inplace=True) # drop the remaining rows with missing values (will be at beginning and end of series) macro_data.dropna(axis=0, inplace=True) macro_data.shape . PERMIT:12 PERMITNE:12 PERMITMW:12 PERMITS:12 PERMITW:12 ACOGNO:398 ANDENOx:109 TWEXAFEGSMTHx:168 UMCSENTx:154 VXOCLSx:42 . (738, 120) . Feature Engineering . Next, we will build in lags. First, we want to create 3-month, 6-month, 9-month, 12-month and 18-month lags for each predictor in the data. Additionally, since we want to attempt to predict the recession, we will shift the Regime data back one period. . For any NAs that are creating from this, we will drop that data. . for col in macro_data.drop([&#39;Date&#39;, &#39;Regime&#39;], axis=1): for n in [3, 6, 9, 12, 18]: macro_data[f&#39;{col} {n}M lag&#39;] = macro_data[col].shift(n).ffill().values macro_data[&#39;Regime&#39;] = macro_data[&#39;Regime&#39;].shift(-1) macro_data.dropna(axis=0, inplace=True) . macro_data.shape . (719, 710) . To control for serial correlation, we will apply the Augmented Dickey-Fuller test. For any predictor with a value greater than 0.01, we will difference the data. We will apply this process 3 times, to reduce serial correlation, and drop any NA rows along the way that the differencing creates. . threshold=0.01 for column in macro_data.drop([&#39;Date&#39;, &#39;Regime&#39;], axis=1): result = adfuller(macro_data[column]) if result[1]&gt;threshold: macro_data[column] = macro_data[column].diff() macro_data.dropna(axis=0, inplace=True) . threshold=0.01 for column in macro_data.drop([&#39;Date&#39;, &#39;Regime&#39;], axis=1): result = adfuller(macro_data[column]) if result[1]&gt;threshold: macro_data[column] = macro_data[column].diff() macro_data.dropna(axis=0, inplace=True) . threshold=0.01 print(&quot;The remaining predictors with Dickey-Fuller score of greater than 1:&quot;) for column in macro_data.drop([&#39;Date&#39;, &#39;Regime&#39;], axis=1): result = adfuller(macro_data[column]) if result[1]&gt;threshold: print(column) macro_data.dropna(axis=0, inplace=True) . The remaining predictors with Dickey-Fuller score of greater than 1: M1SL M2SL MZMSL CUMFNS 9M lag . We will then Standardize the remaining data. To standardized data is calculated as: $$ z = frac{x - bar{x}}{ sigma} $$ . To do this we create a DataFrame of just the features, apply the standization, and then reconstruct the DataFrame. . features = macro_data.drop([&#39;Date&#39;, &#39;Regime&#39;], axis=1) col_names = features.columns scaler = StandardScaler() scaler.fit(features) standardized_features = scaler.transform(features) standardized_features.shape df = pd.DataFrame(data=standardized_features, columns=col_names) df.insert(loc=0, column=&#39;Date&#39;, value=macro_data[&#39;Date&#39;].values) df.insert(loc=1, column=&#39;Regime&#39;, value=macro_data[&#39;Regime&#39;].values) df.shape . (719, 710) . df.to_csv(&quot;./data/recessions/Macro_Data_Cleaned.csv&quot;, index=False) . Lastly, we will convert the Regime target value to a binary 0/1 (Label) using a lambda function. . Label = df[&quot;Regime&quot;].apply(lambda regime: 1. if regime == &quot;Normal&quot; else 0.) df.insert(loc=2, column=&quot;Label&quot;, value=Label.values) df.head() . Date Regime Label RPI W875RX1 DPCERA3M086SBEA CMRMTSPLx RETAILx INDPRO IPFPNSS ... DTCTHFNM 3M lag DTCTHFNM 6M lag DTCTHFNM 9M lag DTCTHFNM 12M lag DTCTHFNM 18M lag INVEST 3M lag INVEST 6M lag INVEST 9M lag INVEST 12M lag INVEST 18M lag . 0 7/1/1960 | Recession | 0.00000 | -1.41929 | -1.45117 | -1.38195 | -1.45716 | -1.15562 | -1.67585 | -1.73979 | ... | -0.96399 | -0.95718 | -0.95014 | -0.94425 | -0.93162 | -0.93091 | -0.92424 | -0.92060 | -0.91459 | -0.90221 | . 1 8/1/1960 | Recession | 0.00000 | -1.42028 | -1.45301 | -1.38250 | -1.45816 | -1.15520 | -1.67689 | -1.74194 | ... | -0.96353 | -0.95692 | -0.94987 | -0.94339 | -0.93162 | -0.93071 | -0.92627 | -0.92094 | -0.91626 | -0.90290 | . 2 9/1/1960 | Recession | 0.00000 | -1.41950 | -1.45280 | -1.37967 | -1.44538 | -1.15532 | -1.68627 | -1.74947 | ... | -0.96272 | -0.95662 | -0.94934 | -0.94280 | -0.93146 | -0.93119 | -0.92800 | -0.92065 | -0.91666 | -0.90480 | . 3 10/1/1960 | Recession | 0.00000 | -1.41769 | -1.45055 | -1.37677 | -1.46046 | -1.15423 | -1.68731 | -1.74409 | ... | -0.96206 | -0.95607 | -0.94932 | -0.94234 | -0.93103 | -0.92947 | -0.92806 | -0.92135 | -0.91732 | -0.90463 | . 4 11/1/1960 | Recession | 0.00000 | -1.42032 | -1.45466 | -1.37885 | -1.46815 | -1.15601 | -1.69981 | -1.75377 | ... | -0.96149 | -0.95561 | -0.94907 | -0.94207 | -0.93051 | -0.92939 | -0.92785 | -0.92341 | -0.91766 | -0.90571 | . 5 rows × 711 columns . Modeling . Feature Reduction . We will begin by creating training and testing subsets. We will train our model up to &#39;12/1/1996&#39;, and then test our model on the period after that. . For this analysis, we will attempt to reduce the features by applying Regularization through a Lasso Logistic Regression. This model will penalize large coefficients forcing many to become 0. The non-zero parameters will be use in future modeling. We will accomplish this using K-fold cross-validation, with 3 folds, and solve for an optimal penalty weight. This penalty will be used in the modeling to reduce the parameters. . Throughout this process will we utizie ROC_AUC as our performance metric . df_idx = df[df.Date == &#39;12/1/1996&#39;].index[0] # separate targets and feature parameters df_targets = df[&#39;Label&#39;].values df_features = df.drop([&#39;Regime&#39;, &#39;Date&#39;, &#39;Label&#39;], axis=1) # separate our testing and training features df_training_features = df_features.iloc[:df_idx, :] df_testing_features = df_features.iloc[df_idx:, :] # separate our testing and training targets df_training_targets = df_targets[:df_idx] df_testing_targets = df_targets[df_idx:] . print( df_training_features.shape, df_testing_features.shape, df_features.shape) print( df_training_targets.shape, df_testing_targets.shape, df_targets.shape) . (435, 708) (282, 708) (717, 708) (435,) (282,) (717,) . seed = 8 # set parameters for the cross-validation penalty search scoring = &quot;roc_auc&quot; kfold = TimeSeriesSplit(n_splits=3) # Regularization hyperparamter space C = np.reciprocal([0.00000001, 0.00000005, 0.0000001, 0.0000005, 0.000001, 0.000005, 0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000, 5000]) hyperparameters = dict(C=C) # instantiate the model and perform the hyperparamter tuning model = LogisticRegression(max_iter=10000, penalty=&#39;l1&#39;, solver=&#39;liblinear&#39;) LR_penalty = GridSearchCV(estimator=model, param_grid=hyperparameters, cv=kfold, scoring=scoring, ).fit(X=df_features, y=df_targets).best_estimator_ LR_penalty . LogisticRegression(C=0.1, max_iter=10000, penalty=&#39;l1&#39;, solver=&#39;liblinear&#39;) . Now, using that tuned penalty value &#39;C&#39;, we will find the reduced parameter space. . From the HeatMap below it can be seen that due to the differencing methods above and the feature reduction performed, the majority of the remaining predictors have very little if any correlation with each other. . X = df_features y = df_targets lr_l1 = LogisticRegression(C=0.1, max_iter=10000, penalty=&#39;l1&#39;, solver=&#39;liblinear&#39;).fit(X, y) model = SelectFromModel(lr_l1, prefit=True) feature_idx = model.get_support() feature_name = X.columns[feature_idx] X_new = model.transform(X) X_new.shape . (717, 43) . feature_name . Index([&#39;INDPRO&#39;, &#39;IPFPNSS&#39;, &#39;CUMFNS&#39;, &#39;HWI&#39;, &#39;UEMPLT5&#39;, &#39;UEMP15OV&#39;, &#39;USGOOD&#39;, &#39;USCONS&#39;, &#39;NDMANEMP&#39;, &#39;USTRADE&#39;, &#39;USFIRE&#39;, &#39;AWOTMAN&#39;, &#39;AWHMAN&#39;, &#39;HOUST&#39;, &#39;BUSINVx&#39;, &#39;BUSLOANS&#39;, &#39;S&amp;P div yield&#39;, &#39;FEDFUNDS&#39;, &#39;TB3MS&#39;, &#39;TB3SMFFM&#39;, &#39;T1YFFM&#39;, &#39;WPSID62&#39;, &#39;IPMAT 9M lag&#39;, &#39;HWIURATIO 9M lag&#39;, &#39;CLAIMSx 9M lag&#39;, &#39;USCONS 3M lag&#39;, &#39;SRVPRD 3M lag&#39;, &#39;USWTRADE 3M lag&#39;, &#39;USFIRE 3M lag&#39;, &#39;USFIRE 12M lag&#39;, &#39;USFIRE 18M lag&#39;, &#39;BUSLOANS 6M lag&#39;, &#39;S&amp;P 500 6M lag&#39;, &#39;S&amp;P div yield 3M lag&#39;, &#39;TB3MS 6M lag&#39;, &#39;AAA 3M lag&#39;, &#39;TB3SMFFM 3M lag&#39;, &#39;T5YFFM 6M lag&#39;, &#39;T10YFFM 6M lag&#39;, &#39;T10YFFM 12M lag&#39;, &#39;EXUSUKx 12M lag&#39;, &#39;OILPRICEx 9M lag&#39;, &#39;CUSR0000SAD 3M lag&#39;], dtype=&#39;object&#39;) . df_2 = df[feature_name] df_2.insert(loc=0, column=&#39;Date&#39;, value=df[&#39;Date&#39;].values) df_2.insert(loc=1, column=&#39;Regime&#39;, value=df[&#39;Regime&#39;].values) df_2.insert(loc=2, column=&#39;Label&#39;, value=df[&#39;Label&#39;].values) df_2.head(), df_2.shape . ( Date Regime Label INDPRO IPFPNSS CUMFNS HWI 0 9/1/1960 Recession 0.0 -0.483530 -0.414744 -1.489253 -0.401757 1 10/1/1960 Recession 0.0 -0.179359 0.065987 -0.498499 -0.413074 2 11/1/1960 Recession 0.0 -0.597645 -0.494773 -2.323340 -0.169770 3 12/1/1960 Recession 0.0 -0.749799 -0.615026 -2.003328 -0.447023 4 1/1/1961 Normal 1.0 -0.103282 -0.174378 -0.172878 -0.028315 UEMPLT5 UEMP15OV USGOOD ... S&amp;P div yield 3M lag TB3MS 6M lag 0 -0.441635 0.394909 -0.416682 ... -0.987957 -1.529339 1 0.547806 1.555983 -0.900883 ... 0.749107 -0.181226 2 -0.310127 0.234497 -1.308631 ... -0.328282 0.149889 3 2.395180 -0.643948 -2.141118 ... 0.921032 -1.955059 4 -1.374716 1.395572 -0.348723 ... 0.622105 -0.370435 AAA 3M lag TB3SMFFM 3M lag T5YFFM 6M lag T10YFFM 6M lag 0 -0.034870 -0.566341 -0.190675 -0.397154 1 -0.171493 -0.665182 -0.264562 -0.427827 2 -0.581361 -0.312179 -0.065066 -0.341943 3 -0.125952 0.478548 0.053154 -0.139503 4 0.238375 0.407947 -0.124176 -0.237656 T10YFFM 12M lag EXUSUKx 12M lag OILPRICEx 9M lag CUSR0000SAD 3M lag 0 -0.089042 -0.096695 -0.022099 -0.002097 1 -0.316569 0.124353 -0.022099 0.416725 2 -0.328867 -0.038801 -0.022099 -0.002097 3 -0.224328 -0.057222 -0.022099 -0.839740 4 -0.205880 0.092775 -0.022099 0.835546 [5 rows x 46 columns], (717, 46)) . plt.figure(figsize=(10,8)) sns.heatmap(df_2.drop([&#39;Date&#39;, &#39;Regime&#39;,&#39;Label&#39;], axis=1).corr(), cmap=&#39;coolwarm&#39;, square=True); . df=df_2 df.shape . (717, 46) . Model Testing . Now that we have reduced our parameter set dramatically, we will test these predictors against a variety of models: . Logistic Regression | Lasso Regression (Logistic w/ L1 penalty) | Ridge Regression (Logistic w/ L2 penalty) | Linear Discriminant Analysis | K-Nearest Neighbors | Gradient Boost | AdaGradient Boost | Random Forest | XG Boost | . From these models we will calculate the ROC_AUC and choose a few to drill down further in the modeling. . df_idx = df[df.Date == &#39;12/1/1996&#39;].index[0] df_targets = df[&#39;Label&#39;].values df_features = df.drop([&#39;Regime&#39;, &#39;Date&#39;, &#39;Label&#39;], axis=1) df_training_features = df_features.iloc[:df_idx, :] df_testing_features = df_features.iloc[df_idx:, :] df_training_targets = df_targets[:df_idx] df_testing_targets = df_targets[df_idx:] . seed = 8 scoring = &#39;roc_auc&#39; kfold = TimeSeriesSplit(n_splits=3) models = [] models.append((&#39;LR&#39;, LogisticRegression(C=1e09))) models.append((&#39;LR_l1&#39;, LogisticRegression(penalty=&#39;l1&#39;, solver=&#39;liblinear&#39;))) models.append((&#39;LR_l2&#39;, LogisticRegression(penalty=&#39;l2&#39;, solver=&#39;liblinear&#39;))) models.append((&#39;LDA&#39;, LinearDiscriminantAnalysis())) models.append((&#39;KNN&#39;, KNeighborsClassifier())) models.append((&#39;GB&#39;, GradientBoostingClassifier())) models.append((&#39;ABC&#39;, AdaBoostClassifier())) models.append((&#39;RF&#39;, RandomForestClassifier())) models.append((&#39;XGB&#39;, xgb.XGBClassifier())) results = [] names = [] lb = preprocessing.LabelBinarizer() for name, model in models: cv_results = cross_val_score(estimator=model, X= df_training_features, y=lb.fit_transform(df_training_targets), cv=kfold, scoring=scoring) model.fit(df_training_features, df_training_targets) fpr, tpr, thresholds = roc_curve(df_training_targets, model.predict_proba(df_training_features)[:,1]) auc = roc_auc_score(df_training_targets, model.predict(df_training_features)) plt.plot(fpr, tpr, label=&#39;%s ROC (area = %0.2f)&#39; % (name, auc)) results.append(cv_results) names.append(name) msg = &quot;%s: %f (%f)&quot; % (name, cv_results.mean(), cv_results.std()) print(msg) plt.plot([0, 1], [0, 1], &#39;r--&#39;) plt.xlim([-0.05, 1.0]) plt.ylim([0.0, 1.05]) plt.xlabel(&#39;1-Specificity(False Postive Rate)&#39;) plt.ylabel(&#39;Sensitivity(True Positive Rate)&#39;) plt.title(&#39;Receiver Operating Characteristic&#39;) plt.legend(loc = &#39;center left&#39;, bbox_to_anchor=(1, 0.5)) plt.show() warnings.filterwarnings(&#39;ignore&#39;) . LR: 0.835008 (0.094913) LR_l1: 0.938928 (0.023580) LR_l2: 0.899869 (0.047857) LDA: 0.807150 (0.047330) KNN: 0.865192 (0.061098) GB: 0.784229 (0.140969) ABC: 0.854698 (0.038334) RF: 0.908651 (0.041237) XGB: 0.855819 (0.082838) . fig = plt.figure() fig.suptitle(&#39;Algorithm Comparison based on Cross Validation Score&#39;) ax = fig.add_subplot(111) plt.boxplot(results) ax.set_xticklabels(names) plt.show(); . Model Evaluation . For each of the following models we will create a DataFrame including a combination of trained probabilites up to &#39;12/1/96&#39; and predicted probabilites therafter. These probabilities will be overlayed on a shaded chart showing our target values for recession periods. . Logistic Model | Regularized Logostic Model | Random Forest Model | XGBoost Model | . rec_spans = [] #rec_spans.append([datetime.datetime(1957,8,1), datetime.datetime(1958,4,1)]) rec_spans.append([datetime.datetime(1960,4,1), datetime.datetime(1961,2,1)]) rec_spans.append([datetime.datetime(1969,12,1), datetime.datetime(1970,11,1)]) rec_spans.append([datetime.datetime(1973,11,1), datetime.datetime(1975,3,1)]) rec_spans.append([datetime.datetime(1980,1,1), datetime.datetime(1980,6,1)]) rec_spans.append([datetime.datetime(1981,7,1), datetime.datetime(1982,10,1)]) rec_spans.append([datetime.datetime(1990,7,1), datetime.datetime(1991,2,1)]) rec_spans.append([datetime.datetime(2001,3,1), datetime.datetime(2001,10,1)]) rec_spans.append([datetime.datetime(2007,12,1), datetime.datetime(2009,5,1)]) rec_spans.append([datetime.datetime(2020,2,1), datetime.datetime(2020,6,1)]) . Logistic Regression . model = LogisticRegression(C=1e09) LR = model.fit(df_training_features, df_training_targets) training_predictions = LR.predict(df_training_features) prob_predictions = LR.predict_proba(df_training_features) prob_pred_LR = np.append(prob_predictions, LR.predict_proba(df_testing_features), axis=0) . prob_pred_LR.shape . (717, 2) . sample_range = pd.date_range(start=&#39;9/1/1960&#39;, end=&#39;5/1/2020&#39;, freq=&#39;MS&#39;) plt.figure(figsize=(20,5)) plt.plot(sample_range.to_series().values, prob_pred_LR[:,0]) for i in range(len(rec_spans)): plt.axvspan(rec_spans[i][0], rec_spans[i][len(rec_spans[i]) - 1], alpha=0.25, color=&#39;grey&#39;) plt.axhline(y=0.5, color=&#39;r&#39;, ls=&#39;dashed&#39;, alpha = 0.5) plt.title(&#39;Recession Prediction Probabalities with Logistic Regression&#39;) plt.show(); . Regularized Logistic Regression . penalty = [&#39;l1&#39;, &#39;l2&#39;] # Create regularization hyperparameter space C = np.reciprocal([0.00000001, 0.00000005, 0.0000001, 0.0000005, 0.000001, 0.000005, 0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000, 5000]) # Create hyperparameter options hyperparameters = dict(C=C, penalty=penalty) # instantiate the model and perform the hyperparameter search model = LogisticRegression(max_iter=10000, solver=&#39;liblinear&#39;) LR_penalty = GridSearchCV(estimator=model, param_grid=hyperparameters, cv=kfold, scoring=scoring).fit(df_training_features, df_training_targets).best_estimator_ # predict on the training data training_predictions = LR_penalty.predict(df_training_features) . prob_predictions = LR_penalty.predict_proba(df_training_features) prob_pred_LR_pen = np.append(prob_predictions, LR_penalty.predict_proba(df_testing_features), axis=0) . sample_range = pd.date_range(start=&#39;9/1/1960&#39;, end=&#39;5/1/2020&#39;, freq=&#39;MS&#39;) plt.figure(figsize=(20,5)) plt.plot(sample_range.to_series().values, prob_pred_LR_pen[:,0]) for i in range(len(rec_spans)): plt.axvspan(rec_spans[i][0], rec_spans[i][len(rec_spans[i]) - 1], alpha=0.25, color=&#39;grey&#39;) plt.axhline(y=0.5, color=&#39;r&#39;, ls=&#39;dashed&#39;, alpha = 0.5) plt.title(&#39;Recession Prediction Probabalities with Regularized Logistic Regression&#39;) plt.show() . Random Forest . n_estimators = [int(x) for x in np.linspace(start=400,stop=800, num=5)] max_depth = [int(x) for x in np.linspace(10, 30, 5)] min_samples_split = [2, 4, 6, 8] min_samples_leaf = [1, 3, 5, 7] random_grid = {&#39;n_estimators&#39;: n_estimators, &#39;max_depth&#39;: max_depth, &#39;min_samples_split&#39;: min_samples_split, &#39;min_samples_leaf&#39;: min_samples_leaf } random_grid . {&#39;n_estimators&#39;: [400, 500, 600, 700, 800], &#39;max_depth&#39;: [10, 15, 20, 25, 30], &#39;min_samples_split&#39;: [2, 4, 6, 8], &#39;min_samples_leaf&#39;: [1, 3, 5, 7]} . randf = GridSearchCV(estimator=RandomForestClassifier(), param_grid=random_grid, cv=kfold, scoring=scoring).fit(df_training_features, lb.fit_transform(df_training_targets)).best_estimator_ # predict on the training set randf.fit(df_training_features, df_training_targets) . RandomForestClassifier(max_depth=30, min_samples_leaf=3, min_samples_split=6, n_estimators=700) . prob_predictions = randf.predict_proba(df_training_features) prob_pred_rf = np.append(prob_predictions, randf.predict_proba(df_testing_features), axis=0) . sample_range = pd.date_range(start=&#39;9/1/1960&#39;, end=&#39;5/1/2020&#39;, freq=&#39;MS&#39;) plt.figure(figsize=(20,5)) plt.plot(sample_range.to_series().values, prob_pred_rf[:,0]) for i in range(len(rec_spans)): plt.axvspan(rec_spans[i][0], rec_spans[i][len(rec_spans[i]) - 1], alpha=0.25, color=&#39;grey&#39;) plt.axhline(y=0.5, color=&#39;r&#39;, ls=&#39;dashed&#39;, alpha = 0.5) plt.title(&#39;Recession Prediction Probabalities with Random Forest&#39;) plt.show(); . feature_importance = [(feature, round(importance, 2)) for feature, importance in zip(feature_name, randf.feature_importances_)] feature_importance = sorted(feature_importance, key = lambda x: x[1], reverse=True) #[print(f&#39;{feature:20}: {importance}&#39;) for feature, importance in feature_importance] . XGBoost . xgboost = GridSearchCV(estimator=xgb.XGBClassifier(), param_grid={&#39;booster&#39;: [&#39;gbtree&#39;]}, scoring=scoring, cv=kfold).fit(df_training_features, lb.fit_transform(df_training_targets)).best_estimator_ xgboost.fit(df_training_features, df_training_targets) . XGBClassifier(base_score=0.5, booster=&#39;gbtree&#39;, colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1, importance_type=&#39;gain&#39;, interaction_constraints=&#39;&#39;, learning_rate=0.300000012, max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan, monotone_constraints=&#39;()&#39;, n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=&#39;exact&#39;, validate_parameters=1, verbosity=None) . prob_predictions = xgboost.predict_proba(df_training_features) # predict on the testing set and combine into single dataset prob_pred_xgb = np.append(prob_predictions, xgboost.predict_proba(df_testing_features), axis=0) . sample_range = pd.date_range(start=&#39;9/1/1960&#39;, end=&#39;5/1/2020&#39;, freq=&#39;MS&#39;) plt.figure(figsize=(20,5)) plt.plot(sample_range.to_series().values, prob_pred_xgb[:,0]) for i in range(len(rec_spans)): plt.axvspan(rec_spans[i][0], rec_spans[i][len(rec_spans[i]) - 1], alpha=0.25, color=&#39;grey&#39;) plt.axhline(y=0.5, color=&#39;r&#39;, ls=&#39;dashed&#39;, alpha = 0.5) plt.title(&#39;Recession Prediction Probabalities with XGBoost&#39;) plt.show(); . headers = df.drop([&#39;Regime&#39;,&#39;Label&#39;, &#39;Date&#39;], axis=1).columns.values.tolist() xgboost_importances = pd.DataFrame(xgboost.feature_importances_, index = headers, columns = [&#39;Relative Importance&#39;]) _ = xgboost_importances.sort_values(by = [&#39;Relative Importance&#39;], ascending = False, inplace=True) xgboost_importances = xgboost_importances[xgboost_importances[&#39;Relative Importance&#39;]&gt;0].iloc[:20] # display importances in bar-chart and pie-chart fig = plt.figure(figsize=(6,6)) plt.xticks(rotation=&#39;90&#39;) plt.barh(y=np.arange(len(xgboost_importances)), width=xgboost_importances[&#39;Relative Importance&#39;], align=&#39;center&#39;, tick_label=xgboost_importances.index) plt.gca().invert_yaxis() plt.show() . fpr, tpr, thresholds = roc_curve(df_testing_targets, LR.predict_proba(df_testing_features)[:,1]) auc = roc_auc_score(df_testing_targets,LR.predict(df_testing_features)) plt.plot(fpr, tpr, label=&#39;%s ROC (area = %0.2f)&#39; % (&#39;LR&#39;, auc)) fpr, tpr, thresholds = roc_curve(df_testing_targets, LR_penalty.predict_proba(df_testing_features)[:,1]) auc = roc_auc_score(df_testing_targets,LR_penalty.predict(df_testing_features)) plt.plot(fpr, tpr, label=&#39;%s ROC (area = %0.2f)&#39; % (&#39;LR_penalty&#39;, auc)) fpr, tpr, thresholds = roc_curve(df_testing_targets, randf.predict_proba(df_testing_features)[:,1]) auc = roc_auc_score(df_testing_targets,randf.predict(df_testing_features)) plt.plot(fpr, tpr, label=&#39;%s ROC (area = %0.2f)&#39; % (&#39;Random_Forest&#39;, auc)) fpr, tpr, thresholds = roc_curve(df_testing_targets, xgboost.predict_proba(df_testing_features)[:,1]) auc = roc_auc_score(df_testing_targets,xgboost.predict(df_testing_features)) plt.plot(fpr, tpr, label=&#39;%s ROC (area = %0.2f)&#39; % (&#39;XGBoost&#39;, auc)) plt.plot([0, 1], [0, 1],&#39;r--&#39;) plt.xlim([-0.05, 1.0]) plt.ylim([0.0, 1.05]) plt.xlabel(&#39;1-Specificity(False Positive Rate)&#39;) plt.ylabel(&#39;Sensitivity(True Positive Rate)&#39;) plt.title(&#39;Receiver Operating Characteristic (Testing Data)&#39;) plt.legend(loc=&#39;center left&#39;, bbox_to_anchor=(1, 0.5)) plt.show() . Conclusion . The results for the above are mixed. While the models do an incredible job of fitting the training data, the models do a medicore job of fitting the testing data. Regularized Logistic Regression performs as well or better than the more complex and uninterprettable Random Forest and XGBoost models. Going forward it will be important to test varying degree of differencing, varying degree of regularization, and applying deep learning models, as well. . As a final check, let&#39;s take a quick look at how these models performed in the 6 months running up to the 3 recession periods and 6 months after they began. . Recession beginning in: . March, 2001 (Tech bubble bursting) | December, 2007 (Great Recession / Financial Crisis) | February, 2020 (Covid Pandemic). | . Admittedly, the last recession will be difficult to predict given the cause. . ensemble_port = pd.DataFrame({ &quot;Label&quot;: df_targets, &quot;Log. Reg.&quot;: prob_pred_LR[:, 0], &quot;LR w Penalty&quot;: prob_pred_LR_pen[:, 0], &quot;Rand. Forest&quot;: prob_pred_rf[:, 0], &quot;XGBoost&quot;: prob_pred_xgb[:, 0], }) ensemble_port.set_index(pd.date_range(start=&#39;9/1/1960&#39;, end=&#39;5/1/2020&#39;, freq=&#39;MS&#39;), inplace=True) . ensemble_port.loc[&#39;10/1/2000&#39;:&#39;9/1/2001&#39;] . Label Log. Reg. LR w Penalty Rand. Forest XGBoost . 2000-10-01 1.00000 | 0.00005 | 0.11836 | 0.06209 | 0.00116 | . 2000-11-01 1.00000 | 0.00000 | 0.13676 | 0.06778 | 0.00192 | . 2000-12-01 1.00000 | 0.00005 | 0.16211 | 0.12199 | 0.01367 | . 2001-01-01 1.00000 | 0.00000 | 0.11636 | 0.18128 | 0.01352 | . 2001-02-01 0.00000 | 0.00498 | 0.20799 | 0.19011 | 0.02765 | . 2001-03-01 0.00000 | 0.00646 | 0.27773 | 0.16869 | 0.02518 | . 2001-04-01 0.00000 | 0.00001 | 0.24675 | 0.26177 | 0.02103 | . 2001-05-01 0.00000 | 0.00332 | 0.23894 | 0.24762 | 0.01678 | . 2001-06-01 0.00000 | 0.00001 | 0.31404 | 0.44684 | 0.39779 | . 2001-07-01 0.00000 | 0.00000 | 0.22346 | 0.34860 | 0.10067 | . 2001-08-01 0.00000 | 0.33395 | 0.32899 | 0.35230 | 0.06956 | . 2001-09-01 0.00000 | 0.00021 | 0.30561 | 0.37141 | 0.17867 | . ensemble_port.loc[&#39;6/1/2007&#39;:&#39;5/1/2008&#39;] . Label Log. Reg. LR w Penalty Rand. Forest XGBoost . 2007-06-01 1.00000 | 0.00000 | 0.13057 | 0.02367 | 0.00356 | . 2007-07-01 1.00000 | 0.00043 | 0.20176 | 0.09850 | 0.01644 | . 2007-08-01 1.00000 | 0.00137 | 0.28275 | 0.27305 | 0.10577 | . 2007-09-01 1.00000 | 0.04727 | 0.29538 | 0.20165 | 0.19782 | . 2007-10-01 1.00000 | 0.00003 | 0.22362 | 0.20095 | 0.00974 | . 2007-11-01 0.00000 | 0.00480 | 0.39546 | 0.32837 | 0.40103 | . 2007-12-01 0.00000 | 0.00030 | 0.30867 | 0.38675 | 0.67886 | . 2008-01-01 0.00000 | 0.00000 | 0.31556 | 0.26732 | 0.32760 | . 2008-02-01 0.00000 | 0.00000 | 0.45533 | 0.44128 | 0.43532 | . 2008-03-01 0.00000 | 0.00000 | 0.48720 | 0.48993 | 0.95703 | . 2008-04-01 0.00000 | 0.00000 | 0.39114 | 0.55082 | 0.93758 | . 2008-05-01 0.00000 | 0.00000 | 0.29485 | 0.38763 | 0.16196 | . ensemble_port.loc[&#39;8/1/2019&#39;:&#39;6/1/2020&#39;] . Label Log. Reg. LR w Penalty Rand. Forest XGBoost . 2019-08-01 1.00000 | 0.00014 | 0.13426 | 0.10102 | 0.00569 | . 2019-09-01 1.00000 | 0.00000 | 0.10636 | 0.07161 | 0.00126 | . 2019-10-01 1.00000 | 0.00009 | 0.09359 | 0.03221 | 0.00191 | . 2019-11-01 1.00000 | 0.00014 | 0.11972 | 0.09253 | 0.00259 | . 2019-12-01 1.00000 | 0.00000 | 0.10443 | 0.06986 | 0.00091 | . 2020-01-01 0.00000 | 0.00000 | 0.05246 | 0.11320 | 0.00082 | . 2020-02-01 0.00000 | 0.00000 | 0.07803 | 0.08793 | 0.00069 | . 2020-03-01 0.00000 | 1.00000 | 0.68848 | 0.39844 | 0.14169 | . 2020-04-01 0.00000 | 1.00000 | 0.96428 | 0.45502 | 0.52915 | . 2020-05-01 0.00000 | 0.00000 | 0.01042 | 0.09681 | 0.00090 | . References: . EDHEC/Coursera - Python and Machine Learning for Asset Management M. McCracken and S. Ng - https://doi.org/10.20955/wp.2015.012 - Working Paper 2015-012B J. H. Stock and M. W. Watson. https://www.nber.org/system/files/chapters/c10968/c10968.pdf, NBER Macroeconomics Annual, 1989. NBER Business Cycle Dating - https://www.nber.org/research/business-cycle-dating Federal Reserve-St.Louis - https://s3.amazonaws.com/files.fred.stlouisfed.org/fred-md/quarterly/current.csv .",
            "url": "https://scottknappnj.github.io/FastPagesBlog/2020/12/11/Identifying-Recessions.html",
            "relUrl": "/2020/12/11/Identifying-Recessions.html",
            "date": " • Dec 11, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Activation and Loss Functions",
            "content": "Deep learning and Neural Nets seem to invoke an awe of science fiction and are only understood by &#39;rocket scientists&#39; or those with PhD&#39;s in math and the sciences. The reality is while in it&#39;s entirety these networks are complex, when you break them down into their individual parts, everything is significantly more digestible and can be easily understood at both an inutitive and technical sense. So much so, that almost everything we will look at can be written in a single line of code. When broken down, neural networks are simply layers of functions solving a linear equation with a non-linear transformation separating each linear layer. These non-linear transformations are called Activation Functions. There are a few with intimidating sounding names, which will be break down and explain how each works in practice. . Additionally, the final layer of the network must produce some type of predictive results. How we evaluate these results against what we expected is done with a Loss Function. The objective of our modeling is to minimize this Loss Function. Again, these functions are much simpler than they initially appear. . Activation Functions . In order to visualize our activation functions, we will create an array of values to plot against each one. . import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline pd.set_option(&#39;display.float_format&#39;, lambda x: &#39;%.5f&#39; % x) np.set_printoptions(suppress=True) . . values = np.linspace(-5, 5, 100) . Binary Step . We will begin with the simplest of the activation functions called binary step. This function will simply return an either/or result. In our case, we will return 0 or 1, if our input is greater than 0. . def binary_step(inputs): return np.array([1 if i &gt; 0 else 0 for i in values]) . output = binary_step(values) plt.plot(values, output) plt.title(&quot;Binary Step Function&quot;); . Rectified Linear Unit (ReLU) . We will take this one step further with the Rectified Linear Unit. This is similar to the Binary Step function, which the exception of returning the input if greater than a value instead of just 1. Let&#39;s take a look . def rectified_linear_unit(inputs): return np.array([i if i &gt; 0 else 0 for i in values]) . output = rectified_linear_unit(values) plt.plot(values, output) plt.title(&quot;Rectified Linear Unit&quot;); . Leaky ReLU . This will be similar to the Rectified Linear Unit, except that anything below 0 will have multiplied by a factor. We will use 0.1. . def leaky_ReLU(inputs): return np.array([i if i &gt; 0 else i*0.1 for i in values]) . output = leaky_ReLU(values) plt.plot(values, output) plt.title(&quot;Leaky ReLU&quot;); . Sigmoid Function . Now we will move to some more complex function, albeit, still relatively simple when broken down. The Sigmoid Function also known as the Logistic Function binds all of its&#39; output between 0 and 1 in a smooth manner by using the formula $ frac{1}{1 + e^{-x}}$. . def sigmoid(inputs): return np.array([ (1 / (1 + np.exp(-i))) for i in inputs ]) . output = sigmoid(values) plt.plot(values, output) plt.title(&quot;Sigmoid&quot;); . def sigmoid(inputs): return np.array([ (1 / (1 + np.exp(-i))) for i in inputs ]) . Tanh Function . The Hyperbolic Tangent Function results in something very similar to the Sigmoid Function with 2 excpetions. 1) It binds the output to between -1 and 1 and 2) it approaches the bounds much faster. . def tanh_func(inputs): return np.array([ np.tanh(i) for i in inputs]) . output = tanh_func(values) plt.plot(values, output) plt.title(&quot;Tanh Function&quot;); . Swish Function . This is a new function that has been developed by Google. It is similar to the sigmoid, replacing 1 with the input in the numerator. It has shown some improvement in practice. . def swish(inputs): return np.array([ (i / (1 + np.exp(-i))) for i in inputs ]) . output = swish(values) plt.plot(values, output) plt.title(&quot;Swish Function&quot;); . Loss Functions . Now we will take a look at various Loss Functions. Again, a loss function is something that tells the model how well it is doing. An important feature of the loss function is that is it continuous and that it can produce a stable derivative (gradient). In an earlier blog post, we discuss gradient descent, which is how the model improves into parameters during training. If a loss function produces zero or infinite gradients, then the model will not be able to improve its&#39; parameters and we will be stuck with a non-optimal solution. Additional, which loss function we use, will depend on whether we are working with a Regression Problem, a Binary Classification problem or a Multi-Class Classification problem. Let&#39;s take a look at each one. . Regression Problems . In regression problems, we are looking to predict a number; the price of a home, the salary of an employee, sales of a product. During training, we can test how close our predictions are to our target values. We will show 3 commonly used loss functions for this: . Mean Absolute Error . | Mean Squared Error . | Root Mean Squared Error . | . In order to illustrate our Loss Function we will use some simulated data. We will created random normal target values, and then our predictions will simply be these target values with some additonal noise around them, created for the purpose of explaining the functions . np.random.seed(42) targets = np.random.normal(100, 10, 5) predictions = targets + np.random.normal(0, 10, 5) pd.DataFrame({ &quot;Targets&quot;: targets, &quot;Predictions&quot;: predictions }) . Targets Predictions . 0 104.967142 | 102.625772 | . 1 98.617357 | 114.409485 | . 2 106.476885 | 114.151233 | . 3 115.230299 | 110.535555 | . 4 97.658466 | 103.084067 | . Mean Absolute Error (MAE) . As with each of our Regression loss function, the name just a good job explaining the loss function. Here we will take the average over the absolute value of the difference between the target and prediction values for each observation. . You will sometimes see this referred to as L1 norm. . def mean_absolute_error(targets, predictions): return abs((targets - predictions)).mean() . mean_absolute_error(targets, predictions) . 7.185637862260793 . Mean Squared Error (MSE) . The reason we take the absolute value in MAE, is that we don&#39;t want high prediction error offsetting low prediction error negating each other, so we take the absolute value. Another way to always have the error shown as a positive value is by squaring the error. This loss function punishes worse predictions more. For example, a prediction off by 2 has a MSE of 4, but a prediction off by 4 has a mean squared error of 16. . def mean_squared_error(targets, predictions): error = targets - predictions return (error**2).mean() . mean_squared_error(targets, predictions) . 73.04933789454411 . Root Mean Squared Error (RMSE) . Now, the problem with MSE, is that it doesn&#39;t have an intuitive result. If we then take the square root of MSE, we then arrive back at our base values for better explanative value. MSE and RMSE will perform equally in practice. . RMSE, is also known as L2 norm. . def root_mean_squared_error(targets, predictions): return np.sqrt(mean_squared_error(targets, predictions)) . root_mean_squared_error(targets, predictions) . 8.546890539520447 . Classification Problems . Classification Problems can range from a variety of tasks. Binary classification can include; have a disease or not, loan default or not, dog or cat. Multi-class Classification can range from; breeds of dogs, identifying handwritten letters. These different sets of problems, binary &amp; multi-class, need different loss functions. We will discuss one for each below. . Binary Classification . Sigmoid . This is the same Sigmoid function that we discussed above in activation functions. It is commonly used for binary classifcation since everything will fall nicely between 0 and 1. It can be used to produce a probability like number for the first variable and then use (1-result) for the probability of the second number. . Multi-class Classification . Sigmoid function does a nice job for binary cases, but it won&#39;t work if you apply it across multiple categories. We can see that the 3 items in each list do not add up to 1, we need to find another solution for multiple classes. . np.random.seed(42) parameters = np.random.normal(0, 2, 15).reshape(5, 3) parameters . array([[ 0.99342831, -0.2765286 , 1.29537708], [ 3.04605971, -0.46830675, -0.46827391], [ 3.15842563, 1.53486946, -0.93894877], [ 1.08512009, -0.92683539, -0.93145951], [ 0.48392454, -3.82656049, -3.44983567]]) . sigmoid(parameters) . array([[0.72976454, 0.43130504, 0.78505592], [0.95461211, 0.38501709, 0.38502487], [0.95923943, 0.82271766, 0.28111273], [0.74746169, 0.28356719, 0.28262871], [0.61867417, 0.02131997, 0.03077376]]) . sigmoid(parameters).sum(axis=1) . array([1.94612551, 1.72465407, 2.06306983, 1.31365758, 0.6707679 ]) . Softmax . The Softmax Function helps with this. Here we simply raise each value to $e$ and calculated each parameters weight in the total of all the parameters. You can see that here in the code written below. Calculate the weight of each parameter forces the total to equal 1. . def softmax_detail(inputs): output = [] for result in inputs: output.append( [np.exp(i) / np.exp(result).sum() for i in result] ) return np.array(output) . softmax_detail(parameters) . array([[0.3797465 , 0.10664942, 0.51360408], [0.94381089, 0.02809409, 0.02809502], [0.82385052, 0.16246018, 0.0136893 ], [0.7893676 , 0.1055597 , 0.1050727 ], [0.96805704, 0.01299814, 0.01894482]]) . softmax_detail(parameters).sum(axis=1) . array([1., 1., 1., 1., 1.]) . Now that we have calculated the probabilities that sum to 1, we can subset out the results based on our target. Below we have set 5 targeted in our random example, and show the code on how to subset out value for the targets. This will be used in our loss function. . def softmax(inputs, targets): outputs = softmax_detail(inputs) idx = range(len(inputs)) return outputs[idx, targets] . targets = np.array([1, 0, 0, 2, 0]) . sm_preds = softmax(parameters, targets) sm_preds . array([0.10664942, 0.94381089, 0.82385052, 0.1050727 , 0.96805704]) . Negative Log Likelihood . Now that we have built up to Softmax we are going to finish up by reverting back to Loss Functions. . One of the drawbacks of both the sigmoid and softmax function is that they outputs we are generating are good for identifying the best predictions as the approach 1/0 quickly for each observation, however this reduces our ability to train the model as the gradient decreases dramatically as we approach these bounds, making improvements harder. If we can then transform this back using the log function (the inverse of the exponential, it may improve the training of our model, and we can use this as an Loss Function. One interest result by using the Negative Log of the Softmax function is that as the output approches 1, as seen below, the loss function will approach 0. However, as the softmax output approaches 0, the loss function will approach $ infty$. . We can see the final loss function outputs below with the code. . softmax_output = np.linspace(0.01, 0.99, 99) neg_log_like = -np.log(softmax_output) plt.plot(softmax_output, neg_log_like); . def neg_log_likelihood(inputs): return -np.log(sm_preds) . neg_log_likelihood(sm_preds) . array([2.23820826, 0.05782946, 0.19376618, 2.25310276, 0.03246427]) . Cross-Entropy . Combining the Softmax Function with Negative Log Likihood is called the Cross-Entropy Loss Function. This is ideally used for Multi-class Classification. Let&#39;s put the whole function together. . def cross_entropy(inputs, targets): output = [] for result in inputs: output.append( [np.exp(i) / np.exp(result).sum() for i in result] ) idx = range(len(inputs)) outputs = np.array(output) preds = outputs[idx, targets] return -np.log(preds) . cross_entropy(parameters, targets) . array([2.23820826, 0.05782946, 0.19376618, 2.25310276, 0.03246427]) . This was a focused discussion on Activation and Loss Functions used to build Deep Learning models. Please see some of the other blog posts, we outline the other building blocks as well. .",
            "url": "https://scottknappnj.github.io/FastPagesBlog/2020/12/09/Activations-and-Loss-Functions.html",
            "relUrl": "/2020/12/09/Activations-and-Loss-Functions.html",
            "date": " • Dec 9, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "The Efficient Frontier",
            "content": "Objective . We have all heard the saying, &quot;There is no such thing a free lunch&quot; and as with other things, this is usually the case in the financial markets, just as with all other cases. However, there is an exception to this rule and it has to do with portfolio diversification. . We will show how diversification can reduce a portfolio&#39;s risk without affecting its&#39; return, with some explicit examples. We will then explore how to use this concept to build the &quot;best&quot; portfolios available, a concept called the efficient frontier, developed by Harry Markowitz back in 1952. After that we will see that that while theoretically pleasing, there are some major drawbacks with this method and propose alternatives to this method of portfolio construction. . Let&#39;s start by looking at his &quot;free lunch&quot; idea. . # Setup / Imports import numpy as np import pandas as pd from pandas_datareader import data import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline from scipy.optimize import minimize from IPython.display import Markdown as md pd.set_option(&#39;display.float_format&#39;, lambda x: &#39;%.5f&#39; % x) print(&quot;Setup Complete&quot;) . . Setup Complete . Free Lunch - Diversification . We will begin by looking at some randomized returns and volatilities of 5 stocks separately and them see what happens if we combine them together in a portfolio. . Let&#39;s create some these randomized annual returns for 5 individual stocks over 10 years assuming: . * expected return of 10% * expected volatility of 20%. . np.random.seed(24) stock_a = np.random.normal(0.10, 0.20, 10) stock_b = np.random.normal(0.10, 0.20, 10) stock_c = np.random.normal(0.10, 0.20, 10) stock_d = np.random.normal(0.10, 0.20, 10) stock_e = np.random.normal(0.10, 0.20, 10) returns = pd.DataFrame({ &quot;Stock A&quot;: stock_a, &quot;Stock B&quot;: stock_b, &quot;Stock C&quot;: stock_c, &quot;Stock D&quot;: stock_d, &quot;Stock E&quot;: stock_e }) returns . Stock A Stock B Stock C Stock D Stock E . 0 0.36584 | 0.23576 | -0.16739 | 0.02286 | 0.35282 | . 1 -0.05401 | 0.47785 | 0.21257 | 0.20396 | 0.15801 | . 2 0.03674 | 0.29231 | 0.37857 | 0.43732 | -0.29406 | . 3 -0.09816 | 0.12080 | 0.08733 | -0.16519 | 0.26078 | . 4 -0.11416 | 0.00377 | 0.12433 | 0.38580 | 0.30611 | . 5 -0.18774 | 0.27005 | 0.34152 | -0.31787 | 0.12362 | . 6 0.21288 | 0.39068 | 0.09959 | 0.07404 | 0.09563 | . 7 0.15914 | 0.31155 | 0.42556 | 0.22630 | 0.10937 | . 8 -0.22528 | 0.13311 | 0.17090 | -0.01731 | -0.22575 | . 9 0.14391 | 0.20300 | 0.30751 | 0.15814 | 0.02153 | . Let&#39;s look at the simulated returns and volatilies for these 5 stocks. . ave_returns = ((1+returns).prod()) ** (1/10) - 1 ave_vol = returns.std() portfolio = pd.DataFrame({ &quot;Ann Returns&quot;: ave_returns, &quot;Ann Volatility&quot;: ave_vol }) portfolio . Ann Returns Ann Volatility . Stock A 0.00801 | 0.19218 | . Stock B 0.23693 | 0.13772 | . Stock C 0.18539 | 0.17600 | . Stock D 0.07670 | 0.23355 | . Stock E 0.07000 | 0.21160 | . def annualize_rets(returns, periods_per_year=12): # compound each years&#39; return at 1+r compounded_growth = (1+returns).prod() # calculate the number of periods in ind_returns n_periods = returns.shape[0] return compounded_growth ** (periods_per_year / n_periods) - 1 def annualize_stdev(returns, periods_per_year=12): return returns.std() * np.sqrt(periods_per_year) . returns[&quot;Equal Weight&quot;] = returns.mean(axis=1) avg_returns = annualize_rets(returns, 1) avg_vol = annualize_stdev(returns, 1) portfolio = pd.DataFrame({ &quot;Ann Returns&quot;: avg_returns, &quot;Ann Volatility&quot;: avg_vol }) portfolio . Ann Returns Ann Volatility . Stock A 0.00801 | 0.19218 | . Stock B 0.23693 | 0.13772 | . Stock C 0.18539 | 0.17600 | . Stock D 0.07670 | 0.23355 | . Stock E 0.07000 | 0.21160 | . Equal Weight 0.12846 | 0.08562 | . WOW! Look at what happened to our Volatility number. For the equal weight portfolio, our volatility has dropped dramatically. How is this possible? . Well, let&#39;s take a look at the first 3 year of returns and see what is happening. . returns.loc[0:2,] . Stock A Stock B Stock C Stock D Stock E Equal Weight . 0 0.36584 | 0.23576 | -0.16739 | 0.02286 | 0.35282 | 0.16198 | . 1 -0.05401 | 0.47785 | 0.21257 | 0.20396 | 0.15801 | 0.19968 | . 2 0.03674 | 0.29231 | 0.37857 | 0.43732 | -0.29406 | 0.17018 | . From these returns, we can see that the extreme return levels seen at the individual stock level are averaged out at the portfolio level, significantly reducing the volatility of the portfolio. . To be honest, this is bit of a contrived example. The reason is that we made the assumption, albeit without explicitly stating it, that all of these 5 stocks have no relationship to each other, or completely uncorrelated. As anyone who has watched the stock market at all, will know that isn&#39;t likely to be true. Do all stocks move perfectly together, no - but the overall direction of the market does have an strong effect on each individual stocks. This is called market risk. . Market risk is one of the two main types of risks that make up an individual stock&#39;s risk. The other being stock-specific or idiosyncratic risk. This risk can be an industry-driven risk, such as the price of oil, which may affect all airlines&#39; stock price, and can be stock-specific, such-as an indiviudal company missing the trend towards online shopping. This idiosyncratic risk is what we can diversify away by holding a variety of stocks from different industries with different specific risk, and what we will be left with is simply market risk. . Let&#39;s back to the how each stock&#39;s correlation with each other impacts volatility. As we can see from this somewhat intimidating equation for the volatility of a 2-stock portfolio: . $$ sigma^2(w_a, w_b) = sigma_A^2w_A^2 + sigma_B^2w_B^2 + 2w_Aw_B sigma_A sigma_B rho_{A,B} $$ . the last term of $2w_Aw_B sigma_A sigma_B rho_{A,B}$ is really what we are interested in. Here $ rho_{A,B}$ represents the correlation between Stock A and Stock B. As the correlation approaches 1, the volatility of the 2-stock portfolio with approach the average of the 2 stock&#39;s volatilies. Above, we showed an example of what happens at a correlation of 0, but if you can actually find two assets that move in opposite directions, this will have an even further reduction in the overall volatility of a portfolio. . Now, with this background under our belts, we will look at expanding this idea to various assets and look to construct an &quot;optimal&quot; portfolio. . The Efficient Frontier . In the above example, we only showed a portfolio constructed with an even amount allocated to each security. However, in the real world different investors will have different goals, and such with have different risk tolerance and reward targets. As such, we should be able to construct a variety of optimal portfolios for each risk tolerance. This concept is called the Efficient Frontier - a set of portfolios which provide the highest expected return for a given level of risk or vice-versa - lowest risk for an expected return. . If we generalize our 2-stock portfolio volatility formula to include n-securities, we can then solve for the optimal weights, $w_i$, for each risk level that provide the highest return. Let&#39;s look at his using the following 10 Exchanged-Traded-Funds to determine this efficient frontier. . IVV - Large Cap US Stocks | IJH - Mid Cap US Stocks | IJR - Small Cap US Stocks | IEFA - Devevloped International Stocks | EEM - Emerging Market Stocks | GOVT - US Treasury Bond | LQD - Investment-Grade Corporate Bonds | HYG - High Yield Bonds | IYR - Real Estate | GSG - Commodities | . We will import this data from Yahaoo Finance. inspect the beginning and end of the data and quickly chart the ETF prices. . tickers = [&quot;IVV&quot;, &quot;IJH&quot;, &quot;IJR&quot;, &quot;IEFA&quot;, &quot;EEM&quot;, &quot;GOVT&quot;, &quot;LQD&quot;, &quot;HYG&quot;, &quot;IYR&quot;, &quot;GSG&quot;] etf_prices = data.DataReader(tickers, data_source=&quot;yahoo&quot;, start=&quot;1/1/2015&quot;, end=&quot;12/31/2019&quot;)[&quot;Adj Close&quot;] etf_prices.head() . Symbols IVV IJH IJR IEFA EEM GOVT LQD HYG IYR GSG . Date . 2015-01-02 183.563370 | 132.017548 | 52.162510 | 47.162361 | 34.465927 | 22.891134 | 98.729385 | 65.700562 | 63.215065 | 21.219999 | . 2015-01-05 180.340149 | 130.036240 | 51.365459 | 46.103111 | 33.852528 | 22.927330 | 99.133095 | 65.091957 | 63.426365 | 20.620001 | . 2015-01-06 178.715271 | 128.593613 | 50.508526 | 45.624733 | 33.710285 | 23.035944 | 99.536781 | 64.842636 | 63.889561 | 20.280001 | . 2015-01-07 180.926254 | 130.337570 | 51.033741 | 46.094570 | 34.439262 | 23.035944 | 99.668640 | 65.245941 | 64.718445 | 20.219999 | . 2015-01-08 184.158295 | 132.346237 | 51.927528 | 46.683990 | 35.025986 | 22.981640 | 99.347290 | 65.737221 | 65.084122 | 20.290001 | . etf_prices.tail() . Symbols IVV IJH IJR IEFA EEM GOVT LQD HYG IYR GSG . Date . 2019-12-24 317.87265 | 202.80856 | 83.10228 | 64.21871 | 44.31289 | 25.40180 | 124.62210 | 83.80858 | 89.70977 | 16.24000 | . 2019-12-26 319.51791 | 203.25221 | 82.99363 | 64.47554 | 44.63104 | 25.43118 | 124.84631 | 83.93262 | 90.18898 | 16.39000 | . 2019-12-27 319.45880 | 202.91702 | 82.61832 | 64.63360 | 44.81000 | 25.46057 | 125.02174 | 83.89445 | 90.43347 | 16.38000 | . 2019-12-30 317.73474 | 202.71982 | 82.56893 | 64.15944 | 44.51173 | 25.44098 | 125.24596 | 83.83720 | 90.50192 | 16.33000 | . 2019-12-31 318.45389 | 202.91702 | 82.81585 | 64.44591 | 44.61115 | 25.41159 | 124.72932 | 83.91354 | 91.03003 | 16.21000 | . etf_norm = etf_prices / etf_prices.iloc[0] etf_norm.plot(figsize=(15, 7)); . Next, in order to calculate the Efficient Frontier, we need 2 inputs. These inputs are the annualized returns and the covariance matrix between each security. . First we will calculate the daily returns for the time series. From that, the covariance is calculated as $ sigma_A sigma_B rho_{A,B}$ or the variance between two stocks. Below, we will calculate the covariance matrix and have a look at the correlation matrix, which is a little more intuitive to understand. As a reminder, a correlation of 1 means that the stock move perfected together, and -1 means they move perfectly opposite each other. . We can see from the correlation heatmap, that the stock ETFs are highly correlated (close to 1) and the bond ETFs are negatively correlated with stocks (below 0). . etf_returns = etf_prices.pct_change() etf_returns.drop(etf_returns.index[0], axis=0, inplace=True) etf_returns . Symbols IVV IJH IJR IEFA EEM GOVT LQD HYG IYR GSG . Date . 2015-01-05 -0.017559 | -0.015008 | -0.015280 | -0.022460 | -0.017797 | 0.001581 | 0.004089 | -0.009263 | 0.003343 | -0.028275 | . 2015-01-06 -0.009010 | -0.011094 | -0.016683 | -0.010376 | -0.004202 | 0.004737 | 0.004072 | -0.003830 | 0.007303 | -0.016489 | . 2015-01-07 0.012372 | 0.013562 | 0.010399 | 0.010298 | 0.021625 | 0.000000 | 0.001325 | 0.006220 | 0.012974 | -0.002959 | . 2015-01-08 0.017864 | 0.015411 | 0.017514 | 0.012787 | 0.017036 | -0.002357 | -0.003224 | 0.007530 | 0.005650 | 0.003462 | . 2015-01-09 -0.008438 | -0.008486 | -0.009759 | -0.004575 | -0.003300 | 0.002757 | 0.002654 | 0.005020 | 0.000375 | -0.010843 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 2019-12-24 0.000124 | -0.000097 | 0.002741 | -0.000922 | -0.002239 | 0.001159 | 0.001018 | 0.000798 | 0.002733 | 0.004329 | . 2019-12-26 0.005176 | 0.002188 | -0.001307 | 0.003999 | 0.007180 | 0.001157 | 0.001799 | 0.001480 | 0.005342 | 0.009236 | . 2019-12-27 -0.000185 | -0.001649 | -0.004522 | 0.002451 | 0.004010 | 0.001156 | 0.001405 | -0.000455 | 0.002711 | -0.000610 | . 2019-12-30 -0.005397 | -0.000972 | -0.000598 | -0.007336 | -0.006656 | -0.000769 | 0.001794 | -0.000682 | 0.000757 | -0.003052 | . 2019-12-31 0.002263 | 0.000973 | 0.002990 | 0.004465 | 0.002234 | -0.001155 | -0.004125 | 0.000910 | 0.005835 | -0.007348 | . 1257 rows × 10 columns . cov_mat = etf_returns.cov() # Calculate and plot the correlation matrix using a Heatmap plt.figure(figsize=(8, 8)) sns.heatmap(etf_returns.corr(), annot=True, cmap=&quot;coolwarm&quot;); . Calculating the efficient frontier . As mentioned earlier, the efficient frontier is simply the various portfolios which have the lowest risk for a given return or the highest return for a given risk. Before we optimize our 10-ETF portfolio, let&#39;s look at a 2-security example to understand a little better. . We will create 2 news functions, which calculate porfolio returns based on the weights of each security and another which calcualtes the portfolio volatility which was that scary formula above. That formula can minimize to a few matrix-multiplications as seen in the function definition. . def portfolio_return(weights, returns): return weights.T @ returns def portfolio_vol(weights, cov_mat): return (weights.T @ cov_mat @ weights)**0.5 . For this example, we will use: . &#39;IVV&#39; - large cap US stocks | &#39;GOVT&#39; - US government bonds. | . We will calcualte their annualized returns and a covariance matrix. Next, we will simply build portfolios from 0% in stocks and 100% in bonds to 100% / 0% and do so in 5% increments. . From these weightings, we can then calculate the portfolio&#39;s return and risk, and then plot it. One important thing to take away from the plot, if these are the only 2 securities that you can invest in, you would never invest in 100% &#39;GOVT&#39;. The reason is that there are portfolios that &#39;dominate&#39; that portfolio. This means that other portfolios provide either less risk or more return, or in fact you can actually achieve both, by just sliding a few portfolios to the left. . This is the Efficient Frontier for a 2-security portfolio. . port_2 = etf_returns[[&quot;IVV&quot;, &quot;GOVT&quot;]] port_2_rets = annualize_rets(port_2, 252) cov_2 = port_2.cov() . n_points = 21 weights = [ np.array([w, 1-w]) for w in np.linspace(0, 1, n_points) ] port_returns = [portfolio_return(w, port_2_rets) for w in weights] port_vol = [portfolio_vol(w, cov_2) for w in weights] ef = pd.DataFrame({ &quot;Return&quot;: port_returns, &quot;Vol&quot;: port_vol }) . ef.plot.line(x=&quot;Vol&quot;, y=&quot;Return&quot;, figsize=(12,8), style=&quot;.-&quot;, legend=None) plt.title(&quot;Efficient Frontier for 2-Security Portfolio&quot;); . Efficient Frontier - Multi-Asset . To build the efficient frontier for a multi-asset portfolio, we need to create an optimiztion function. Thankfully, scipy provides an optimization function, minimize. . First, we will create the minimization routine. We set a variety of constraints; each weight must be bound between 0 and 1, weights must sum to 1, and we will set our target to a given return. From this we will optimize the weights to minimize the portfolio_vol using Quadratic Programming. . Next, we will create a function which returns a list of weights given a list of returns by calling that minimuzation routine. The list of returns will be created by simply identifying the minimum and maximum return from the series and creating a linearly spaced array along the possible returns. . Lastly, we will createa function which plots those results. . def minimize_vol(target_return, exp_rets, cov_mat): n = len(exp_rets) init_guess = np.repeat(1/n, n) # equal weight as initial guess bounds = ((0.0, 1.0),) * n # creates a tuple of tuples with a range for weights for each weight # constraints for optimizer; type = &#39;eq&#39;, and &#39;fun&#39; are solved for equal to 0 return_is_target = { &#39;type&#39;: &#39;eq&#39;, &#39;args&#39;: (exp_rets,), &#39;fun&#39;: lambda weights, exp_rets: target_return - portfolio_return(weights, exp_rets) } weights_sum_to_1 = { &#39;type&#39;: &#39;eq&#39;, &#39;fun&#39;: lambda weights: np.sum(weights) - 1 } # run the optimizer results = minimize(portfolio_vol, init_guess, args=(cov_mat,), method=&quot;SLSQP&quot;, options={&#39;disp&#39;: False}, constraints=(return_is_target, weights_sum_to_1), bounds=bounds ) return results.x def optimal_weights(n_points, exp_rets, cov_mat): target_rets = np.linspace(max(exp_rets.min(),0), exp_rets.max(), n_points) weights = [minimize_vol(target_return, exp_rets, cov_mat) for target_return in target_rets] return weights def plot_ef(n_points, exp_rets, cov_mat, figsize=(10, 7)): weights = optimal_weights(n_points, exp_rets, cov_mat) rets = [portfolio_return(w, exp_rets) for w in weights] vols = [portfolio_vol(w, cov_mat) for w in weights] ef = pd.DataFrame({ &quot;Returns&quot;: rets, &quot;Volatility&quot;: vols }) return ef.plot.line(x=&quot;Volatility&quot;, y=&quot;Returns&quot;, style=&quot;.-&quot;, figsize=figsize) . Now that we have created all the tools we need to calculate and plot the efficient frontier. Let&#39;s see how the efficient frontier is mapped to our 10-etf portfolio. We will calculate the annualized returns and covariance matrix and plot the results for 10 different optimal portfolios. . Further, we will display the weights for each of the portfolios created. Here, we see the first flaw in the efficient frontier. All the portfolios are only allocated to a few ETFs. In fact, IEFA, IYR and EEM are never assigned any weights. So, the inputs or historical time period over which the inputs are calculated can drive the allocation dramatically. . In the 2nd weight matrix, we simply change the expected return for IVV to 8.5% and you can see that it dramatically shifts the allocations, now EEM is the only security to not receive an allocation. . etf_ann_rets = annualize_rets(etf_returns, 252) etf_cov_mat = etf_returns.cov() plot_ef(10, etf_ann_rets, etf_cov_mat); . pd.DataFrame(optimal_weights(10, etf_ann_rets, etf_cov_mat), columns=etf_returns.columns) . Symbols IVV IJH IJR IEFA EEM GOVT LQD HYG IYR GSG . 0 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.71295 | 0.00000 | 0.00000 | 0.00000 | 0.28705 | . 1 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.88015 | 0.00000 | 0.00631 | 0.00000 | 0.11354 | . 2 0.00000 | 0.00054 | 0.02014 | 0.00000 | 0.00000 | 0.75004 | 0.00000 | 0.19569 | 0.00000 | 0.03358 | . 3 0.08731 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.58608 | 0.04059 | 0.28602 | 0.00000 | 0.00000 | . 4 0.16782 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.30057 | 0.34171 | 0.18990 | 0.00000 | 0.00000 | . 5 0.30462 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.15142 | 0.54397 | 0.00000 | 0.00000 | 0.00000 | . 6 0.43420 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.56580 | 0.00000 | 0.00000 | 0.00000 | . 7 0.62280 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.37720 | 0.00000 | 0.00000 | 0.00000 | . 8 0.81140 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.18860 | 0.00000 | 0.00000 | 0.00000 | . 9 1.00000 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | . ex_ann_rets = etf_ann_rets ex_ann_rets[&#39;IVV&#39;] = 0.085 pd.DataFrame(optimal_weights(10, ex_ann_rets, etf_cov_mat), columns=etf_returns.columns) . Symbols IVV IJH IJR IEFA EEM GOVT LQD HYG IYR GSG . 0 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.71295 | 0.00000 | 0.00000 | 0.00000 | 0.28705 | . 1 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.85930 | 0.00000 | 0.00000 | 0.00000 | 0.14070 | . 2 0.00416 | 0.00000 | 0.03216 | 0.00000 | 0.00000 | 0.81500 | 0.00000 | 0.08429 | 0.00000 | 0.06438 | . 3 0.00301 | 0.00131 | 0.04721 | 0.00289 | 0.00000 | 0.69785 | 0.00000 | 0.24773 | 0.00000 | 0.00000 | . 4 0.03720 | 0.02238 | 0.03875 | 0.00000 | 0.00000 | 0.35650 | 0.32046 | 0.22472 | 0.00000 | 0.00000 | . 5 0.02025 | 0.00000 | 0.12191 | 0.00000 | 0.00000 | 0.04324 | 0.64600 | 0.16861 | 0.00000 | 0.00000 | . 6 0.08178 | 0.06723 | 0.22194 | 0.00000 | 0.00000 | 0.00000 | 0.62905 | 0.00000 | 0.00000 | 0.00000 | . 7 0.14925 | 0.15687 | 0.25037 | 0.00000 | 0.00000 | 0.00000 | 0.33124 | 0.00000 | 0.11227 | 0.00000 | . 8 0.21771 | 0.24764 | 0.30797 | 0.00000 | 0.00000 | 0.00000 | 0.05659 | 0.00000 | 0.17008 | 0.00000 | . 9 0.00000 | 0.00000 | 1.00000 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | . Conclusion . We have seen the potential benefits of diversifcations, and the theoretical framework for creating an optimal portfolio, however, this comes with signficant drawbacks. In a future post, we will look at an alternative allocation strategy called the Global Minimum Variance Portfolio (along with variants of this idea), as well as using the Black Litterman approach to assigning weights to each security in a portfolio. . References: . EDHEC/Coursera - Investment Management with Python and Machine Learning Investopedia - https://www.investopedia.com/terms/e/efficientfrontier.asp Investopedia - https://www.investopedia.com/terms/m/modernportfoliotheory.asp .",
            "url": "https://scottknappnj.github.io/FastPagesBlog/2020/12/07/Markowitz-Efficient-Frontier-and-Alternatives.html",
            "relUrl": "/2020/12/07/Markowitz-Efficient-Frontier-and-Alternatives.html",
            "date": " • Dec 7, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Tuning Random Forest Models",
            "content": "On a weekly basis, a dataset is released for the R community to create visualizations and perform other analysis. Julia Silge, of R-Studio, produces a periodic tutorial on the Tidymodels framework utilizing these datasets. Here we will replicate her analysis in Python. . This week we have a dataset from IKEA. We will attempt to predict prices using a Random Forest model. There is a relatively small dataset 3,694 items with 13 features. Of these 13 features many are just descriptive of the item (an index, item_id, name, web link, online availability, color_availablity, designer). We have identified 4 variables which may offer some predictive capacity, 3 numeric (depth, height, weight) and 1 categorical (category). We will do some feature engineering with these variables, including imputation using K-Nearest-Neighbors (KNN), and One-Hot encoding. . Let&#39;s begin. . Setup and Data Import . import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from sklearn.model_selection import train_test_split from sklearn.impute import KNNImputer from sklearn.preprocessing import OneHotEncoder from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_absolute_error from sklearn.model_selection import RandomizedSearchCV from sklearn.model_selection import GridSearchCV %matplotlib inline print(&quot;Setup Complete&quot;) . Setup Complete . We will import the data from the website provided and then perform for data cleaning and exploratory data anaylsis. . It is usually a good idea to inspeact the head and tail of the dataset to for a first check that the import was successful. | We will drop the Unnamed: 0 column, as it appears to just be an index. | . One thing that you will notice throughout is the constant use a .shape. This is used as a quick check to ensure that we are at least returning a shape that&#39;s expected. . ikea_raw = pd.read_csv(&quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-11-03/ikea.csv&quot;) ikea_raw.head() . Unnamed: 0 item_id name category price old_price sellable_online link other_colors short_description designer depth height width . 0 0 | 90420332 | FREKVENS | Bar furniture | 265.0 | No old price | True | https://www.ikea.com/sa/en/p/frekvens-bar-tabl... | No | Bar table, in/outdoor, 51x51 cm | Nicholai Wiig Hansen | NaN | 99.0 | 51.0 | . 1 1 | 368814 | NORDVIKEN | Bar furniture | 995.0 | No old price | False | https://www.ikea.com/sa/en/p/nordviken-bar-tab... | No | Bar table, 140x80 cm | Francis Cayouette | NaN | 105.0 | 80.0 | . 2 2 | 9333523 | NORDVIKEN / NORDVIKEN | Bar furniture | 2095.0 | No old price | False | https://www.ikea.com/sa/en/p/nordviken-nordvik... | No | Bar table and 4 bar stools | Francis Cayouette | NaN | NaN | NaN | . 3 3 | 80155205 | STIG | Bar furniture | 69.0 | No old price | True | https://www.ikea.com/sa/en/p/stig-bar-stool-wi... | Yes | Bar stool with backrest, 74 cm | Henrik Preutz | 50.0 | 100.0 | 60.0 | . 4 4 | 30180504 | NORBERG | Bar furniture | 225.0 | No old price | True | https://www.ikea.com/sa/en/p/norberg-wall-moun... | No | Wall-mounted drop-leaf table, ... | Marcus Arvonen | 60.0 | 43.0 | 74.0 | . ikea_raw.tail() . Unnamed: 0 item_id name category price old_price sellable_online link other_colors short_description designer depth height width . 3689 3689 | 99157902 | ELVARLI | Wardrobes | 750.0 | SR 820 | True | https://www.ikea.com/sa/en/p/elvarli-1-section... | No | 1 section, 92x51x222-350 cm | Ehlén Johansson | 50.0 | NaN | 91.0 | . 3690 3690 | 9158152 | ELVARLI | Wardrobes | 1572.0 | SR 1,755 | True | https://www.ikea.com/sa/en/p/elvarli-2-section... | No | 2 sections, 135x51x222-350 cm | Ehlén Johansson | 50.0 | NaN | 135.0 | . 3691 3691 | 59157541 | ELVARLI | Wardrobes | 924.0 | SR 1,050 | True | https://www.ikea.com/sa/en/p/elvarli-2-section... | No | 2 sections, 175x51x222-350 cm | Ehlén Johansson | 50.0 | NaN | 175.0 | . 3692 3692 | 89157573 | ELVARLI | Wardrobes | 2745.0 | SR 3,130 | True | https://www.ikea.com/sa/en/p/elvarli-3-section... | No | 3 sections, 178x51x222-350 cm | Ehlén Johansson | 50.0 | NaN | 178.0 | . 3693 3693 | 69157376 | ELVARLI | Wardrobes | 1231.0 | SR 1,535 | True | https://www.ikea.com/sa/en/p/elvarli-2-section... | No | 2 sections, 175x51x222-350 cm | Ehlén Johansson | 50.0 | NaN | 175.0 | . ikea = ikea_raw.drop(&quot;Unnamed: 0&quot;, axis=1) . ikea.shape . (3694, 13) . After this initial cursory look at the data. We next look to identify the completeness of our data. We see that we have quite of few NAs, from both a table perspective and using a heatmap to see the distribution of the NAs. Visually, it looks like we may be able to preserve some of the incomplete observations by imputing the missing values. This is beacuse the missing data is all in the dimensions of the product. If an item is only missing a single dimension, we can attempt to impute the missing value given the other 2 dimenstions using KNN, this will be discusseed in more depth later on. Unfortunately, we will have to drop those items which are missing 2 or more dimensions. . ikea.isna().sum() . item_id 0 name 0 category 0 price 0 old_price 0 sellable_online 0 link 0 other_colors 0 short_description 0 designer 0 depth 1463 height 988 width 589 dtype: int64 . plt.figure(figsize=(10, 10)) sns.heatmap(ikea.isna()); . drop_index = ikea.isna().sum(axis=1) &gt; 1 ikea.drop(ikea[drop_index].index).isna().sum() . item_id 0 name 0 category 0 price 0 old_price 0 sellable_online 0 link 0 other_colors 0 short_description 0 designer 0 depth 692 height 251 width 24 dtype: int64 . ikea.drop(ikea[drop_index].index, inplace=True) . Data Visualizations . Next, we will quickly tale a look at our features as compared to our target variable. When we look at the price variable, we see that it ranges from a minimum of 5 to a maximum of 9585. To contend with this large range, we will do a log-transformation on the price. We will quickly chart the target vs each or the dimensions and against the various categories. . For the categpry feature, we see that we have 17 different categories, likely too many for our model given the small number of observations, we will take the 11 largest categories and combine the remaining categories into one called other. . From all these visuals, there does not appear to be great predictive value in any one feature, so we must temper our expectations of our model predictions. . ikea[&#39;price&#39;].describe() . count 2866.000000 mean 1113.905652 std 1401.535864 min 5.000000 25% 220.000000 50% 585.000000 75% 1468.000000 max 9585.000000 Name: price, dtype: float64 . ikea[&quot;price_log&quot;] = np.log(ikea[&quot;price&quot;]) . fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (15, 7)) ax1.scatter(&quot;depth&quot;, &quot;price_log&quot;, data=ikea, color = &quot;red&quot;) ax1.set_xlabel(&quot;Depth&quot;) ax2.scatter(&quot;height&quot;, &quot;price_log&quot;, data=ikea, color = &quot;green&quot;) ax2.set_xlabel(&quot;Height&quot;) ax3.scatter(&quot;width&quot;, &quot;price_log&quot;, data=ikea, color = &quot;blue&quot;) ax3.set_xlabel(&quot;Width&quot;); . ikea[&quot;category&quot;].value_counts() . Bookcases &amp; shelving units 493 Tables &amp; desks 390 Chairs 334 Sofas &amp; armchairs 271 Cabinets &amp; cupboards 254 Wardrobes 233 TV &amp; media furniture 152 Beds 150 Outdoor furniture 148 Chests of drawers &amp; drawer units 123 Children&#39;s furniture 111 Nursery furniture 87 Bar furniture 37 Trolleys 27 Sideboards, buffets &amp; console tables 23 Café furniture 21 Room dividers 12 Name: category, dtype: int64 . top_11_cats = ikea[&quot;category&quot;].value_counts().head(11).index ikea[&quot;category&quot;] = ikea[&quot;category&quot;].apply(lambda cat: &quot;Other&quot; if cat not in top_11_cats else cat) ikea[&quot;category&quot;].value_counts() . Bookcases &amp; shelving units 493 Tables &amp; desks 390 Chairs 334 Sofas &amp; armchairs 271 Cabinets &amp; cupboards 254 Wardrobes 233 Other 207 TV &amp; media furniture 152 Beds 150 Outdoor furniture 148 Chests of drawers &amp; drawer units 123 Children&#39;s furniture 111 Name: category, dtype: int64 . plt.figure(figsize=(15, 7)) plt.xticks(rotation=90) plt.bar(&quot;category&quot;, &quot;price_log&quot;, data=ikea); . Data Preparation . We have done some feature engineering above. But we will need to category to ensure that our data is prepared to be entered into a model. . First, we will subset our dataset into the X (predictors or features) and y (target). We will then create a training set, in which we will use to fit the model, and a test set, over which we will evaluate the model. . Numerical variables For the numerical variables, we still need to replace our missing values. As mentioned we will use the KNN model to impute the missing values from the known dimensions. In the KNN algorithm, we will use 3 nearest neighbors, meaning that we will find the 3 closest obeservations to determine the missing value. The closest observations will be found by calculating the eclidian distance between each observation and then simply using the 3 closest to imput the missing value. Note: When performing the transformation, we fit and transform the training set, but then simply use that fit to only transform the test set. . Categorical variables For the categorical variables, we will use One-Hot Encoding to convert the categories into numeric values. In One-Hot Encoding, we simply create a dummy feature for each category with a value of 1 or 0 for each variable depending if the category pertains to that observation or not. . Lastly, we concatenate the numeric and categorical feature back into a single DataFrame ready for modeling. . X = ikea[[&#39;category&#39;, &#39;depth&#39;, &#39;height&#39;, &#39;width&#39;]] y = ikea[&#39;price_log&#39;] X.shape, y.shape . ((2866, 4), (2866,)) . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42) X_train.shape, X_test.shape, y_train.shape, y_test.shape . ((2149, 4), (717, 4), (2149,), (717,)) . num_cats = [&#39;depth&#39;, &#39;height&#39;, &#39;width&#39;] X_train_num = X_train[num_cats] X_test_num = X_test[num_cats] # perform the imputation using KNN, with 3 neighbors (copy=False allows for in-place transformations) imputer = KNNImputer(n_neighbors=3, copy=False) imputer.fit_transform(X_train_num) imputer.transform(X_test_num) X_train_num.shape, X_test_num.shape . ((2149, 3), (717, 3)) . X_train_num.isnull().sum(), X_test_num.isnull().sum() . (depth 0 height 0 width 0 dtype: int64, depth 0 height 0 width 0 dtype: int64) . One_Hot = OneHotEncoder(handle_unknown=&#39;ignore&#39;, sparse=False) X_train_cat = X_train[[&#39;category&#39;]] X_test_cat = X_test[[&#39;category&#39;]] X_train_cat = pd.DataFrame(One_Hot.fit_transform(X_train_cat)) X_test_cat = pd.DataFrame(One_Hot.transform(X_test_cat)) # One-Hot Encoding drops the index, so we need to replace it X_train_cat.index = X_train.index X_test_cat.index = X_test.index X_train_cat.shape, X_test_cat.shape . ((2149, 12), (717, 12)) . X_train_trans = pd.concat([X_train_num, X_train_cat], axis=1) X_test_trans = pd.concat([X_test_num, X_test_cat], axis=1) X_train_trans.shape, X_test_trans.shape . ((2149, 15), (717, 15)) . Train the Model . Now, that our data has been transformed for modeling, we can run our initial base model. Here we will use n_estimators of 200 (we will discuss this in the tuning section). We will then use that fitted model to make predictions on our test data. Those predictions will allow us to evaluate our model. We will use a simple Mean Absolute Error for evaluation. We must remember to transform our predictions back so that the MAE makes more intuitive sense. . We didn&#39;t have high expectations for the model, and a MAE of 396 or 35% isn&#39;t very impressive. Next, we will see if we tuning the model improves our predictive capacity. . rf_model = RandomForestRegressor(n_estimators=200, random_state=42) rf_model.fit(X_train_trans, y_train) preds = rf_model.predict(X_test_trans) mean_absolute_error(y_test, preds) . 0.4496975465145763 . mean_absolute_error(np.exp(y_test), np.exp(preds)), ikea[&#39;price&#39;].mean() . (396.9394406603129, 1113.90565247732) . Tune the Model . Here we will look to tune the model&#39;s hyperparamters. The Random Forest model has a variety of tuning parameters, we will focus on 4 here. . n_estimators: the number of trees that will be constructed | max_depth: maximum number of levels of each tree | min_samples_split: minimum number of items requires to split at a node | min_samples_leaf: minimum number of items at each leaf node | . Other parameters that can be tuned: . max_features: which is the number of features to be considered at each split | bootstrap: whether to use bootstrapping to selct the samples. | . When tuning you provide a range of inputs that you want the model to calculate over, based on what is chosen below this would leave $6*10*3*3$ or 540 iterations of the model. Rather than doing each iteration which would be done using GridSearchCV, we will utilize RandomizedSearchCV which will perform a choose random searches up to the defined number of iterations, we will run 50. . In addition, in order to find the best parameters, we will utilize cross-validation with 5 folds. This will divide our training set into 5 groups, in the first run, it will train on groups (1,2,3,4) and evaluate on 5, in the 2nd run, it will train on groups (1,2,3,5) and test on 4...etc. We will perform this cross-validation on each of the 50 randomly choosen parameter sets and the model will output the best set of parameters for us to use. . n_estimators = [int(x) for x in np.linspace(start=200,stop=1200, num=6)] max_depth = [int(x) for x in np.linspace(10, 50, 10)] min_samples_split = [2, 5, 10] min_samples_leaf = [1, 2, 4] random_grid = {&#39;n_estimators&#39;: n_estimators, &#39;max_depth&#39;: max_depth, &#39;min_samples_split&#39;: min_samples_split, &#39;min_samples_leaf&#39;: min_samples_leaf } random_grid, (6*10*3*3) . ({&#39;n_estimators&#39;: [200, 400, 600, 800, 1000, 1200], &#39;max_depth&#39;: [10, 14, 18, 23, 27, 32, 36, 41, 45, 50], &#39;min_samples_split&#39;: [2, 5, 10], &#39;min_samples_leaf&#39;: [1, 2, 4]}, 540) . rf = RandomForestRegressor(verbose=0) rf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid, n_iter=50, cv=5, verbose=0, random_state=42) rf_random.fit(X_train_trans, y_train) . RandomizedSearchCV(cv=5, estimator=RandomForestRegressor(), n_iter=50, param_distributions={&#39;max_depth&#39;: [10, 14, 18, 23, 27, 32, 36, 41, 45, 50], &#39;min_samples_leaf&#39;: [1, 2, 4], &#39;min_samples_split&#39;: [2, 5, 10], &#39;n_estimators&#39;: [200, 400, 600, 800, 1000, 1200]}, random_state=42) . rf_model.estimators_ . [DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1608637542), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1273642419), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1935803228), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=787846414), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=996406378), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1201263687), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=423734972), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=415968276), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=670094950), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1914837113), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=669991378), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=429389014), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=249467210), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1972458954), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1572714583), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1433267572), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=434285667), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=613608295), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=893664919), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=648061058), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=88409749), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=242285876), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=2018247425), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=953477463), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1427830251), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1883569565), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=911989541), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=3344769), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=780932287), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=2114032571), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=787716372), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=504579232), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1306710475), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=479546681), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=106328085), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=30349564), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1855189739), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=99052376), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1250819632), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=106406362), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=480404538), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1717389822), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=599121577), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=200427519), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1254751707), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=2034764475), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1573512143), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=999745294), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1958805693), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=389151677), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1224821422), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=508464061), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=857592370), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1642661739), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=61136438), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=2075460851), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=396917567), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=2004731384), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=199502978), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1545932260), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=461901618), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=774414982), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=732395540), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1934879560), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=279394470), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=56972561), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1927948675), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1899242072), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1999874363), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=271820813), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1324556529), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1655351289), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1308306184), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=68574553), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=419498548), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=991681409), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=791274835), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1035196507), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1890440558), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=787110843), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=524150214), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=472432043), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=2126768636), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1431061255), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=147697582), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=744595490), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1758017741), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1679592528), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1111451555), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=782698033), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=698027879), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1096768899), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1338788865), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1826030589), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=86191493), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=893102645), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=200619113), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=290770691), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=793943861), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=134489564), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=2016850622), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1470101905), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1181686489), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1931679275), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1887633569), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1697157321), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1695770557), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1832485859), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=420477197), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=976125790), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1811936047), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1402481934), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=380072391), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=302554573), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=841739990), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=89482491), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=194249720), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1980718781), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1397283111), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1479761620), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1669356239), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1062231788), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1165435217), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=170476398), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1411916852), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=372593431), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1532243865), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1998256344), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1206604539), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=459708603), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=183378299), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1185407468), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=605264936), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1272485020), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1297926158), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=709816108), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=320192576), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=67157848), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=2091163462), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1818495496), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1169282391), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1696003200), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=853477355), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1260522119), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=23717335), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=60472382), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1354896522), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=854021618), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=888445520), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=907706759), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=983578274), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1246294434), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1165097248), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=455094650), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=318019332), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1830948329), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1539598566), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=648870905), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=497653800), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1782238235), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1559517318), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1503404232), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=529561415), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1930375947), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1421196193), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=409783328), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=272981039), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1592652278), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1335658902), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=725167677), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1396651735), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=712631076), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=986151010), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=392121003), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=590804839), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1179921109), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1663066074), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=263038498), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=2028147648), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1644658402), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=513653348), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=2025988014), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=915879373), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1498573442), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1120063232), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=952321028), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=263183577), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1013547510), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1163795198), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1099805069), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=2120835942), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=173660954), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=97636744), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=904790222), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1836274702), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=476272473), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=109174313), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=1886935931), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=463390156), DecisionTreeRegressor(max_features=&#39;auto&#39;, random_state=866377394)] . rf_random.best_params_ . {&#39;n_estimators&#39;: 1200, &#39;min_samples_split&#39;: 2, &#39;min_samples_leaf&#39;: 1, &#39;max_depth&#39;: 23} . rf_best= rf_random.best_estimator_ preds = rf_best.predict(X_test_trans) mean_absolute_error(np.exp(y_test), np.exp(preds)) . 397.27203830019045 . plt.figure(figsize=(12, 6)) plt.scatter(y_test, preds, alpha=0.6) plt.title(&quot;Predictions vs Target Variables&quot;) plt.xlabel(&quot;Target&quot;) plt.ylabel(&quot;Predictions&quot;); . Conclusion: . Visually, our model seems to have done a reasonable just at estimating price, even if our MAE is still near 400. And unfortunately, tuning the model did not improve the predictions on our test data. This isn&#39;t entirely suprising given the preliminary analysis we did in the beginning. However, we were able to work through some feature engineering and do some model tuning along the way. . References: . Tune random forests for #TidyTuesday IKEA prices: https://juliasilge.com/blog/ikea-prices/ . Hyperparameter Tuning the Random Forest in Python: https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 .",
            "url": "https://scottknappnj.github.io/FastPagesBlog/2020/12/05/Tuning-Random-Forests.html",
            "relUrl": "/2020/12/05/Tuning-Random-Forests.html",
            "date": " • Dec 5, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Using Python OOP to Create HTML Code",
            "content": "We will utilize Python&#39;s Obejct-Oriented-Programming to ease the ability to construct some simple HTML code. First, let&#39;s take a look at what is needed for HTML. An HTML 4 documents contains three elements: . 1) a line containing the version information of HTML being used 2) a header section enclosed in &lt;HEAD&gt; tags 3) a body section, where contains the document&#39;s content, enclosed in &lt;BODY&gt; tags . Here&#39;s an example of an HTML document: . &lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.01//EN&quot; &quot;http://www.w3.org/TR/html4/strict.dtd&quot;&gt; &lt;HTML&gt; &lt;HEAD&gt; &lt;TITLE&gt;My first HTML document&lt;/TITLE&gt; &lt;/HEAD&gt; &lt;BODY&gt; &lt;P&gt;Hello world! &lt;/BODY&gt; &lt;/HTML&gt; . From this we can start to see what we need in order to create this simple code. . We will begin by creating a Tag class. We will use this class across the classes we will create below. The Tag class will take a name and contents as input. From these, __init__ will create the start_tag, the contents, and the end_tag. This will allow us to create a __str__ and display method to print the Tag. . class Tag(object): def __init__(self, name, contents): self.start_tag = &#39;&lt;{}&gt;&#39;.format(name) self.end_tag = &#39;&lt;/{}&gt;&#39;.format(name) self.contents = contents def __str__(self): return &quot;{0.start_tag}{0.contents}{0.end_tag}&quot;.format(self) def display(self, file=None): print(self, file=file) . With the Tag class created, we can now use Inheritance to create the following classes: DocType, Head and Body. . The DocType class will need to adjust the use of Tag slightly. Using super() to inherit the Tag methods, we specify name as our doctype, leaving contents as a blank string. We will also have to overwrite the end_tag to a blank string to construct the DocType correctly. . class DocType(Tag): def __init__(self): super().__init__(&#39;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.01//EN&quot; http://www.w3.org/TR/html4/strict.dtd&#39;, &#39;&#39;) . Next, we will create our Head class. Here, we will again utilize super() to inherit the Tag class, but we will include the ability to add a title. We will initialize title equal to None and include an if statement to include the information if provided. . class Head(Tag): def __init__(self, title=None): super().__init__(&#39;HEAD&#39;, &#39;&#39;) if title: self._title_tag = Tag(&#39;TITLE&#39;, title) self.contents = str(self._title_tag) . We will continue on, creating our Body class. Here, we will be including an add_tag method to provide the ability to include a variety of tags to the Body. This will be done by appending the new_tag to the _body_contents list. Additionally, we will have to overwrite the display method as now, we will have to loop through the _body_contents list to print. . This is a good place for a quick aside on the leading _ that is used throughout the code. By including a leading underscore, such as with _body_contents, Python does not display this as a potential object to be called. It is still accessible if the user enters the full object name, but by not displaying it as a possible object, it reduces the likelihood that a user will directly change this object. . class Body(Tag): def __init__(self): super().__init__(&#39;BODY&#39;, &#39;&#39;) self._body_contents = [] def add_tag(self, name, contents): new_tag = Tag(name, contents) self._body_contents.append(&quot; n t&quot;) self._body_contents.append(new_tag) def display(self, file=None): for tag in self._body_contents: self.contents += str(tag) super().display(file=file) . Lastly, we will combine all these to create our HtmlDoc class, which will combine the use of Composition and Aggregation. First we will define our class as needing two inputs, head and body. These will be instantiated and defined by the user and then fed into the HtmlDoc object. The use of a created class as an input into another class is known as Aggregation. Composition is when you use another class during the construction of the class, as we will do with the DocType() class. We set the doc_type this way as it is unlikely that we will need to amend the contents of DocType so we can hard-code it into the HtmlDoc construction and away from the user. . We have added the add_tag method which allows you to add to Body of the document if you&#39;d like. And created an expanded Display method to produce our final code. . class HtmlDoc(object): def __init__(self, head, body): self._doc_type = DocType() self._head = head self._body = body def add_tag(self, name, contents): self._body.add_tag(name, contents) def display(self, file=None): self._doc_type.display(file=file) print(&#39;&lt;HTML&gt;&#39;, file=file) self._head.display(file=file) self._body.display(file=file) print(&#39;&lt;/HTML&gt;&#39;, file=file) . Now that we&#39;ve created all the necessary classes, let&#39;s test it out. We will display the text here, as well as create a &#39;test.html&#39; file, which we can open in our browser to confirm everything is working. . We include the . if __name__ == &#39;__main__&#39;: . to ensure that if this module was imported into another one, this code isn&#39;t run. . if __name__ == &#39;__main__&#39;: new_header = Head(&#39;HTML Test Document&#39;) new_body = Body() new_body.add_tag(&#39;h1&#39;, &#39;HTML Code&#39;) new_body.add_tag(&#39;h2&#39;, &#39;Using Python OOP&#39;) new_body.add_tag(&#39;p&#39;, &#39;This code uses Python OOP to create HTML Code&#39;) new_body.add_tag(&#39;p&#39;, &#39;The code utulizes the concepts of Inheritance, Composition and Aggregation&#39;) new_page = HtmlDoc(new_header, new_body) new_page.display() with open(&#39;test.html&#39;, &#39;w&#39;) as test_doc: new_page.display(file=test_doc) . &lt;!DOCTYPE HTML PUBLIC &#34;-//W3C//DTD HTML 4.01//EN&#34; http://www.w3.org/TR/html4/strict.dtd&gt;&lt;/!DOCTYPE HTML PUBLIC &#34;-//W3C//DTD HTML 4.01//EN&#34; http://www.w3.org/TR/html4/strict.dtd&gt; &lt;HTML&gt; &lt;HEAD&gt;&lt;TITLE&gt;HTML Test Document&lt;/TITLE&gt;&lt;/HEAD&gt; &lt;BODY&gt; &lt;h1&gt;HTML Code&lt;/h1&gt; &lt;h2&gt;Using Python OOP&lt;/h2&gt; &lt;p&gt;This code uses Python OOP to create HTML Code&lt;/p&gt; &lt;p&gt;The code utulizes the concepts of Inheritance, Composition and Aggregation&lt;/p&gt;&lt;/BODY&gt; &lt;/HTML&gt; . OK. Everything looks good. We could do a little work on the spacing to make everything more readable, but this is simply a cosmetic issue, as HTML does not use new lines or tabs in compliing the code. So, all-in-all, we learn a little Object-Oriented-Programming concepts while learning some HTML along the way. . References: . Learn Python Programming Masterclass - Udemy - Tim Buchalka | The Global Structure of an HTML document - https://www.w3.org/TR/html401/struct/global.html | .",
            "url": "https://scottknappnj.github.io/FastPagesBlog/2020/12/03/Using-Python-OOP-to-Create-HTML.html",
            "relUrl": "/2020/12/03/Using-Python-OOP-to-Create-HTML.html",
            "date": " • Dec 3, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Equal Weighted vs Value Weighted Portfolio Construction",
            "content": "import pandas as pd import numpy as np from scipy.stats import norm import matplotlib.pyplot as plt %matplotlib inline . . The Data . Dartmouth College, through their Fama-French Data Library, offers an extensive array of raw data and factor portfolios going back to 1926. We will be utilizing the 49 Industry Portfolio dataset, analzying monthly data from January, 2000 to September, 2020. This time period will encompass 3 full market cycles, which will enable a more robust analysis of the various allocation methodologies. . The data can be downloaded here: https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html . We wil begin by importing monthly return data for both the value-weighted and equal-weighted company portfolios. These portfolios denote the weighting of the individual company in each industry. For example, if there are 44 firms in the Cnstr industry, the equal-weighted portfolio will assume an allocation of $ frac{1}{44}$, or more generally $ frac{1}{N}$, to each company. . Additionally, we will import the datasets for both the average firm size in each industry and the number of firms in each industry to be able to calculate a cap-weighted (value-weighted) index. . (This is not to be confused with the above. First, we separate how each industry is weighted to each company; then we determine how we allocate to each industry; value vs equal). | . m_vw_rets = pd.read_csv(&#39;data/ind49_m_vw_rets.csv&#39;, header=0, index_col=0, parse_dates=True) / 100 # convert the index to equal the date, for time-series analysis m_vw_rets.index = pd.to_datetime(m_vw_rets.index, format=&quot;%Y%m&quot;).to_period(&#39;M&#39;) # eliminate white space in column names for easier indexing m_vw_rets.columns = m_vw_rets.columns.str.strip() m_vw_rets = m_vw_rets[&quot;2000&quot;:] . The Industries that this dataset uses are: . m_vw_rets.columns . Index([&#39;Agric&#39;, &#39;Food&#39;, &#39;Soda&#39;, &#39;Beer&#39;, &#39;Smoke&#39;, &#39;Toys&#39;, &#39;Fun&#39;, &#39;Books&#39;, &#39;Hshld&#39;, &#39;Clths&#39;, &#39;Hlth&#39;, &#39;MedEq&#39;, &#39;Drugs&#39;, &#39;Chems&#39;, &#39;Rubbr&#39;, &#39;Txtls&#39;, &#39;BldMt&#39;, &#39;Cnstr&#39;, &#39;Steel&#39;, &#39;FabPr&#39;, &#39;Mach&#39;, &#39;ElcEq&#39;, &#39;Autos&#39;, &#39;Aero&#39;, &#39;Ships&#39;, &#39;Guns&#39;, &#39;Gold&#39;, &#39;Mines&#39;, &#39;Coal&#39;, &#39;Oil&#39;, &#39;Util&#39;, &#39;Telcm&#39;, &#39;PerSv&#39;, &#39;BusSv&#39;, &#39;Hardw&#39;, &#39;Softw&#39;, &#39;Chips&#39;, &#39;LabEq&#39;, &#39;Paper&#39;, &#39;Boxes&#39;, &#39;Trans&#39;, &#39;Whlsl&#39;, &#39;Rtail&#39;, &#39;Meals&#39;, &#39;Banks&#39;, &#39;Insur&#39;, &#39;RlEst&#39;, &#39;Fin&#39;, &#39;Other&#39;], dtype=&#39;object&#39;) . m_ew_rets = pd.read_csv(&#39;data/ind49_m_ew_rets.csv&#39;, header=0, index_col=0, parse_dates=True) / 100 m_ew_rets.index = pd.to_datetime(m_ew_rets.index, format=&quot;%Y%m&quot;).to_period(&#39;M&#39;) m_ew_rets.columns = m_ew_rets.columns.str.strip() m_ew_rets = m_ew_rets[&quot;2000&quot;:] . We have imported and cleaned the returns data for both equal-weighted company portfolios and value-weighted portfolios. Next we will bring in the average firm size and number of firms, so we can create a total-market value-weighted portfolio for both value-weighted industries and equal-weighted industries. . ind_size = pd.read_csv(&#39;data/ind49_m_size.csv&#39;, header=0, index_col=0, parse_dates=True) ind_size.index = pd.to_datetime(ind_size.index, format=&quot;%Y%m&quot;).to_period(&#39;M&#39;) ind_size.columns = ind_size.columns.str.strip() ind_size = ind_size[&quot;2000&quot;:] ind_nfirms = pd.read_csv (&#39;data/ind49_m_nfirms.csv&#39;, header=0, index_col=0, parse_dates=True) ind_nfirms.index = pd.to_datetime(ind_nfirms.index, format=&quot;%Y%m&quot;).to_period(&#39;M&#39;) ind_nfirms.columns = ind_nfirms.columns.str.strip() ind_nfirms = ind_nfirms[&quot;2000&quot;:] . In order to create the total-market value-weighted portfolios for each industry-weighted portfolio, we will write a quick function to do the calculation. Then we will create those portfolios and combine all 4 into a DataFrame . def value_weighted_returns(ind_returns, ind_size, ind_nfirms): # Calculate the market cap for each industry ind_mktcap = ind_size * ind_nfirms # Colculate the total market cap for all industries total_mktcap = ind_mktcap.sum(axis=&quot;columns&quot;) # Calculate the weighting of each industry in the total market cap ind_cap_wgt = ind_mktcap.divide(total_mktcap, axis = &quot;rows&quot;) # Calcualte the total market return for each period total_market_return = (ind_cap_wgt * ind_returns).sum(axis=&quot;columns&quot;) return total_market_return . m_vw_vw_rets = value_weighted_returns(m_vw_rets, ind_size, ind_nfirms) # Calculate the value-weighted portfolio market returns for the equal-weighted industries m_vw_ew_rets = value_weighted_returns(m_ew_rets, ind_size, ind_nfirms) . Calculating the equal-weighted industry portfolios is simply and average return across all industries. We will make that calculation here. . m_ew_vw_rets = m_vw_rets.mean(axis=&quot;columns&quot;) # Calculate the equal-weighted portfolios returns for the equal-weigthed industries m_ew_ew_rets = m_ew_rets.mean(axis=&quot;columns&quot;) . returns = pd.DataFrame({ &quot;Value-Weighted - EW Port&quot;: m_vw_ew_rets, &quot;Value-Weighted - VW Port&quot;: m_vw_vw_rets, &quot;Equal-Weighted - EW Port&quot;: m_ew_ew_rets, &quot;Equal-Weighted - VW Port&quot;: m_ew_vw_rets, }) returns . Value-Weighted - EW Port Value-Weighted - VW Port Equal-Weighted - EW Port Equal-Weighted - VW Port . 2000-01 0.070894 | -0.040130 | 0.044173 | -0.033673 | . 2000-02 0.188816 | 0.019456 | 0.090876 | -0.021078 | . 2000-03 -0.005319 | 0.074055 | 0.022084 | 0.076998 | . 2000-04 -0.113548 | -0.045298 | -0.056082 | -0.003986 | . 2000-05 -0.070897 | -0.030830 | -0.040545 | -0.016059 | . ... ... | ... | ... | ... | . 2020-05 0.085765 | 0.055291 | 0.069798 | 0.055661 | . 2020-06 0.069212 | 0.023037 | 0.073351 | 0.018688 | . 2020-07 0.057484 | 0.058037 | 0.060947 | 0.060033 | . 2020-08 0.051738 | 0.077836 | 0.051535 | 0.065782 | . 2020-09 -0.029560 | -0.036431 | -0.023606 | -0.017476 | . 249 rows × 4 columns . Next, we will create some summary statistics to be to compare these portfolios. . Annualized Returns: the compounded annualized return over the period. . $(1 + R_{t,t+1}) ^{n} - 1$ | . | Annualized Vol: the annualized standard deviation over the period . $ sigma_R = sqrt{ frac{1}{N} sum_{i=1}^N(R_i - bar{R})^2} $ | . | Sharpe Ratio: measures a unit of excess return over of the risk-free rate for each additional unit of risk. . $ text{Sharpe Ratio} = frac{Return - Risk Free Rate}{Volatility} $ | . | Max Drawdown: shows the largest percentage drop in a portfolio from a previous high valuation. . | Skewness: measures the distortion from a normal distribution . $S(R) = frac{E[(R - E(R))^3]}{[Var(R)^{3/2}]}$ | . | Kurtosis: measures the thickness of the tails as compared to a normal distribution . $K(R) = frac{E[(R - E(R))^4]}{[Var(R)^{2}]}$ | . | Histroic VaR (5%): represents the level in which 5% of historical period losses were greater than . | Cornish-Fisher VaR: parametric calculation of Value-at-Risk, which adjusts for the skewness and kurtosis of a distribution . $ tilde{z_a} = z_a + frac{1}{6}(z_a^2 - 1)S + frac{1}{24}(z_a^3 - 3Z_a)(K-3) - frac{1}{36}(2z_a^3 - 5Z_a)S^2$ | . | . def annualize_rets(returns, periods_per_year=12): # compound each years&#39; return at 1+r compounded_growth = (1+returns).prod() # calculate the number of periods in ind_returns n_periods = returns.shape[0] return compounded_growth ** (periods_per_year / n_periods) - 1 def annualize_stdev(returns, periods_per_year=12): return returns.std() * np.sqrt(periods_per_year) def sharpe_ratio(returns, risk_free_rate=0, periods_per_year=12): # calculate the per period risk_free_rate rf_per_period = (1+risk_free_rate) ** (1/periods_per_year) - 1 # calculate the excess return excess_ret = returns - rf_per_period # annualize the excess return ann_ex_ret = annualize_rets(excess_ret, periods_per_year) # calculate the annual volatility ann_sd = annualize_stdev(returns, periods_per_year) return ann_ex_ret / ann_sd def max_drawdown(returns): # calculate the accumulated growth at each period compounded_growth = (1+returns).cumprod() # calculate the previous peak value at each period previous_peaks = compounded_growth.cummax() # calculate the drawdowns at each period drawdowns = (compounded_growth - previous_peaks) / previous_peaks return -drawdowns.min() def skewness(returns): # calculate each period&#39;s return difference from the average return demeaned_r = returns - returns.mean() # calculate the standard devistion of the portfolio sigma_r = returns.std(ddof=0) # using ddof=0, to calculate population standard deviation # caluclate the numerator in the equation exp = (demeaned_r**3).mean() return exp / sigma_r**3 def kurtosis(returns): # calculate each period&#39;s return difference from the average return demeaned_r = returns - returns.mean() # calculate the standard devistion of the portfolio sigma_r = returns.std(ddof=0) # using ddof=0, to calculate population standard deviation # caluclate the numerator in the equation exp = (demeaned_r**4).mean() return exp / sigma_r**4 def var_historic(returns, level=5): return -np.percentile(returns, level) def var_cornish_fisher(returns, level=5): # compute the Z score assuming it was Gaussian z = norm.ppf(level/100) # compute the skewness s = skewness(returns) # compute the kurtosis k = kurtosis(returns) # compute the adjusted Z score z = (z + (z**2 - 1) * s/6 + (z**3 - 3*z) * (k-3)/24 - (2*z**3 - 5*z) * (s**2)/36 ) return -(returns.mean() + z * returns.std(ddof=0)) def summary_stats(returns, periods_per_year=12, risk_free_rate=0.02): summary_df = pd.DataFrame({ &quot;Annualized Return&quot;: returns.aggregate(annualize_rets, periods_per_year=periods_per_year), &quot;Annualized Vol&quot;: returns.aggregate(annualize_stdev, periods_per_year=periods_per_year), &quot;Sharpe Ratio&quot;: returns.aggregate(sharpe_ratio, risk_free_rate=risk_free_rate, periods_per_year=periods_per_year), &quot;Max Drawdown&quot;: returns.aggregate(max_drawdown), &quot;Skewness&quot;: returns.aggregate(skewness), &quot;Kurtosis&quot;: returns.aggregate(kurtosis), &quot;Historic 5% VaR&quot;: returns.aggregate(var_historic), &quot;CF 5% VaR&quot;: returns.aggregate(var_cornish_fisher) }) return summary_df . summary_stats(returns) . Annualized Return Annualized Vol Sharpe Ratio Max Drawdown Skewness Kurtosis Historic 5% VaR CF 5% VaR . Value-Weighted - EW Port 0.091434 | 0.216458 | 0.323901 | 0.583520 | -0.067811 | 4.919207 | 0.093561 | 0.092089 | . Value-Weighted - VW Port 0.066911 | 0.154981 | 0.297115 | 0.500088 | -0.568248 | 3.966089 | 0.078383 | 0.073090 | . Equal-Weighted - EW Port 0.101640 | 0.207453 | 0.386287 | 0.598352 | -0.271297 | 5.841772 | 0.087737 | 0.089501 | . Equal-Weighted - VW Port 0.088844 | 0.165339 | 0.408761 | 0.528351 | -0.642084 | 5.549245 | 0.079556 | 0.075951 | . Additionally, let&#39;s take a look at the growth of these portfolios graphically. First, we will create a cumulative returns DataFrame and then plot that. . cum_returns = (1+returns).cumprod() cum_returns.plot(figsize=(14,6)) . &lt;AxesSubplot:&gt; . &lt;Figure size 1008x432 with 0 Axes&gt; . Takeaways . So, what can we take away from these tables. . Equal-Weighted Company portfolio vs Value-Weighted Company Portfolios. . It appears that for both industry weighting schemes; value-weight (9.14% vs 6.69%), equal-weight (10.16 vs 8.88%), the equally weighted company portfolios outperformed the value-weighted ones. This may simply be explained to the small-cap vs large-cap bias. And that can be seen in the increased volatility exhibited (21.65% vs 15.49% and 20.74 vs 16.53%) and increased Max Drawdowns (58.3% vs 50.0% and 59.8% vs 52.8%). And further looking at the Sharpe Ratio of these portfolios it is not clear, what the best company weighting is, as there seems to be a clear trade-off of risk vs return. . Equal-Weighted Industry portfolio vs Value-Weighted Industry Portfolios. . Here, while holding the company-weighting constant, it is clear that equal-weighting the industries outperforms the value-weighted industry portfolios (10.16% vs 9.14% and 8.88% vs 6.69%). And this is accomplished with little change in the volatility of the portfolios (20.74% vs 21.64% and 16.53% vs 15.50%). This results in dramatically improved Sharpe Ratios (.38 vs .32 and 0.40 vs 0.30). . Overall, from a Sharpe Ratio perspective, the portfolio that performed best over this period and construction schemes was the Equal-Weighted Industry and Value-Weighted Company portfolio, with a Sharpe Ratio of 0.40. .",
            "url": "https://scottknappnj.github.io/FastPagesBlog/2020/12/01/Equal-vs-Value-Weighted-Portfolios.html",
            "relUrl": "/2020/12/01/Equal-vs-Value-Weighted-Portfolios.html",
            "date": " • Dec 1, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Understanding Gradient Descent",
            "content": "!pip install -Uqq fastbook import fastbook fastbook.setup_book() . . from fastai.vision.all import * from fastbook import * matplotlib.rc(&#39;image&#39;, cmap=&#39;Greys&#39;) . . In recent years, with the increase in computing speed, increaed access to GPUs (Graphics Processing Unit) and enhanced machine learning libraries, Gradient Descent has become an integral part of many machine learning and deep learning processes, especially those associated with Neural Networks. . We will begin by defining what Gradient Descent is, why it is useful and how it is used in practice. We will illustrate this with a simple example from the fastai course and book Deep Learning for Coders with fastai and PyTorch. . What is Gradient Descent? . Let&#39;s start by defining what a gradient is. If you remember back to high school calculus, a derivative of a function calculates the rate of change of that function at a specific point, its&#39; rise over run. It tells you how much the output changes if you change the inputs a tiny amount. . Well, when you have a bunch of variables in an equation, you will need to know how much the output changes if you change each of these input variables a tiny bit. This is done by calculating the partial derivatives with respect to each of the variables. And the gradient is the vector which basically stores all of this information. . Gradient Descent is simply the algorithm that uses this information to update a model&#39;s parameters a little at a time improving its&#39; performance. . A Simple Example . Let&#39;s look at an example of the fictional equation of the speed of a roller coaster going over a hill for a period of time, 20. Using PyTorch&#39;s arange method we can create a sequence from 0-19 for this example. . time = torch.arange(0, 20).float() time . tensor([ 0., 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19.]) . We will then simply create a random function which has the speed decreasing to 0 at the midpoint and increasing again after. We will also add some random noise using randn. And we will graph what the function looks like. . speed = torch.randn(20)*2 + 0.6*(time-9.5)**2 + 1 plt.scatter(time, speed); . Next, we will estimate that the function is a quadratic in the form $ a t^2 + bt + c$ and we will look to solve for a, b, c. Let&#39;s go ahead on define this function. . def f(t, params): a, b, c = params return a*(t**2) + (b*t) + c . How will we know if our estimated paramters are good or not? We need to create a loss function that calculates the error rate of our predictions against the actual data. In our case, we will use a simple Mean Squared Error loss function. To calculate this, we simply: . 1) Calculate the difference between our predictions and our target values 2) Square the difference 3) Take a mean of the Squared differences. . This is the loss function that we will use our Gradient Descent algorithm to minimize. . def mse(preds, target): return ((preds - target)**2).mean() . Seven-Step Process . Now, we begin the process of Gradient Descent, which involves seven-steps. . 1) Initialize the parameters 2) Calculate the predictions 3) Calculate the loss 4) Calculate the gradients 5) Step the weights 6) Repeat steps 2) through 5) 7) Stop the process . Step 1: Initialize the Parameters . We begin by setting the initial values for our parameters. It has been proven that using random initial parameters works as well as any other process, so that&#39;s what we will do here utilizing randn again. . We will also add the method .requires_grad_(). This tells PyTorch to remember the necessary information so that we can later calculate the gradients with respect to these parameters. . params = torch.randn(3).requires_grad_() . Step 2: Calculate the Predictions . In this step, we will calculate the predictions based on our current parameter estimates. We will also write a quick function to view our targets results along with our current estimates. . preds = f(time, params) . def show_preds(preds, ax=None): if ax is None: ax=plt.subplots()[1] ax.scatter(time, speed) ax.scatter(time, to_np(preds), color=&#39;red&#39;) # to_np() converts a tensor to an array ax.set_ylim(-300, 100) show_preds(preds) . These predictions look pretty terrible, but we did just start with random parameters. Hopefully, our Gradient Descent algorithm will help improve these results . Step 3: Calculate the loss . In this step, we will simply calcualte the loss using our mse function that we previously created. As you can see from the output, the information for the gradient is also created along with the result. . loss = mse(preds, speed) loss . tensor(24176.3906, grad_fn=&lt;MeanBackward0&gt;) . Step 4: Calculate the gradients . In this step, we will calculate the gradients. And thankfully, we won&#39;t have to do this calcuation manually, PyTorch does this all on its&#39; own. And as you can see from the results, it returns the gradient for each of our parameters. . loss.backward() params.grad . tensor([-51698.6758, -3324.1123, -243.4835]) . Step 5: Step the parameters . Here, we will update the parameters of our model, but how? From the last step we know how much our loss function will change as we change our parameters, but how much should we adjust them by? If we adjust our parameters too fast, we run the risk of overshooting our optimal solution and simply bouncing back and forth and never converging to an answer. If we adjust our parameters too slowly, it will take a long time to converge to an optimal solution. Additionally, we may be fooled if there are local minimums in the function and if we see the loss going back up, we may interpret that as an incorrect global minimum. . The number that we use as a scalar of the gradients is called the learning rate. There is no answer as to what the correct learning rate is, this can be a tunable parameter in the model, meaning we can try a few different rates and choose the best performing one. For this purpose, we will set our learning rate to 1e-5. . Another step with we need to do after adjusting the parameters is to zero-out the gradients for our next iteration, as PyTorch we simply add any calculated gradients to the existing ones, which we don&#39;t want. . lr = 1e-5 params.data -= lr * params.grad.data params.grad = None . Step 6: Repeat the process . Here, we will go back to step #2 and #3. We will make the predictions with our new parameters and calculate the loss and visualize our new predictions. . preds = f(time, params) mse(preds, speed) . tensor(4919.5947, grad_fn=&lt;MeanBackward0&gt;) . show_preds(preds) . We&#39;ve improved our Mean Squared Error significantly. Not too bad, for only updating the parameters once. Let&#39;s write a function including everything we&#39;ve done so far, so we can repeat the process a few more times. . def apply_step(params, prn=True): preds = f(time, params) loss = mse(preds, speed) loss.backward() params.data -= lr * params.grad.data params.grad = None if prn: print(loss.item()) return preds . for i in range(10): apply_step(params) . 4919.5947265625 1275.6175537109375 586.0635986328125 455.57635498046875 430.88177490234375 426.20635986328125 425.319091796875 425.14862060546875 425.114013671875 425.10498046875 . Step 7: Stop the process . We have decided to stop after an additional 10 epochs arbitrarily. In practice, we would likely watch the loss function and our metrics to decide when to step. But here we have seen our Mean Squared Error to a stable amount in only 5-6 epochs. . an epoch is the number of times our algorithm works through all of our data. | . Summary . From this simple example, I think you can begin to see the power of using the Gradient Descent algorithm to update model parameters to improve performance. .",
            "url": "https://scottknappnj.github.io/FastPagesBlog/2020/11/29/Understanding-Gradient-Descent.html",
            "relUrl": "/2020/11/29/Understanding-Gradient-Descent.html",
            "date": " • Nov 29, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://scottknappnj.github.io/FastPagesBlog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://scottknappnj.github.io/FastPagesBlog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Since you are here reading my blog, I figured I’d tell you a little about how I began this journey into Data Science. My entire career has been spent in finance arena with the first half in the Interest Rate Derivative world. That world of swaps and swaptions, caps and floors, and anything else which would cause an unassuming questioner’s eyes to glaze over after innocently asking ‘So, what do you do?’ I spent about a dozen years managing interest rate derivative portfolios and pricing cutting-edge structured products for clients to help them hedge their interest rate exposure. It was an interesting place to be, for an Accounting major, who’s last math class was Multivariate Calculus, freshmen year of college and without any programming background. And as interested as I was to dive into the math and coding, back in the late 1990’s / early 2000’s, the paths to learning higher-levels of math, programming skills and financial modeling weren’t as easily available as today. As a result, I developed very strong intutions of how models behave by taking a top-down approach to financial modeling which served as a very nice balance to the PhD math/physics/engineering backgrounds of the quantitative modeling team which tended to look at everything from a bottom-up approach. But even with this lack of technical skills, I still was able to develop the ability to identify cutting-edge modeling; implementing the SABR (Stochastic Alpha-Beta-Rho) option model, early after the original paper was published and developed new derivative solutions to hedge mortgage pre-payment risk, dubbed ‘Balanced-Guaranteed’ products. . After the 2008/09 financial crisis, there was a huge shift in the banking industry in the ability to manage risk, meaning they no longer did, banks simply hedged it away. This meant a huge contraction in the industry, leaving most positions to be filled by traders with quantitative backgrounds, who could automate trade execution and risk management. This led me to move away from the markets and to work with my wife for a few years as we tried to start a franchise business, and then to me taking my experience in the finanical markets and starting my own Financial Planning practice. . The old saying, ‘you don’t know what you don’t know’ was never more true, and that first year was filled with starts and stops as I learned the industry and redefined what type of practice I wanted to create. The end result was to be a planning-first practice, which was product and company agnostic, and used deep comprehensive planning in a variety of areas (Retirement, Investment, Tax, Insurance and Estate) to drive recommendations for the clients. Over the next few years, I developed a great deal of knowledge in all of those areas and experience some levels of success and acquired my CFA certification, but the point came where an unfortunate realization had to be made. This realization was that success in the financial advisory arena, is not determined by the knowledge of your craft or the quality of the work you perform, but how well you can sell. And while I consider myself a very good planner, and a good relatoionship manager, if I’m being honest I am a mediocre sales person. Then the opportunity arose to sell my practice and I had to make that difficult choice of selling. Then Covid hit. . While job-seeking became difficult, this gave me the opporunity to dive head first into learning to code with a focus on Data Science. It began with the 10-course specialization offered by John Hopkins through Coursera. This course is taught in the R language, which for me was a great initital foray to learning programming while dipping my toes into what Data Science encompasses. From that I’ve taken a bunch of other online courses, focusing on math: (Statistics, Linear Algebra, Calculus), coding (R, Python, SQL) and in machine learning (ISLR, NLP, deep learning, etc.). Additionally, I’ve sprinkled in some Sustainable Investing education, taking a v ariety of courses offered by PRI (Principles for Responsible Investment) and being the first class to earn the Sustainability and Climate Risk certification offered by GARP. . Now, I’m focusing on a few things. I’m working through the Deep Learning with fastai book/course, still brushing on some Linear Algebra/Calculus and beginning to put together some projects. These projects will be a culmination of all the different skills I’ve developed over the past few months, focusing on some of the specific applications that I’ve learned in a recent course on Coursera, Investment Management with Python. There will also be other shorter blog posts about things I’m currently learning. . It hasn’t been a straight line, but I do think I’m beginning to converge on the correct path. And in the end, hopefully this will not all be for nought. Obviously, the near-term goal of all of this is to be able to find a role to in the Data Science world, that I can transition into and best utilize my extensive financial background while continuing to develop these newly acquired skills. . I hope you enjoy reading about my journey. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://scottknappnj.github.io/FastPagesBlog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://scottknappnj.github.io/FastPagesBlog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}